{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataOrtho Dynamic \n",
    "\n",
    "Dynamic Subset Modeling Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eduardo\\miniconda3\\envs\\dataortho_env\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 2.10.0\n",
      "tfa: 0.20.0\n",
      "GPU devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import  Callback,ModelCheckpoint,TensorBoard, EarlyStopping # ,LearningRateScheduler - using dynamic one\n",
    "\n",
    "print('tf:',tf.__version__)\n",
    "print('tfa:',tfa.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        print('GPU devices: ',gpus)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vars and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "resampled_dir  = 'C:/Users/Eduardo/Desktop/DataOrtho_Resampled/'\n",
    "tfrecords_dir = 'C:/Users/Eduardo/Desktop/DataOrtho_Serialized/'\n",
    "logs_dir = 'C:/Users/Eduardo/Desktop/dataortho_edu/logs/' \n",
    "\n",
    "# Cross validation vars\n",
    "fold_idx = 1\n",
    "n_folds = 5\n",
    "\n",
    "# Training parameters\n",
    "params = {'augment':True,\n",
    "          'mode': tf.estimator.ModeKeys.TRAIN, \n",
    "          'seed':42,\n",
    "          'subset':'DATASET_DYNAMIC',\n",
    "          'interpolation':'Linear Interpolation',\n",
    "          'normalization': 'batchnorm',#'batchnorm',\n",
    "          'total_train_samples':0, # value attributed in dataset_split\n",
    "          'total_val_samples':0,   # value attributed in dataset_split\n",
    "          'lr': 0.0001, # 0.001 de forma a ver\n",
    "          'loss':'dice+ce',\n",
    "          'dropout':0.2,\n",
    "          'batch_size':2,\n",
    "          'norm_params_minmax':(0.0,1072.0),\n",
    "          'norm_params_meanstd':None\n",
    "          }\n",
    "# Direct vars\n",
    "batch_size = 2\n",
    "seed = 42 # check our context\n",
    "\n",
    "#Numver of channel for each subset\n",
    "num_channels = {'DATASET_AXIAL': 12,'DATASET_SAGITTAL':8,'DATASET_DYNAMIC':19}\n",
    "\n",
    "# Data Shapes (Before batching) \n",
    "xshape =  {'DATASET_AXIAL': (32,256, 256,1), 'DATASET_SAGITTAL': (32,256, 256,1),'DATASET_DYNAMIC': (64,256, 256,1)}\n",
    "yshape = {'DATASET_AXIAL': (32,256, 256,12), 'DATASET_SAGITTAL': (32,256, 256,8),'DATASET_DYNAMIC': (64, 256, 256, 19)}\n",
    "input_shape={'DATASET_AXIAL': (32,256, 256,1), 'DATASET_SAGITTAL': (32,256, 256,1),'DATASET_DYNAMIC': (64,256, 256,1)} \n",
    "\n",
    "\n",
    "# Test subset individuals \n",
    "test_subjects = {'DATASET_DYNAMIC': [5, 20, 23, 33, 43, 51, 61, 67, 71, 76, 12, 13, 37, 41, 42, 47, 59, 65]}\n",
    "# landmark Subset Classes\n",
    "landmarkClasses = {'DATASET_DYNAMIC':\n",
    "                       {0:'D0',\n",
    "                        1:'D1', \n",
    "                        2:'D2', \n",
    "                        3:'D3', \n",
    "                        4:'D4', \n",
    "                        5:'D5', \n",
    "                        6:'D6', \n",
    "                        7:'D7', \n",
    "                        8:'D8', \n",
    "                        9:'D9', \n",
    "                        10:'D10', \n",
    "                        11:'D11',\n",
    "                        12:'D12', \n",
    "                        13:'D13', \n",
    "                        14:'D14', \n",
    "                        15:'D15', \n",
    "                        16:'D16', \n",
    "                        17:'D17', \n",
    "                        18:'DB'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4D (channels dimension) Visualization \n",
    "- Help functions for Data augmentation Visualization\n",
    "- Help functions for Evaluation Visualization\n",
    "- String path refactor for Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_montage_mri_4D_channel(mri_volume,heatmaps_masks,channel, start_slice, end_slice, step=1):\n",
    "    num_landmarks = heatmaps_masks.shape[-1] \n",
    "    \n",
    "    fig, axarr = plt.subplots(1, (end_slice - start_slice) // step, figsize=(20, 5*num_landmarks))\n",
    "    \n",
    "    max_intensity_idx = np.unravel_index(np.argmax(heatmaps_masks[..., channel]), heatmaps_masks[..., channel].shape)\n",
    "    \n",
    "    print('Intensity masks: ', heatmaps_masks[max_intensity_idx[0],max_intensity_idx[1],max_intensity_idx[2], channel])\n",
    "    print('Intensity volume: ', mri_volume[max_intensity_idx[0],max_intensity_idx[1],max_intensity_idx[2], 0])\n",
    "\n",
    "    print(f\"Channel {channel} has maximum intensity at slice: {max_intensity_idx[0]}\")\n",
    "    max_intensity = np.argmax(heatmaps_masks[..., channel])\n",
    "    print('max: ', max_intensity)\n",
    "    for i, idx in enumerate(range(start_slice, end_slice, step)):\n",
    "        axarr[i].imshow(mri_volume[idx,:, :,:], cmap='gray')  # MRI in grayscale\n",
    "        axarr[i].imshow(heatmaps_masks[idx,:, :,channel], cmap='jet', alpha=0.5,vmin = 0, vmax=1)  # specific landmark heatmap overlay\n",
    "        axarr[i].axis('off')\n",
    "        if i == 0:\n",
    "            if channel == num_landmarks - 1:\n",
    "                axarr[i].set_ylabel(f'Background')\n",
    "            else:\n",
    "                axarr[i].set_ylabel(f'Landmark {channel + 1}')\n",
    "            \n",
    "        axarr[i].set_title(f'Slice: {idx}')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_specific_slice_4Dmask(slice_idx,channel,mri_volume, heatmaps_channel):\n",
    "    fig, axarr = plt.subplots(1, figsize=(16, 8))\n",
    "    axarr.imshow(mri_volume[slice_idx,:,:],cmap='gray')\n",
    "    axarr.imshow(heatmaps_channel[slice_idx,:, :,  channel], cmap='jet', alpha=0.5,vmin = 0, vmax=1)\n",
    "    axarr.set_title(f'Slice: {slice_idx}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#---------- Help functions for Data augmentation Visualization ----------#\n",
    "# Function to visualize a single slice\n",
    "def visualize_slice(volume_slice, mask_slice=None, cmap='gray'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(volume_slice, cmap=cmap)\n",
    "    plt.title('Volume Slice')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if mask_slice is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_slice, cmap=cmap)\n",
    "        plt.title('Mask Slice')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#---------- Help functions for Evaluation Visualization ----------#\n",
    "\n",
    "def visualize_learning_curves(epochs,train_loss,val_loss):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# not needed in ubuntu\n",
    "def refactor_path(path_example):\n",
    "    return path_example.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# returns the maximum intensity for each channel slice \n",
    "def analyze_heatmaps_predictions(heatmaps_masks,subset):\n",
    "    num_channels = heatmaps_masks.shape[-1] # Number of landmarks (channels)\n",
    "    max_intensity_slices = []\n",
    "    landmarksNames = landmarkClasses[subset]\n",
    "    for channel in range(num_channels):\n",
    "        # Find the index of maximum intensity in the channel\n",
    "        max_intensity_idx = np.unravel_index(np.argmax(heatmaps_masks[..., channel]), \n",
    "                                             heatmaps_masks[..., channel].shape)\n",
    "        # Get the slice number (depth) with maximum intensity\n",
    "        slice_with_max_intensity = max_intensity_idx[0]\n",
    "\n",
    "        max_intensity_slices.append((channel, slice_with_max_intensity))\n",
    "    \n",
    "    for channel, slice_idx in max_intensity_slices:\n",
    "        print(f\"Landmark {landmarksNames[channel]} has maximum intensity at slice: {slice_idx}\")\n",
    "    #return max_intensity_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D procedures from Architecture were analysed and used as base for the following:\n",
    "\n",
    "- random_horizontal_flip()\n",
    "- random_rotate_3d() \n",
    "- random_translate_3d() : 2D only\n",
    "- blur()\n",
    "- noising()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validated**\n",
    "- Random Horizontal Flip 3D \n",
    "- Random Rotate 3D \n",
    "- Random Translate 3D\n",
    "- Blur \n",
    "- Noising\n",
    "\n",
    "**process_augmentation** - main Augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_horizontal_flip(samples, labels, threshold=0.3):\n",
    "    h_flip = tf.random.uniform([]) > threshold \n",
    "    def hflip_volume(volume):\n",
    "        return tf.image.flip_left_right(volume)\n",
    "\n",
    "    samples = tf.cond(h_flip, lambda: hflip_volume(samples), lambda: samples)\n",
    "    labels = tf.cond(h_flip, lambda: hflip_volume(labels), lambda: labels)\n",
    "    return samples, labels\n",
    "\n",
    "# rotate\n",
    "def random_rotate_3d(samples,labels,n_channels_samples,n_channels_labels,angle_range=(-0.7,0.7)):\n",
    "    angle = tf.random.uniform([], minval=angle_range[0], maxval=angle_range[1])\n",
    "    perform_augmentation = tf.random.uniform([]) > 0.3\n",
    "    samples = tf.cond(perform_augmentation, lambda: rotate_volume(samples, angle,n_channels_samples), lambda: samples)\n",
    "    labels = tf.cond(perform_augmentation, lambda: rotate_volume(labels, angle,n_channels_labels), lambda: labels)\n",
    "    return samples, labels\n",
    "\n",
    "\n",
    "def rotate_volume(volume,angle,n_channels):\n",
    "    def rotate_slice(slice):\n",
    "        # ensure the number of channels is known\n",
    "        # angle comes from higher lvl fnction\n",
    "        rotated_channels = [tfa.image.rotate(slice[:, :, c], angle) for c in range(n_channels)]\n",
    "        return tf.stack(rotated_channels, axis=-1)\n",
    "    # Apply the rotation to each slice in the volume or mask\n",
    "    rotated_volume = tf.map_fn(rotate_slice, volume, dtype=volume.dtype)\n",
    "    return rotated_volume\n",
    "\n",
    "# translate\n",
    "def random_translate_3d(samples, labels,n_slices,n_channels_samples,n_channels_labels, threshold=10):\n",
    "    perform_augmentation = tf.random.uniform([]) > 0.4 # type: ignore\n",
    "    translations = tf.random.uniform([2], minval=-threshold, maxval=threshold)\n",
    "    samples = tf.cond(perform_augmentation, lambda: translate_volume(samples, translations,n_slices,n_channels_samples), lambda: samples) \n",
    "    labels = tf.cond(perform_augmentation, lambda: translate_volume(labels, translations,n_slices,n_channels_labels), lambda: labels)\n",
    "    return samples, labels\n",
    "\n",
    "\n",
    "def translate_volume(volume, translations,n_slices, n_channels):\n",
    "    # collect the translated slices in this list.\n",
    "    translated_slices = []\n",
    "\n",
    "    # Loop over the depth dimension and translate each 2D slice.\n",
    "    for i in range(n_slices):\n",
    "        if n_channels == 1:\n",
    "            # Translate the 2D slice and keep the channel dimension as the last dimension.\n",
    "            slice_2d = volume[i, :, :, 0]  # Extract the 2D slice.\n",
    "            translated_slice = tfa.image.translate(slice_2d, translations)\n",
    "            translated_slice = tf.expand_dims(translated_slice, axis=-1)  # Add the channel dimension back.\n",
    "        else:\n",
    "            # Translate the 2D slice for each channel.\n",
    "            slice_3d = volume[i, :, :, :]  # Extract the 3D slice (depth,height, width, channels).\n",
    "            translated_slice = [tfa.image.translate(slice_3d[:, :, c], translations) for c in range(n_channels)]\n",
    "            translated_slice = tf.stack(translated_slice, axis=-1)\n",
    "\n",
    "        # append the translated slice to the list.\n",
    "        translated_slices.append(translated_slice)\n",
    "\n",
    "    # stack the translated slices along the depth dimension.\n",
    "    translated_volume = tf.stack(translated_slices, axis=0)\n",
    "    \n",
    "    return translated_volume\n",
    "\n",
    "def blur(samples, labels, sigma=2.0):\n",
    "    def blur_slice(slice):\n",
    "        return tfa.image.gaussian_filter2d(slice, sigma=sigma)\n",
    "    \n",
    "    perform_blurring = tf.random.uniform([]) > 0.4 # type: ignore\n",
    "    blurred_samples = tf.cond(perform_blurring,\n",
    "                              lambda: tf.map_fn(blur_slice, samples, dtype=samples.dtype),\n",
    "                              lambda: samples)\n",
    "    return blurred_samples, labels\n",
    "    #blurred_samples = tf.map_fn(lambda slice: tfa.image.gaussian_filter2d(slice, sigma=sigma), samples, dtype=samples.dtype)\n",
    "    #return blurred_samples, labels\n",
    "\n",
    "def noising(samples, labels, mean=0.0, stddev=0.01):\n",
    "    perform_noising = tf.random.uniform([]) > 0.4 # type: ignore\n",
    "    noise = tf.random.normal(shape=tf.shape(samples), mean=mean, stddev=stddev)\n",
    "    noised_samples = tf.cond(perform_noising,\n",
    "                            lambda: samples + noise,\n",
    "                            lambda: samples)\n",
    "    return noised_samples, labels\n",
    "\n",
    "def process_augmentation(samples,labels):\n",
    "    #samples,labels = random_rotate_3d(samples,labels,xshape[params['subset']][-1],yshape[params['subset']][-1])\n",
    "    # samples, labels = random_horizontal_flip(samples, labels)\n",
    "    samples, labels = random_translate_3d(samples, labels,xshape[params['subset']][0],xshape[params['subset']][-1],yshape[params['subset']][-1])\n",
    "    samples, labels = blur(samples, labels)\n",
    "    samples, labels = noising(samples, labels)\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input, Load and Normalization\n",
    " - mean and standard deviation calculation for normalization.\n",
    " - type 0 normalization with min and max\n",
    " - type 1 or else for normalization with mean and stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notes: TFRecords format data**\n",
    "\n",
    "Shuffle, batch, and prefetch methods called in the correct order and with the correct parameters to ensure efficient loading and training.\n",
    "\n",
    "Functions responsible for the train, validation and test input dataset to the network, **train_tfr_fn**, **val_tfr_fn**, **test_tfr_fn** respectively\n",
    "\n",
    "**dataset_tfr_split**: Function responsible for the division and creation for specific Subset data: train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(serialized_sequence):\n",
    "    feature_description = {\n",
    "        'volume': tf.io.FixedLenFeature([], tf.string),\n",
    "        'mask': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "    sequence = tf.io.parse_single_example(serialized_sequence,feature_description)\n",
    "    volume = tf.io.parse_tensor(sequence['volume'], out_type=tf.float32)\n",
    "    mask = tf.io.parse_tensor(sequence['mask'], out_type=tf.float32)\n",
    "\n",
    "    return volume,mask\n",
    "\n",
    "def get_norm_params(tfrecords_paths, norm_type=0):\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_paths)\n",
    "    dataset = dataset.map(parse_tfrecord,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    global_min = tf.constant(float('inf'), tf.float32)\n",
    "    global_max = tf.constant(float('-inf'), tf.float32)\n",
    "\n",
    "    for vol, _ in dataset:\n",
    "        batch_min = tf.reduce_min(vol)\n",
    "        batch_max = tf.reduce_max(vol)\n",
    "        \n",
    "        global_min = tf.reduce_min([global_min, batch_min])\n",
    "        global_max = tf.reduce_max([global_max, batch_max])\n",
    "    \n",
    "    # Convert TensorFlow tensors to numpy values before returning\n",
    "    return global_min.numpy(), global_max.numpy()\n",
    "\n",
    "def tf_min_max_normalize(volume, min_val, max_val):\n",
    "    return (volume - min_val) / (max_val - min_val)\n",
    "\n",
    "def tf_standard_normalize(volume, mean, std):\n",
    "    return (volume - mean) / std\n",
    "\n",
    "# load tfrecords function \n",
    "def load_tfr_dataset(tfrecords_paths, normalization_params, norm_type=0):\n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_paths)\n",
    "    dataset = dataset.map(parse_tfrecord,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    if norm_type == 0:\n",
    "        _min, _max = normalization_params\n",
    "        dataset = dataset.map(lambda volume_file, mask_file: (tf_min_max_normalize(volume_file,_min,_max),mask_file))\n",
    "            \n",
    "    else:\n",
    "        mean, std = normalization_params            \n",
    "        dataset = dataset.map(\n",
    "            lambda volume_file, mask_file: (tf_standard_normalize(volume_file, mean, std), mask_file))\n",
    "    \n",
    "    volume_shape = xshape[params['subset']]\n",
    "    mask_shape = yshape[params['subset']]\n",
    "\n",
    "    dataset = dataset.map(lambda volume, mask: (tf.ensure_shape(volume, volume_shape),\n",
    "                                                tf.ensure_shape(mask, mask_shape)))\n",
    "    return dataset\n",
    "\n",
    "def parse_test_tfrecords(data_dir,test_subjects):\n",
    "    sequence_files = []\n",
    "     \n",
    "    for individual in os.listdir(data_dir):\n",
    "        if int(individual) in test_subjects:\n",
    "            individual_sequence = os.path.join(data_dir, individual)\n",
    "            for knee in os.listdir(individual_sequence):\n",
    "                    knee_sequence = os.path.join(individual_sequence, knee)\n",
    "                    for sequence in os.listdir(knee_sequence):\n",
    "                        sequence_path = os.path.join(knee_sequence, sequence)\n",
    "                        file_path = os.path.join(sequence_path, sequence + '.tfrecord')\n",
    "                        sequence_files.append(file_path)\n",
    "\n",
    "    #return list(zip(volume_files, masks_files))\n",
    "    return sequence_files\n",
    "\n",
    "def load_tfr_subject_test(subject_path,normalization_params,norm_type=0):\n",
    "    data = tf.data.TFRecordDataset(subject_path)\n",
    "    data = data.map(parse_tfrecord,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    if norm_type == 0:\n",
    "        _min, _max = normalization_params\n",
    "        data = data.map(lambda volume_file, mask_file: (tf_min_max_normalize(volume_file,_min,_max),mask_file))\n",
    "            \n",
    "    else:\n",
    "        mean, std = normalization_params            \n",
    "        dataset = data.map(\n",
    "            lambda volume_file, mask_file: (tf_standard_normalize(volume_file, mean, std), mask_file))\n",
    "    \n",
    "    volume_shape = xshape[params['subset']]\n",
    "    mask_shape = yshape[params['subset']]\n",
    "\n",
    "    data = data.map(lambda vol, mask: (tf.ensure_shape(vol,volume_shape),\n",
    "                                       tf.ensure_shape(mask,mask_shape)))\n",
    "    # input dimension for predict call is 5D\n",
    "    data = data.batch(1)\n",
    "    return data\n",
    "\n",
    "# List of tuples, where each tuple contains the path to a volume file and its corresponding mask file.\n",
    "def create_subject_tfrpaths(subset_dir,test_subjects):\n",
    "    sequence_files = []\n",
    "\n",
    "    for individual in os.listdir(subset_dir):\n",
    "        if int(individual) not in test_subjects:\n",
    "            individual_sequences = os.path.join(subset_dir, individual) \n",
    "            for knee in os.listdir(individual_sequences):\n",
    "                knee_sequences = os.path.join(individual_sequences, knee)\n",
    "                for sequence in os.listdir(knee_sequences):\n",
    "                    sequence_file = os.path.join(knee_sequences, sequence,sequence + '.tfrecord')\n",
    "                    sequence_files.append(sequence_file)\n",
    "                         \n",
    "    return sequence_files\n",
    "\n",
    "def subset_cross_validation_tfr(subset_dir, test_subjects,fold_idx, n_folds):\n",
    "    # get all subject paths\n",
    "    tfrecords_paths = create_subject_tfrpaths(subset_dir,test_subjects)\n",
    "    # convert to numpy array for easy manipulation\n",
    "    \n",
    "    # perform cross-validation\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    folds = list(kf.split(tfrecords_paths))\n",
    "    \n",
    "    # get the current fold data\n",
    "    train_indices, val_indices = folds[fold_idx]\n",
    "    train_paths = [tfrecords_paths[i] for i in train_indices]\n",
    "    val_paths = [tfrecords_paths[i] for i in val_indices]\n",
    "\n",
    "    return train_paths, val_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfr_fn(subject_paths,normalization_params_train,norm_type,batch_size,seed,params):\n",
    "        \"\"\" Create dataset for training \"\"\"\n",
    "        dataset = load_tfr_dataset(subject_paths,normalization_params_train,norm_type)\n",
    "    \n",
    "\n",
    "        if params['augment']:\n",
    "            dataset = dataset.map(\n",
    "                map_func=lambda x, y: process_augmentation(x, y),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "            )\n",
    "        dataset = dataset.shuffle(10, seed)\n",
    "        dataset = dataset.batch(batch_size,\n",
    "                                drop_remainder=True)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "def val_tfr_fn(val_paths,normalization_params,batch_size):\n",
    "    \"\"\" Create dataset for Validation \"\"\"\n",
    "    dataset = load_tfr_dataset(val_paths, normalization_params)\n",
    "    #dataset = dataset.cache()\n",
    "    dataset = dataset.batch(batch_size=batch_size) # not\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def test_tfr_fn(test_paths,normalization_params):\n",
    "    \"\"\" Create dataset for Test\"\"\"\n",
    "    dataset = load_tfr_dataset(test_paths, normalization_params)\n",
    "    dataset = dataset.batch(1)\n",
    "                            #drop_remainder=self.params.benchmark)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    #print_dataset_shapes(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def validate_dataset(filenames, reader_opts=None):\n",
    "    \"\"\"\n",
    "    Attempt to iterate over every record in the supplied iterable of TFRecord filenames\n",
    "    :param filenames: iterable of filenames to read\n",
    "    :param reader_opts: (optional) tf.python_io.TFRecordOptions to use when constructing the record iterator\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    corrupts = []\n",
    "    corrupt = 0\n",
    "    for fname in filenames:\n",
    "        #print('validating ', fname)\n",
    "        record_iterator = tf.compat.v1.io.tf_record_iterator(path=fname, options=reader_opts)\n",
    "        try:\n",
    "            for _ in record_iterator:\n",
    "                i += 1\n",
    "        except Exception as e:\n",
    "            print('error in {} at record {}'.format(fname, i))\n",
    "            corrupt = 1\n",
    "            corrupts.append(fname)\n",
    "            print(e)\n",
    "    \n",
    "    return corrupt\n",
    "\n",
    "def dataset_tfr_split():\n",
    "    data_dir = os.path.join(tfrecords_dir,params['subset'])\n",
    "\n",
    "    test_paths = parse_test_tfrecords(data_dir,test_subjects[params['subset']])\n",
    "    # perform cross-validation on the paths that are valid for training,\n",
    "    # subset_cross_validation must make sure that trains_paths and val_paths return without\n",
    "    # any of the subject indicated on test_subjects list \n",
    "    train_paths, val_paths  = subset_cross_validation_tfr(data_dir, test_subjects[params['subset']], fold_idx, n_folds) #separate norm_params training and validation\n",
    "\n",
    "    if params['norm_params_minmax']: \n",
    "        normalization_params_train = params['norm_params_minmax']\n",
    "    else:\n",
    "        normalization_params_train = get_norm_params(train_paths,0)\n",
    "    print(\"Number of training tuple paths:\", len(train_paths))\n",
    "    print(\"Number of validation tuple paths:\", len(val_paths))\n",
    "    print(\"Number of test tuple paths:\", len(test_paths))\n",
    "    print(\"Normalization parameters training:\", normalization_params_train)\n",
    "    \n",
    "    # defined for number of steps on the model fit\n",
    "    params['total_train_samples'] = len(train_paths)\n",
    "    params['total_val_samples'] =  len(val_paths)\n",
    "    \n",
    "    # create the train_ds input and tensorflow.python.data.ops.map_op._MapDataset Dataset\n",
    "    train_ds = train_tfr_fn(train_paths,normalization_params_train,0,batch_size,seed,params)\n",
    "    \n",
    "    val_ds = val_tfr_fn(val_paths,normalization_params_train,batch_size)\n",
    "    \n",
    "    \n",
    "    return train_paths, train_ds,val_ds, test_paths ,normalization_params_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures Layers\n",
    "\n",
    "Methods based on basic 3D U-Net architecture [https://catalog.ngc.nvidia.com/orgs/nvidia/resources/unet3d_medical_for_tensorflow](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightblue\"> Lower level methods </span>\n",
    "\n",
    "Convolution, normalization and activation methods:\n",
    " - more control of the architecture\n",
    " - use of regularization (?)\n",
    "\n",
    "Kernel Initialiser: how the weights of the kernels are initially set before training begins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:lightblue\"> Higher Level methods </span>\n",
    "\n",
    "Not using MaxPooling! The key difference between using strided convolutions and MaxPooling for downsampling is that strided convolutions involve learnable parameters and can learn to downsample in a more data-driven manner, whereas MaxPooling is a fixed operation that simply takes the maximum value over the pooling window.\n",
    "\n",
    "By not applying normalization or activation immediately after upsampling, you allow the model to first merge these upsampled features with the skip connection features. This can be important because the skip connections carry high-resolution spatial information from the encoder, which can be more effectively integrated with the upsampled features before any further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalization(inputs, name, mode):\n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    if name == 'instancenorm':\n",
    "        return tf.keras.layers.LayerNormalization(\n",
    "            axis=[1, 2, 3],  # Normalizing across the spatial dimensions\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            epsilon=1e-6)(inputs)\n",
    "\n",
    "    if name == 'groupnorm':\n",
    "        return tfa.layers.GroupNormalization(\n",
    "            groups=16,\n",
    "            axis=-1,  # Channel axis\n",
    "            epsilon=1e-5)(inputs)\n",
    "\n",
    "    if name == 'batchnorm':\n",
    "        return tf.keras.layers.BatchNormalization(axis=-1, #channels index is the last one\n",
    "                                                  trainable=True,\n",
    "                                                  virtual_batch_size=None)(inputs, training=training)\n",
    "    if name == 'none':\n",
    "        return inputs\n",
    "\n",
    "    raise ValueError('Invalid normalization layer')\n",
    "\n",
    "\n",
    "def _activation(out, activation):\n",
    "    if activation == 'relu':\n",
    "        return tf.keras.layers.ReLU()(out)\n",
    "    if activation == 'leaky_relu':\n",
    "        return tf.keras.layers.LeakyReLU(alpha=0.01)(out)\n",
    "    if activation == 'sigmoid':\n",
    "        return tf.keras.layers.Activation('sigmoid')(out)\n",
    "    if activation == 'softmax':\n",
    "        return tf.keras.layers.Activation('softmax')(out)\n",
    "    if activation == 'none':\n",
    "        return out\n",
    "\n",
    "    raise ValueError(\"Unknown activation {}\".format(activation))\n",
    "\n",
    "def convolution(inputs,  \n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                normalization='batchnorm',\n",
    "                activation='relu',\n",
    "                transpose=False):\n",
    "\n",
    "    if transpose:\n",
    "        conv = tf.keras.layers.Conv3DTranspose\n",
    "    else:\n",
    "        conv = tf.keras.layers.Conv3D\n",
    "    regularizer = None #tf.keras.regularizers.l2(1e-5) # trying L2 Regularization\n",
    "\n",
    "    use_bias = normalization == \"none\"\n",
    "    inputs = conv(filters=out_channels,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=stride,\n",
    "                  activation=None,\n",
    "                  padding='same',\n",
    "                  data_format='channels_last',\n",
    "                  kernel_initializer=tf.compat.v1.initializers.he_uniform(), # perhaps use HE\n",
    "                  kernel_regularizer=regularizer,\n",
    "                  bias_initializer='zeros',\n",
    "                  bias_regularizer=regularizer,\n",
    "                  use_bias=use_bias)(inputs)\n",
    "    \n",
    "    # batch normalization before each activation\n",
    "    inputs = _normalization(inputs, normalization, mode)\n",
    "    #print(' Convolution INPUTS SHAPE',inputs.shape)\n",
    "    return _activation(inputs, activation)\n",
    "\n",
    "\n",
    "''' Padding and Cropping (Not currently used)'''\n",
    "# if feature maps align perfectly, we dont need this method\n",
    "def dynamic_crop_and_concat(up_conv,skip_feature):\n",
    "    print('Skip feature before crop :',up_conv.shape)\n",
    "\n",
    "    crop_size = [(up_conv.shape[i+1] - skip_feature.shape[i+1]) // 2 for i in range(3)]\n",
    "    additional_crop = [(up_conv.shape[i+1] - skip_feature.shape[i+1]) % 2 for i in range(3)]\n",
    "    crop_size = [(crop_size[i], crop_size[i] + additional_crop[i]) for i in range(3)]\n",
    "    \n",
    "    # Concatenate along the feature axis\n",
    "    cropped_up_conv = tf.keras.layers.Cropping3D(cropping=crop_size)(up_conv)\n",
    "    print('INPUT SHAPE cropped to CONCATENATE:',cropped_up_conv.shape,'Skip feature to CONCATENATE:',skip_feature.shape)\n",
    "\n",
    "    return tf.keras.layers.Concatenate(axis=-1)([cropped_up_conv, skip_feature])\n",
    "\n",
    "'''Encoding'''\n",
    "def input_block(inputs, out_channels, normalization, mode):\n",
    "    #stride = 1 kernel_size= 3\n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    return inputs\n",
    "\n",
    "# Downsample with Residual blocks\n",
    "def downsample_Residual_block(inputs, out_channels, normalization, mode):\n",
    "    # Convolutional path\n",
    "    #the next convolution imitates the maxpooling behaviour\n",
    "    # offering additional benefits in feature learning and representation\n",
    "    conv_path = convolution(inputs, out_channels=out_channels, normalization=normalization, \n",
    "                            mode=mode, stride=2)\n",
    "    conv_path = convolution(conv_path, out_channels=out_channels, normalization=normalization, \n",
    "                            mode=mode)\n",
    "\n",
    "    # Residual path modification adds a residual connection that includes a 1x1x1 \n",
    "    # convolution with a stride of 2 to downsample the input before adding  it \n",
    "    # to the output of the convolutional layers within the block\n",
    "    # previous kernel = 3\n",
    "    residual_path = convolution(inputs, out_channels=out_channels, kernel_size=1, stride=2,\n",
    "                                normalization=normalization, mode=mode, activation='none')\n",
    "    \n",
    "    # Add the residual connection\n",
    "    out = tf.keras.layers.Add()([conv_path, residual_path])\n",
    "    out = _activation(out, 'relu')\n",
    "\n",
    "    return out\n",
    "\n",
    "# classic U-Net\n",
    "def downsample_block(inputs, out_channels, normalization, mode):\n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode, stride=2)\n",
    "    return convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "\n",
    "\n",
    "'''Decoding'''\n",
    "\n",
    "def attention_gate(inputs,attention,inter_channel):\n",
    "    theta_x = tf.keras.layers.Conv3D(inter_channel,kernel_size=2,strides=2,padding='same')(inputs)\n",
    "    phi_g = tf.keras.layers.Conv3D(inter_channel, kernel_size=1, padding='same')(attention)\n",
    "\n",
    "    concat_xg = tf.keras.layers.Add()([theta_x, phi_g])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv3D(1, kernel_size=1, padding='same')(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    \n",
    "    upsample_psi = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(sigmoid_xg)\n",
    "    scale_attention = tf.keras.layers.Multiply()([upsample_psi, inputs])\n",
    "\n",
    "    return scale_attention\n",
    "\n",
    "# Upsample with attention gates\n",
    "def upsample_attention_block(inputs, skip_connection, out_channels, normalization, mode):\n",
    "    # attention gate where we give special importance to features that are the most revelant\n",
    "    attention = attention_gate(skip_connection,inputs,out_channels//2)\n",
    "    inputs = convolution(inputs, kernel_size=3, out_channels=out_channels, stride=2,\n",
    "                         normalization='none', activation='none', transpose=True)\n",
    "\n",
    "    # Use dynamic crop and concat\n",
    "    #inputs = dynamic_crop_and_concat(inputs, skip_connection)\n",
    "    inputs = tf.keras.layers.Concatenate(axis=-1)([inputs, attention])\n",
    "    \n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    print('Input shape AFTER concatenate:',inputs.shape)\n",
    "    return inputs\n",
    "\n",
    "# Upsample with residual block\n",
    "def upsample_Residual_block(inputs, skip_connection, out_channels, normalization, mode, residual_kernel=1):\n",
    "    main_path = convolution(inputs, kernel_size=3, out_channels=out_channels, stride=2,\n",
    "                            normalization=normalization, activation='none', transpose=True)\n",
    "    main_path = tf.keras.layers.Concatenate(axis=-1)([main_path, skip_connection])\n",
    "    main_path = convolution(main_path, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    main_path = convolution(main_path, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "\n",
    "    # Residual path: Transposed convolution (or another upsampling technique) to match the dimensions\n",
    "    residual_path = convolution(inputs, out_channels=out_channels, kernel_size=residual_kernel, stride=2,\n",
    "                                normalization=normalization, mode=mode, activation='none', transpose=True)\n",
    "    \n",
    "    out = tf.keras.layers.Add()([main_path, residual_path])\n",
    "    out = _activation(out, 'relu')\n",
    "    return out\n",
    "\n",
    "# Upsample Layer with Residual block and attention gates\n",
    "def upsample_ResidualAttention_block(inputs, skip_connection,out_channels,normalization,mode):\n",
    "\n",
    "    attention = attention_gate(skip_connection,inputs,out_channels//2)\n",
    "\n",
    "    upsampled = convolution(inputs, kernel_size=3, out_channels=out_channels, stride=2,\n",
    "                         normalization='none', activation='none', transpose=True)\n",
    "    \n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([upsampled, attention])\n",
    "\n",
    "    # Convolutional path\n",
    "    conv_path = convolution(concat, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    conv_path = convolution(conv_path, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "\n",
    "    residual_path = convolution(inputs, out_channels=out_channels, kernel_size=1, stride=2,\n",
    "                                normalization=normalization, mode=mode, activation='none', transpose=True)\n",
    "    \n",
    "    out = tf.keras.layers.Add()([conv_path, residual_path])\n",
    "    out = _activation(out, 'relu')\n",
    "    return out\n",
    "\n",
    "# normal 3D U-Net upsample block\n",
    "def upsample_block(inputs, skip_connection, out_channels, normalization, mode):\n",
    "    inputs = convolution(inputs, kernel_size=3, out_channels=out_channels, stride=2,\n",
    "                         normalization='none', activation='none', transpose=True)\n",
    "\n",
    "    # Use dynamic crop and concat\n",
    "    #inputs = dynamic_crop_and_concat(inputs, skip_connection)\n",
    "    inputs = tf.keras.layers.Concatenate(axis=-1)([inputs, skip_connection])\n",
    "    \n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    inputs = convolution(inputs, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    return inputs\n",
    "\n",
    "def output_layer(inputs, out_channels, activation):\n",
    "    return convolution(inputs, out_channels=out_channels, kernel_size=3, normalization='none', activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "- param n_classes: Number of output channels\n",
    "- param mode: Estimator's execution mode\n",
    "- param normalization: Name of the normalization layer    \n",
    "- param features: Input features\n",
    "- return: Output of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####      Simple UNet3D build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet3d_mod(n_classes,mode,features,normalization='none'):\n",
    "        skip_1 = input_block(inputs=features,\n",
    "                               out_channels=16,\n",
    "                               normalization=normalization,\n",
    "                               mode=mode)\n",
    "\n",
    "        skip_2 = downsample_block(inputs=skip_1,\n",
    "                                   out_channels=32,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "\n",
    "        skip_3 = downsample_block(inputs=skip_2,\n",
    "                                   out_channels=64,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "        \n",
    "        skip_4 = downsample_block(inputs=skip_3,\n",
    "                                   out_channels=128,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "\n",
    "        # bridge\n",
    "        out = downsample_block(inputs=skip_4,\n",
    "                               out_channels=512,\n",
    "                               normalization=normalization,\n",
    "                               mode=mode)\n",
    "\n",
    "        out = tf.keras.layers.Dropout(params['dropout'])(out)\n",
    "\n",
    "        out = upsample_block(out, skip_4,\n",
    "                             out_channels=128,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "        \n",
    "        out = upsample_block(out, skip_3,\n",
    "                             out_channels=64,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_block(out, skip_2,\n",
    "                             out_channels=32,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_block(out, skip_1,\n",
    "                             out_channels=16,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        return output_layer(out,\n",
    "                            out_channels=n_classes,\n",
    "                            activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The features (input volumes) are represented as 5D tensor with the shape \n",
    "**[batch_size, depth, height, width, channels]**. Since medical volumes are often single-channel (grayscale), this last dimension might be 1.\n",
    "\n",
    "- The labels (masks), being a multi-class segmentation where each channel represents a different class, the channels dimension typically is the last dimension. So the masks are shaped as **[batch_size, depth, height, width, channels]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####    Residual 3D U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_unet3d(n_classes, mode, features, normalization='batchnorm'):\n",
    "    skip_connections = []\n",
    "    out_channels_sequence = [16,32,64,128]\n",
    "\n",
    "    # input block\n",
    "    features = input_block(inputs=features,\n",
    "                           out_channels=out_channels_sequence[0],\n",
    "                           normalization=normalization,\n",
    "                           mode=mode)\n",
    "    skip_connections.append(features)\n",
    "    \n",
    "    # downsampling blocks\n",
    "    for out_channels in out_channels_sequence[1:]:  # first one used on the input_block\n",
    "        features = downsample_Residual_block(inputs=features,\n",
    "                                             out_channels=out_channels,\n",
    "                                             normalization=normalization,\n",
    "                                             mode=mode)\n",
    "        skip_connections.append(features)\n",
    "    \n",
    "    # Bottom layer add residual to this one \n",
    "    out = downsample_Residual_block(inputs=features,\n",
    "                           out_channels=512,\n",
    "                           normalization=normalization,\n",
    "                           mode=mode)\n",
    "    \n",
    "    out = tf.keras.layers.Dropout(params['dropout'])(out)\n",
    "    \n",
    "    # upsampling blocks\n",
    "    for out_channels, skip_connection in zip(reversed(out_channels_sequence), reversed(skip_connections)):\n",
    "        out = upsample_Residual_block(inputs=out,\n",
    "                             skip_connection=skip_connection,\n",
    "                             out_channels=out_channels,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)       \n",
    "    # output layer\n",
    "    out = output_layer(out, out_channels=n_classes, activation='softmax')\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####    3D U-Net with Attention Gates on Decoder path\n",
    "\n",
    "During training, the weights also get trained making the model pay more attention to relevant regions. It adds weights to voxels based on the relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttUnet3d_1(n_classes, mode, features, normalization='batchnorm'):\n",
    "    skip_connections = []\n",
    "    out_channels_sequence = [16,32,64,128]\n",
    "\n",
    "    # input block\n",
    "    features = input_block(inputs=features,\n",
    "                           out_channels=out_channels_sequence[0],\n",
    "                           normalization=normalization,\n",
    "                           mode=mode)\n",
    "    skip_connections.append(features)\n",
    "    \n",
    "    # downsampling blocks\n",
    "    for out_channels in out_channels_sequence[1:]:  # first one used on the input_block\n",
    "        features = downsample_block(inputs=features,\n",
    "                                             out_channels=out_channels,\n",
    "                                             normalization=normalization,\n",
    "                                             mode=mode)\n",
    "        #features = tf.keras.layers.Dropout(params['dropout'])(features, training=mode==tf.estimator.ModeKeys.TRAIN)\n",
    "        skip_connections.append(features)\n",
    "    \n",
    "    # Bottom layer add residual to this one \n",
    "    bottleneck = downsample_block(inputs=features,\n",
    "                           out_channels=320,\n",
    "                           normalization=normalization,\n",
    "                           mode=mode)\n",
    "    \n",
    "    # upsampling blocks with attention mechanioms: attention gates\n",
    "    for out_channels, skip_connection in zip(reversed(out_channels_sequence), reversed(skip_connections)):\n",
    "        bottleneck = upsample_attention_block(inputs=bottleneck, skip_connection=skip_connection, out_channels=out_channels, normalization=normalization, mode=mode)\n",
    "    \n",
    "    # Output layer\n",
    "    output = output_layer(bottleneck, out_channels=n_classes, activation='softmax')\n",
    "    return output\n",
    "\n",
    "def AttUnet3d(n_classes, mode, features, normalization):\n",
    "        skip_1 = input_block(inputs=features,\n",
    "                               out_channels=16,\n",
    "                               normalization=normalization,\n",
    "                               mode=mode)\n",
    "        #skip_64\n",
    "        skip_2 = downsample_block(inputs=skip_1,\n",
    "                                   out_channels=32,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "        # skip_32\n",
    "        skip_3 = downsample_block(inputs=skip_2,\n",
    "                                   out_channels=64,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "        # skip_16\n",
    "        skip_4 = downsample_block(inputs=skip_3,\n",
    "                                   out_channels=128,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "\n",
    "        # bridge\n",
    "        out = downsample_block(inputs=skip_4,\n",
    "                               out_channels=512,\n",
    "                               normalization=normalization,\n",
    "                               mode=mode)\n",
    "        \n",
    "        out = tf.keras.layers.Dropout(params['dropout'])(out)\n",
    "\n",
    "\n",
    "        out = upsample_attention_block(out, skip_4,\n",
    "                             out_channels=128,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_attention_block(out, skip_3,\n",
    "                             out_channels=64,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_attention_block(out, skip_2,\n",
    "                             out_channels=32,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_attention_block(out, skip_1,\n",
    "                             out_channels=16,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        return output_layer(out,\n",
    "                            out_channels=n_classes,\n",
    "                            activation='softmax')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####    Residual Attention U-Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resAtt_unet3d(n_classes, mode, features, normalization='batchnorm'):\n",
    "        #skip_128\n",
    "        skip_1 = input_block(inputs=features,\n",
    "                               out_channels=16,\n",
    "                               normalization=normalization,\n",
    "                               mode=mode)\n",
    "        #skip_64\n",
    "        skip_2 = downsample_Residual_block(inputs=skip_1,\n",
    "                                   out_channels=32,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "        # skip_32\n",
    "        skip_3 = downsample_Residual_block(inputs=skip_2,\n",
    "                                   out_channels=64,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "        # skip_16\n",
    "        skip_4 = downsample_Residual_block(inputs=skip_3,\n",
    "                                   out_channels=128,\n",
    "                                   normalization=normalization,\n",
    "                                   mode=mode)\n",
    "\n",
    "        # bridge\n",
    "        out = downsample_Residual_block(inputs=skip_4,\n",
    "                               out_channels=512,\n",
    "                               normalization=normalization,\n",
    "                               mode=mode)\n",
    "\n",
    "\n",
    "        out = upsample_ResidualAttention_block(out, skip_4,\n",
    "                             out_channels=128,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_ResidualAttention_block(out, skip_3,\n",
    "                             out_channels=64,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_ResidualAttention_block(out, skip_2,\n",
    "                             out_channels=32,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        out = upsample_ResidualAttention_block(out, skip_1,\n",
    "                             out_channels=16,\n",
    "                             normalization=normalization,\n",
    "                             mode=mode)\n",
    "\n",
    "        return output_layer(out,\n",
    "                            out_channels=n_classes,\n",
    "                            activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Metrics for Landmark Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Categorical Loss entropy: perhaps good on 2D problems. \n",
    "- Dice Similarity Coefficient: Commonly used for segmentation tasks, it measures the spatial overlap between predicted and ground truth landmarks.\n",
    "- Mean Squared Error (MSE) between predicted and ground truth heatmaps can be used (Heatmap-based metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss(y_true, y_pred):\n",
    "    \"\"\" Factory method for loss functions\n",
    "\n",
    "    :param params: Dict with additional parameters\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: Loss\n",
    "    \"\"\"\n",
    "    if params['loss'] == 'dice':\n",
    "        return _dice(y_true, y_pred)\n",
    "    if params['loss'] == 'ce':\n",
    "        return _ce(y_true, y_pred)\n",
    "    if params['loss'] == 'dice+ce':\n",
    "        return tf.add(_ce(y_true, y_pred), _dice(y_true, y_pred), name=\"total_loss_ref\")\n",
    "    if params['loss'] == 'mse':\n",
    "        return _mse(y_true, y_pred)\n",
    "\n",
    "    raise ValueError('Unknown loss: {}'.format(params['loss']))\n",
    "\n",
    "\n",
    "def _ce(y_true, y_pred):\n",
    "    \"\"\" Crossentropy\n",
    "\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(\n",
    "        tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tf.cast(y_true, tf.float32), y_pred), axis=[0, 1, 2, 3]),\n",
    "        name='crossentropy_loss_ref')\n",
    "\n",
    "def _mse(y_true, y_pred):\n",
    "    \"\"\" Mean Squared Error loss.\n",
    "    \n",
    "    :param y_true: Ground truth labels.\n",
    "    :param y_pred: Predicted labels.\n",
    "    :return: MSE loss.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def _dice(y_true, y_pred):\n",
    "    \"\"\" Training dice\n",
    "\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(dice_loss(predictions=y_pred, targets=y_true), name='dice_loss_ref')\n",
    "\n",
    "def eval_dice(y_true, y_pred):\n",
    "    \"\"\" Evaluation dice\n",
    "\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    return eval_dice_loss(predictions=y_pred, targets=y_true)\n",
    "\n",
    "\n",
    "def eval_dice_loss(predictions,\n",
    "              targets,\n",
    "              squared_pred=False,\n",
    "              smooth=1e-5,\n",
    "              top_smooth=0.0):\n",
    "    \"\"\" Dice\n",
    "\n",
    "    :param predictions: Predicted labels\n",
    "    :param targets: Ground truth labels\n",
    "    :param squared_pred: Square the predicate\n",
    "    :param smooth: Smooth term for denominator\n",
    "    :param top_smooth: Smooth term for numerator\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    n_len = len(predictions.shape)\n",
    "    #print('n_len:',n_len)\n",
    "    reduce_axis = list(range(1, n_len))\n",
    "    #print(reduce_axis)\n",
    "    intersection = tf.reduce_sum(targets * predictions, axis=reduce_axis)\n",
    "\n",
    "    if squared_pred:\n",
    "        targets = tf.square(targets)\n",
    "        predictions = tf.square(predictions)\n",
    "\n",
    "    y_true_o = tf.reduce_sum(targets, axis=reduce_axis)\n",
    "    y_pred_o = tf.reduce_sum(predictions, axis=reduce_axis)\n",
    "\n",
    "    denominator = y_true_o + y_pred_o \n",
    "\n",
    "    dice = (2.0 * intersection + top_smooth) / (denominator + smooth)\n",
    "\n",
    "    return 1 - tf.reduce_mean(dice, axis=0)\n",
    "\n",
    "def dice_loss(predictions,\n",
    "              targets,\n",
    "              squared_pred=False,\n",
    "              smooth=1e-5,\n",
    "              top_smooth=0.0):\n",
    "    \"\"\" Dice\n",
    "\n",
    "    :param predictions: Predicted labels\n",
    "    :param targets: Ground truth labels\n",
    "    :param squared_pred: Square the predicate\n",
    "    :param smooth: Smooth term for denominator\n",
    "    :param top_smooth: Smooth term for numerator\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    is_channels_first = False\n",
    "\n",
    "    n_len = len(predictions.get_shape())\n",
    "    reduce_axis = list(range(2, n_len)) if is_channels_first else list(range(1, n_len - 1))\n",
    "    intersection = tf.reduce_sum(targets * predictions, axis=reduce_axis)\n",
    "\n",
    "    if squared_pred:\n",
    "        targets = tf.square(targets)\n",
    "        predictions = tf.square(predictions)\n",
    "\n",
    "    y_true_o = tf.reduce_sum(targets, axis=reduce_axis)\n",
    "    y_pred_o = tf.reduce_sum(predictions, axis=reduce_axis)\n",
    "\n",
    "    denominator = y_true_o + y_pred_o\n",
    "\n",
    "    dice = (2.0 * intersection + top_smooth) / (denominator + smooth)\n",
    "\n",
    "    return 1 - tf.reduce_mean(dice, axis=0)\n",
    "\n",
    "def total_dice(predictions,\n",
    "               targets,\n",
    "               smooth=1e-5,\n",
    "               top_smooth=0.0):\n",
    "    \"\"\" Total Dice\n",
    "\n",
    "    :param predictions: Predicted labels\n",
    "    :param targets: Ground truth labels\n",
    "    :param smooth: Smooth term for denominator\n",
    "    :param top_smooth: Smooth term for numerator\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    n_len = len(predictions.get_shape())\n",
    "    reduce_axis = list(range(1, n_len-1))\n",
    "    targets = tf.reduce_sum(targets, axis=-1)\n",
    "    predictions = tf.reduce_sum(predictions, axis=-1)\n",
    "    intersection = tf.reduce_sum(targets * predictions, axis=reduce_axis)\n",
    "\n",
    "    y_true_o = tf.reduce_sum(targets, axis=reduce_axis)\n",
    "    y_pred_o = tf.reduce_sum(predictions, axis=reduce_axis)\n",
    "\n",
    "    denominator = y_true_o + y_pred_o\n",
    "\n",
    "    return tf.reduce_mean((2.0 * intersection + top_smooth) / (denominator + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Methods\n",
    "\n",
    "- Avoiding overfitting\n",
    "- early stopping when validation accuracy no longer drops for a number of epochs\n",
    "- record session to be available on tensorboard. **cd into log directory and <: tensorboard --logdir (logs/path if not in lod directoy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminateOnNaN(Callback):\n",
    "    # terminates training when a NaN loss is encountered.\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print('Stopping training due to NaN loss')\n",
    "                self.model.stop_training = True\n",
    "\n",
    "\n",
    "class LearningCurveSaver(Callback):\n",
    "    def __init__(self, save_path):\n",
    "        super(LearningCurveSaver, self).__init__()\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        history = self.model.history.history\n",
    "        learning_curve_data = {\n",
    "            'loss': history['loss'],\n",
    "            'val_loss': history['val_loss']\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.save_path, 'learning_curve.json'), 'w') as f:\n",
    "            json.dump(learning_curve_data, f, indent=4)\n",
    "\n",
    "def prepareCallbacks(path,log_and_model_path):\n",
    "\n",
    "    file_path = f'{log_and_model_path}/{path}/cp.ckpt'\n",
    "    save_lr_path = f'{log_and_model_path}/{path}'\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath= file_path, \n",
    "                                monitor = 'val_loss',\n",
    "                                verbose=1, \n",
    "                                save_weights_only=True,\n",
    "                                save_best_only=True)\n",
    "\n",
    "\n",
    "    earlyStopper = EarlyStopping(monitor='val_loss', min_delta = 0.0001,\n",
    "                                patience = 14, \n",
    "                                verbose = 1)\n",
    "\n",
    "    tbCallBack = TensorBoard(log_dir=f'{log_and_model_path}/{path}_log', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "    # reducing \n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                  monitor='val_loss', factor=0.2, patience=8, min_lr=1e-6)\n",
    "    \n",
    "    # nan loss values monitor\n",
    "    terminateNan = TerminateOnNaN()\n",
    "\n",
    "    # store loss image\n",
    "    save_lr_path = f'{log_and_model_path}/{path}'\n",
    "    learnCurve = LearningCurveSaver(save_lr_path)\n",
    "\n",
    "    return file_path, [checkpointer, earlyStopper, tbCallBack, lr_scheduler,terminateNan,learnCurve] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Fit Approach - Dynamic Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input\n",
    "\n",
    "For test data lets get first the paths that we have at our disposal, this way we can get one subject and correctly compare the ground truth with the correspondent subject.\n",
    "\n",
    "Model learns to normalize input data consistently, applying the same transformation to both seen (training) and unseen (validation/testing) data, in terms of **Normalization**.\n",
    "\n",
    "Trying to use the same runned train_paths and val paths for the same model when testing different normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tuple paths: 209\n",
      "Number of validation tuple paths: 53\n",
      "Number of test tuple paths: 68\n",
      "Normalization parameters training: (0.0, 1072.0)\n",
      "WARNING:tensorflow:From c:\\Users\\Eduardo\\miniconda3\\envs\\dataortho_env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "# tfrecords\n",
    "train_paths,train_dynamic, val_dynamic,test_paths, norm_params = dataset_tfr_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (TensorSpec(shape=(2, 64, 256, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(2, 64, 256, 256, 19), dtype=tf.float32, name=None))\n",
      "Val: (TensorSpec(shape=(None, 64, 256, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 64, 256, 256, 19), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print('Train:',train_dynamic.element_spec)\n",
    "print('Val:',val_dynamic.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'augment': True,\n",
       " 'mode': 'train',\n",
       " 'seed': 42,\n",
       " 'subset': 'DATASET_DYNAMIC',\n",
       " 'interpolation': 'Linear Interpolation',\n",
       " 'normalization': 'groupnorm',\n",
       " 'total_train_samples': 209,\n",
       " 'total_val_samples': 53,\n",
       " 'lr': 0.0001,\n",
       " 'loss': 'dice+ce',\n",
       " 'dropout': 0.2,\n",
       " 'batch_size': 2,\n",
       " 'norm_params_minmax': (0.0, 1072.0),\n",
       " 'norm_params_meanstd': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['normalization'] = 'groupnorm'\n",
    "params['augment'] = True\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [MODEL] - Res_unet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs Directory: C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC\n",
      "Training session id: Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\n"
     ]
    }
   ],
   "source": [
    "logs_dynamic = refactor_path(os.path.join(logs_dir,params['interpolation'],params['subset']))\n",
    "# Define a unique path for this training session\n",
    "training_session_path = 'Sigma3/BatchNorm/residual_unet3d_{}'.format(datetime.datetime.now().strftime(\"(%Y-%m-%d)/%H.%M.%S\"))\n",
    "print('Logs Directory:',logs_dynamic)\n",
    "print('Training session id:', training_session_path)\n",
    "file_path, callbacks_list = prepareCallbacks(training_session_path, logs_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual_unet3d input: 19 train KerasTensor(type_spec=TensorSpec(shape=(None, 64, 256, 256, 1), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\") True batchnorm\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 64, 256, 25  0           []                               \n",
      "                                6, 1)]                                                            \n",
      "                                                                                                  \n",
      " conv3d_23 (Conv3D)             (None, 64, 256, 256  432         ['input_2[0][0]']                \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 64, 256, 256  64         ['conv3d_23[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 64, 256, 256  0           ['batch_normalization_30[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_24 (Conv3D)             (None, 64, 256, 256  6912        ['re_lu_26[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 64, 256, 256  64         ['conv3d_24[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 64, 256, 256  0           ['batch_normalization_31[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_25 (Conv3D)             (None, 32, 128, 128  13824       ['re_lu_27[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 32, 128, 128  128        ['conv3d_25[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 32, 128, 128  0           ['batch_normalization_32[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_26 (Conv3D)             (None, 32, 128, 128  27648       ['re_lu_28[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 32, 128, 128  128        ['conv3d_26[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_27 (Conv3D)             (None, 32, 128, 128  512         ['re_lu_27[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 32, 128, 128  0           ['batch_normalization_33[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 32, 128, 128  128        ['conv3d_27[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 128, 128  0           ['re_lu_29[0][0]',               \n",
      "                                , 32)                             'batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 32, 128, 128  0           ['add_8[0][0]']                  \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_28 (Conv3D)             (None, 16, 64, 64,   55296       ['re_lu_30[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 64, 64,   256        ['conv3d_28[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 16, 64, 64,   0           ['batch_normalization_35[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_29 (Conv3D)             (None, 16, 64, 64,   110592      ['re_lu_31[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 64, 64,   256        ['conv3d_29[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_30 (Conv3D)             (None, 16, 64, 64,   2048        ['re_lu_30[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 16, 64, 64,   0           ['batch_normalization_36[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16, 64, 64,   256        ['conv3d_30[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 64, 64,   0           ['re_lu_32[0][0]',               \n",
      "                                64)                               'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)                (None, 16, 64, 64,   0           ['add_9[0][0]']                  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_31 (Conv3D)             (None, 8, 32, 32, 1  221184      ['re_lu_33[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_31[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)                (None, 8, 32, 32, 1  0           ['batch_normalization_38[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_32 (Conv3D)             (None, 8, 32, 32, 1  442368      ['re_lu_34[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_32[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_33 (Conv3D)             (None, 8, 32, 32, 1  8192        ['re_lu_33[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)                (None, 8, 32, 32, 1  0           ['batch_normalization_39[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_33[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 8, 32, 32, 1  0           ['re_lu_35[0][0]',               \n",
      "                                28)                               'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)                (None, 8, 32, 32, 1  0           ['add_10[0][0]']                 \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_34 (Conv3D)             (None, 4, 16, 16, 5  1769472     ['re_lu_36[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 16, 16, 5  2048       ['conv3d_34[0][0]']              \n",
      " ormalization)                  12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)                (None, 4, 16, 16, 5  0           ['batch_normalization_41[0][0]'] \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_35 (Conv3D)             (None, 4, 16, 16, 5  7077888     ['re_lu_37[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 16, 16, 5  2048       ['conv3d_35[0][0]']              \n",
      " ormalization)                  12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_36 (Conv3D)             (None, 4, 16, 16, 5  65536       ['re_lu_36[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)                (None, 4, 16, 16, 5  0           ['batch_normalization_42[0][0]'] \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 16, 16, 5  2048       ['conv3d_36[0][0]']              \n",
      " ormalization)                  12)                                                               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4, 16, 16, 5  0           ['re_lu_38[0][0]',               \n",
      "                                12)                               'batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)                (None, 4, 16, 16, 5  0           ['add_11[0][0]']                 \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 16, 16, 5  0           ['re_lu_39[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_8 (Conv3DTran  (None, 8, 32, 32, 1  1769472    ['dropout_1[0][0]']              \n",
      " spose)                         28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_transpose_8[0][0]']     \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 8, 32, 32, 2  0           ['batch_normalization_44[0][0]', \n",
      "                                56)                               're_lu_36[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_37 (Conv3D)             (None, 8, 32, 32, 1  884736      ['concatenate_4[0][0]']          \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_37[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)                (None, 8, 32, 32, 1  0           ['batch_normalization_45[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_38 (Conv3D)             (None, 8, 32, 32, 1  442368      ['re_lu_40[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_38[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_9 (Conv3DTran  (None, 8, 32, 32, 1  65536      ['dropout_1[0][0]']              \n",
      " spose)                         28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)                (None, 8, 32, 32, 1  0           ['batch_normalization_46[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_transpose_9[0][0]']     \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 8, 32, 32, 1  0           ['re_lu_41[0][0]',               \n",
      "                                28)                               'batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)                (None, 8, 32, 32, 1  0           ['add_12[0][0]']                 \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_10 (Conv3DTra  (None, 16, 64, 64,   221184     ['re_lu_42[0][0]']               \n",
      " nspose)                        64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 64, 64,   256        ['conv3d_transpose_10[0][0]']    \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 16, 64, 64,   0           ['batch_normalization_48[0][0]', \n",
      "                                128)                              're_lu_33[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_39 (Conv3D)             (None, 16, 64, 64,   221184      ['concatenate_5[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 64, 64,   256        ['conv3d_39[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)                (None, 16, 64, 64,   0           ['batch_normalization_49[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_40 (Conv3D)             (None, 16, 64, 64,   110592      ['re_lu_43[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 16, 64, 64,   256        ['conv3d_40[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_11 (Conv3DTra  (None, 16, 64, 64,   8192       ['re_lu_42[0][0]']               \n",
      " nspose)                        64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_44 (ReLU)                (None, 16, 64, 64,   0           ['batch_normalization_50[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 16, 64, 64,   256        ['conv3d_transpose_11[0][0]']    \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 64, 64,   0           ['re_lu_44[0][0]',               \n",
      "                                64)                               'batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_45 (ReLU)                (None, 16, 64, 64,   0           ['add_13[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_12 (Conv3DTra  (None, 32, 128, 128  55296      ['re_lu_45[0][0]']               \n",
      " nspose)                        , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 32, 128, 128  128        ['conv3d_transpose_12[0][0]']    \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 128, 128  0           ['batch_normalization_52[0][0]', \n",
      "                                , 64)                             're_lu_30[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_41 (Conv3D)             (None, 32, 128, 128  55296       ['concatenate_6[0][0]']          \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 32, 128, 128  128        ['conv3d_41[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_46 (ReLU)                (None, 32, 128, 128  0           ['batch_normalization_53[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_42 (Conv3D)             (None, 32, 128, 128  27648       ['re_lu_46[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 32, 128, 128  128        ['conv3d_42[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_transpose_13 (Conv3DTra  (None, 32, 128, 128  2048       ['re_lu_45[0][0]']               \n",
      " nspose)                        , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_47 (ReLU)                (None, 32, 128, 128  0           ['batch_normalization_54[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 32, 128, 128  128        ['conv3d_transpose_13[0][0]']    \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 32, 128, 128  0           ['re_lu_47[0][0]',               \n",
      "                                , 32)                             'batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_48 (ReLU)                (None, 32, 128, 128  0           ['add_14[0][0]']                 \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_transpose_14 (Conv3DTra  (None, 64, 256, 256  13824      ['re_lu_48[0][0]']               \n",
      " nspose)                        , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 64, 256, 256  64         ['conv3d_transpose_14[0][0]']    \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 64, 256, 256  0           ['batch_normalization_56[0][0]', \n",
      "                                , 32)                             're_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_43 (Conv3D)             (None, 64, 256, 256  13824       ['concatenate_7[0][0]']          \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 64, 256, 256  64         ['conv3d_43[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_49 (ReLU)                (None, 64, 256, 256  0           ['batch_normalization_57[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_44 (Conv3D)             (None, 64, 256, 256  6912        ['re_lu_49[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 64, 256, 256  64         ['conv3d_44[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_transpose_15 (Conv3DTra  (None, 64, 256, 256  512        ['re_lu_48[0][0]']               \n",
      " nspose)                        , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_50 (ReLU)                (None, 64, 256, 256  0           ['batch_normalization_58[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 64, 256, 256  64         ['conv3d_transpose_15[0][0]']    \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 64, 256, 256  0           ['re_lu_50[0][0]',               \n",
      "                                , 16)                             'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_51 (ReLU)                (None, 64, 256, 256  0           ['add_15[0][0]']                 \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_45 (Conv3D)             (None, 64, 256, 256  8227        ['re_lu_51[0][0]']               \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 64, 256, 256  0           ['conv3d_45[0][0]']              \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,721,555\n",
      "Trainable params: 13,715,155\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initial input features\n",
    "features = tf.keras.layers.Input(shape=input_shape[params['subset']])\n",
    "print('residual_unet3d input:',yshape[params['subset']][-1],params['mode'],features,params['augment'],params['normalization'])\n",
    "output = residual_unet3d(yshape[params['subset']][-1],params['mode'],features,params['normalization'])\n",
    "res_unet_model = tf.keras.Model(inputs=features, outputs=output)\n",
    "res_unet_model.compile(optimizer=Adam(learning_rate=params['lr']), # type: ignore\n",
    "                   loss=make_loss, \n",
    "                   metrics=[eval_dice])\n",
    "res_unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss options from make_loss to _dice to 'categorical_entropy', etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "      6/Unknown - 204s 23s/step - loss: 22.5260 - eval_dice: 0.9502WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4194s vs `on_train_batch_end` time: 18.6873s). Check your callbacks.\n",
      "    104/Unknown - 4293s 41s/step - loss: 20.2311 - eval_dice: 0.6814\n",
      "Epoch 1: val_loss improved from inf to 18.66487, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5472s 52s/step - loss: 20.2311 - eval_dice: 0.6814 - val_loss: 18.6649 - val_eval_dice: 0.3242 - lr: 1.0000e-04\n",
      "Epoch 2/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.2190 - eval_dice: 0.1327 \n",
      "Epoch 2: val_loss improved from 18.66487 to 18.08116, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5108s 49s/step - loss: 18.2190 - eval_dice: 0.1327 - val_loss: 18.0812 - val_eval_dice: 0.0673 - lr: 1.0000e-04\n",
      "Epoch 3/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.9131 - eval_dice: 0.0535 \n",
      "Epoch 3: val_loss improved from 18.08116 to 17.79660, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5115s 49s/step - loss: 17.9131 - eval_dice: 0.0535 - val_loss: 17.7966 - val_eval_dice: 0.0276 - lr: 1.0000e-04\n",
      "Epoch 4/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.4475 - eval_dice: 0.0322 \n",
      "Epoch 4: val_loss improved from 17.79660 to 17.36290, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5103s 49s/step - loss: 17.4475 - eval_dice: 0.0322 - val_loss: 17.3629 - val_eval_dice: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 5/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 16.7531 - eval_dice: 0.0263 \n",
      "Epoch 5: val_loss improved from 17.36290 to 16.83733, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5080s 49s/step - loss: 16.7531 - eval_dice: 0.0263 - val_loss: 16.8373 - val_eval_dice: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 6/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 15.9191 - eval_dice: 0.0263 \n",
      "Epoch 6: val_loss improved from 16.83733 to 16.28652, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5103s 49s/step - loss: 15.9191 - eval_dice: 0.0263 - val_loss: 16.2865 - val_eval_dice: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 7/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 15.2436 - eval_dice: 0.0242 \n",
      "Epoch 7: val_loss improved from 16.28652 to 15.84509, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5107s 49s/step - loss: 15.2436 - eval_dice: 0.0242 - val_loss: 15.8451 - val_eval_dice: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 8/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.8350 - eval_dice: 0.0261 \n",
      "Epoch 8: val_loss did not improve from 15.84509\n",
      "104/104 [==============================] - 5104s 49s/step - loss: 14.8350 - eval_dice: 0.0261 - val_loss: 15.8487 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 9/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.5791 - eval_dice: 0.0259 \n",
      "Epoch 9: val_loss improved from 15.84509 to 15.60569, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 4974s 48s/step - loss: 14.5791 - eval_dice: 0.0259 - val_loss: 15.6057 - val_eval_dice: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 10/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.2832 - eval_dice: 0.0247 \n",
      "Epoch 10: val_loss improved from 15.60569 to 15.39079, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 2469s 24s/step - loss: 14.2832 - eval_dice: 0.0247 - val_loss: 15.3908 - val_eval_dice: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 11/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.0288 - eval_dice: 0.0247 \n",
      "Epoch 11: val_loss improved from 15.39079 to 15.31313, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 2475s 24s/step - loss: 14.0288 - eval_dice: 0.0247 - val_loss: 15.3131 - val_eval_dice: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 12/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.6467 - eval_dice: 0.0257 \n",
      "Epoch 12: val_loss improved from 15.31313 to 15.06541, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 3040s 29s/step - loss: 13.6467 - eval_dice: 0.0257 - val_loss: 15.0654 - val_eval_dice: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 13/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.3271 - eval_dice: 0.0260 \n",
      "Epoch 13: val_loss improved from 15.06541 to 14.85597, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5106s 49s/step - loss: 13.3271 - eval_dice: 0.0260 - val_loss: 14.8560 - val_eval_dice: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 14/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.1251 - eval_dice: 0.0252 \n",
      "Epoch 14: val_loss improved from 14.85597 to 14.61371, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5095s 49s/step - loss: 13.1251 - eval_dice: 0.0252 - val_loss: 14.6137 - val_eval_dice: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 15/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.9893 - eval_dice: 0.0254 \n",
      "Epoch 15: val_loss did not improve from 14.61371\n",
      "104/104 [==============================] - 5092s 49s/step - loss: 12.9893 - eval_dice: 0.0254 - val_loss: 14.8453 - val_eval_dice: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 16/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.9862 - eval_dice: 0.0257 \n",
      "Epoch 16: val_loss did not improve from 14.61371\n",
      "104/104 [==============================] - 5100s 49s/step - loss: 12.9862 - eval_dice: 0.0257 - val_loss: 14.7601 - val_eval_dice: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 17/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.8771 - eval_dice: 0.0265 \n",
      "Epoch 17: val_loss improved from 14.61371 to 14.56465, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 4265s 41s/step - loss: 12.8771 - eval_dice: 0.0265 - val_loss: 14.5646 - val_eval_dice: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 18/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.8397 - eval_dice: 0.0270 \n",
      "Epoch 18: val_loss improved from 14.56465 to 14.46711, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 3645s 35s/step - loss: 12.8397 - eval_dice: 0.0270 - val_loss: 14.4671 - val_eval_dice: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 19/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.7761 - eval_dice: 0.0282 \n",
      "Epoch 19: val_loss improved from 14.46711 to 14.28164, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 3656s 35s/step - loss: 12.7761 - eval_dice: 0.0282 - val_loss: 14.2816 - val_eval_dice: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 20/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.5857 - eval_dice: 0.0256 \n",
      "Epoch 20: val_loss did not improve from 14.28164\n",
      "104/104 [==============================] - 4922s 47s/step - loss: 12.5857 - eval_dice: 0.0256 - val_loss: 14.3272 - val_eval_dice: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 21/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.4931 - eval_dice: 0.0261 \n",
      "Epoch 21: val_loss did not improve from 14.28164\n",
      "104/104 [==============================] - 4934s 47s/step - loss: 12.4931 - eval_dice: 0.0261 - val_loss: 14.3845 - val_eval_dice: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 22/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.4611 - eval_dice: 0.0252 \n",
      "Epoch 22: val_loss improved from 14.28164 to 14.25466, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 4932s 47s/step - loss: 12.4611 - eval_dice: 0.0252 - val_loss: 14.2547 - val_eval_dice: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 23/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.4400 - eval_dice: 0.0268 \n",
      "Epoch 23: val_loss did not improve from 14.25466\n",
      "104/104 [==============================] - 4931s 47s/step - loss: 12.4400 - eval_dice: 0.0268 - val_loss: 14.3222 - val_eval_dice: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 24/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.4076 - eval_dice: 0.0259 \n",
      "Epoch 24: val_loss improved from 14.25466 to 14.08475, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 4947s 47s/step - loss: 12.4076 - eval_dice: 0.0259 - val_loss: 14.0848 - val_eval_dice: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 25/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.2469 - eval_dice: 0.0256 \n",
      "Epoch 25: val_loss did not improve from 14.08475\n",
      "104/104 [==============================] - 4946s 47s/step - loss: 12.2469 - eval_dice: 0.0256 - val_loss: 14.1963 - val_eval_dice: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 26/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.8091 - eval_dice: 0.0241 \n",
      "Epoch 26: val_loss improved from 14.08475 to 13.76348, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 4908s 47s/step - loss: 11.8091 - eval_dice: 0.0241 - val_loss: 13.7635 - val_eval_dice: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 27/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5741 - eval_dice: 0.0225 \n",
      "Epoch 27: val_loss improved from 13.76348 to 13.63141, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 4932s 47s/step - loss: 11.5741 - eval_dice: 0.0225 - val_loss: 13.6314 - val_eval_dice: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 28/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5035 - eval_dice: 0.0239 \n",
      "Epoch 28: val_loss did not improve from 13.63141\n",
      "104/104 [==============================] - 4948s 47s/step - loss: 11.5035 - eval_dice: 0.0239 - val_loss: 13.8641 - val_eval_dice: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 29/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4837 - eval_dice: 0.0249 \n",
      "Epoch 29: val_loss improved from 13.63141 to 13.61348, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5246s 50s/step - loss: 11.4837 - eval_dice: 0.0249 - val_loss: 13.6135 - val_eval_dice: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 30/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4552 - eval_dice: 0.0237 \n",
      "Epoch 30: val_loss did not improve from 13.61348\n",
      "104/104 [==============================] - 4935s 47s/step - loss: 11.4552 - eval_dice: 0.0237 - val_loss: 13.6625 - val_eval_dice: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 31/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3650 - eval_dice: 0.0228 \n",
      "Epoch 31: val_loss did not improve from 13.61348\n",
      "104/104 [==============================] - 4937s 47s/step - loss: 11.3650 - eval_dice: 0.0228 - val_loss: 13.6661 - val_eval_dice: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 32/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3356 - eval_dice: 0.0233 \n",
      "Epoch 32: val_loss improved from 13.61348 to 13.41211, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5302s 51s/step - loss: 11.3356 - eval_dice: 0.0233 - val_loss: 13.4121 - val_eval_dice: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 33/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2667 - eval_dice: 0.0243 \n",
      "Epoch 33: val_loss did not improve from 13.41211\n",
      "104/104 [==============================] - 5209s 50s/step - loss: 11.2667 - eval_dice: 0.0243 - val_loss: 13.6877 - val_eval_dice: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 34/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2508 - eval_dice: 0.0255 \n",
      "Epoch 34: val_loss improved from 13.41211 to 13.38884, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5217s 50s/step - loss: 11.2508 - eval_dice: 0.0255 - val_loss: 13.3888 - val_eval_dice: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 35/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2852 - eval_dice: 0.0255 \n",
      "Epoch 35: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5247s 50s/step - loss: 11.2852 - eval_dice: 0.0255 - val_loss: 13.5563 - val_eval_dice: 0.0193 - lr: 1.0000e-04\n",
      "Epoch 36/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2557 - eval_dice: 0.0245 \n",
      "Epoch 36: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5194s 50s/step - loss: 11.2557 - eval_dice: 0.0245 - val_loss: 13.7281 - val_eval_dice: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 37/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2353 - eval_dice: 0.0233 \n",
      "Epoch 37: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5196s 50s/step - loss: 11.2353 - eval_dice: 0.0233 - val_loss: 13.8016 - val_eval_dice: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 38/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2170 - eval_dice: 0.0243 \n",
      "Epoch 38: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5493s 53s/step - loss: 11.2170 - eval_dice: 0.0243 - val_loss: 13.5765 - val_eval_dice: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 39/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2123 - eval_dice: 0.0238 \n",
      "Epoch 39: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5207s 50s/step - loss: 11.2123 - eval_dice: 0.0238 - val_loss: 13.5986 - val_eval_dice: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 40/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2058 - eval_dice: 0.0242 \n",
      "Epoch 40: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5226s 50s/step - loss: 11.2058 - eval_dice: 0.0242 - val_loss: 13.5597 - val_eval_dice: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 41/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1865 - eval_dice: 0.0233 \n",
      "Epoch 41: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5194s 50s/step - loss: 11.1865 - eval_dice: 0.0233 - val_loss: 13.5755 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 42/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1715 - eval_dice: 0.0228 \n",
      "Epoch 42: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5190s 50s/step - loss: 11.1715 - eval_dice: 0.0228 - val_loss: 13.8544 - val_eval_dice: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 43/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1486 - eval_dice: 0.0227 \n",
      "Epoch 43: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5197s 50s/step - loss: 11.1486 - eval_dice: 0.0227 - val_loss: 13.4260 - val_eval_dice: 0.0150 - lr: 2.0000e-05\n",
      "Epoch 44/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1149 - eval_dice: 0.0221 \n",
      "Epoch 44: val_loss did not improve from 13.38884\n",
      "104/104 [==============================] - 5183s 50s/step - loss: 11.1149 - eval_dice: 0.0221 - val_loss: 13.4025 - val_eval_dice: 0.0147 - lr: 2.0000e-05\n",
      "Epoch 45/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1155 - eval_dice: 0.0224 \n",
      "Epoch 45: val_loss improved from 13.38884 to 13.33571, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5210s 50s/step - loss: 11.1155 - eval_dice: 0.0224 - val_loss: 13.3357 - val_eval_dice: 0.0147 - lr: 2.0000e-05\n",
      "Epoch 46/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1155 - eval_dice: 0.0238 \n",
      "Epoch 46: val_loss improved from 13.33571 to 13.29588, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5215s 50s/step - loss: 11.1155 - eval_dice: 0.0238 - val_loss: 13.2959 - val_eval_dice: 0.0149 - lr: 2.0000e-05\n",
      "Epoch 47/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1085 - eval_dice: 0.0227 \n",
      "Epoch 47: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5623s 54s/step - loss: 11.1085 - eval_dice: 0.0227 - val_loss: 13.3180 - val_eval_dice: 0.0155 - lr: 2.0000e-05\n",
      "Epoch 48/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1120 - eval_dice: 0.0244 \n",
      "Epoch 48: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5222s 50s/step - loss: 11.1120 - eval_dice: 0.0244 - val_loss: 13.3633 - val_eval_dice: 0.0163 - lr: 2.0000e-05\n",
      "Epoch 49/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1103 - eval_dice: 0.0244 \n",
      "Epoch 49: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5215s 50s/step - loss: 11.1103 - eval_dice: 0.0244 - val_loss: 13.3439 - val_eval_dice: 0.0157 - lr: 2.0000e-05\n",
      "Epoch 50/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1041 - eval_dice: 0.0225 \n",
      "Epoch 50: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5187s 50s/step - loss: 11.1041 - eval_dice: 0.0225 - val_loss: 13.3798 - val_eval_dice: 0.0153 - lr: 2.0000e-05\n",
      "Epoch 51/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1048 - eval_dice: 0.0219 \n",
      "Epoch 51: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5218s 50s/step - loss: 11.1048 - eval_dice: 0.0219 - val_loss: 13.3196 - val_eval_dice: 0.0147 - lr: 2.0000e-05\n",
      "Epoch 52/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0979 - eval_dice: 0.0229 \n",
      "Epoch 52: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5199s 50s/step - loss: 11.0979 - eval_dice: 0.0229 - val_loss: 13.3559 - val_eval_dice: 0.0151 - lr: 2.0000e-05\n",
      "Epoch 53/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1000 - eval_dice: 0.0233 \n",
      "Epoch 53: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5185s 50s/step - loss: 11.1000 - eval_dice: 0.0233 - val_loss: 13.3088 - val_eval_dice: 0.0159 - lr: 2.0000e-05\n",
      "Epoch 54/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0963 - eval_dice: 0.0238 \n",
      "Epoch 54: val_loss did not improve from 13.29588\n",
      "104/104 [==============================] - 5738s 55s/step - loss: 11.0963 - eval_dice: 0.0238 - val_loss: 13.3365 - val_eval_dice: 0.0153 - lr: 2.0000e-05\n",
      "Epoch 55/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0993 - eval_dice: 0.0234 \n",
      "Epoch 55: val_loss improved from 13.29588 to 13.27772, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/residual_unet3d_(2024-07-22)/00.18.43\\cp.ckpt\n",
      "104/104 [==============================] - 5447s 52s/step - loss: 11.0993 - eval_dice: 0.0234 - val_loss: 13.2777 - val_eval_dice: 0.0157 - lr: 4.0000e-06\n",
      "Epoch 56/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0962 - eval_dice: 0.0237 \n",
      "Epoch 56: val_loss did not improve from 13.27772\n",
      "104/104 [==============================] - 5455s 52s/step - loss: 11.0962 - eval_dice: 0.0237 - val_loss: 13.3118 - val_eval_dice: 0.0164 - lr: 4.0000e-06\n",
      "Epoch 57/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0931 - eval_dice: 0.0236 \n",
      "Epoch 57: val_loss did not improve from 13.27772\n",
      "104/104 [==============================] - 5426s 52s/step - loss: 11.0931 - eval_dice: 0.0236 - val_loss: 13.3010 - val_eval_dice: 0.0161 - lr: 4.0000e-06\n",
      "Epoch 58/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0986 - eval_dice: 0.0257 \n",
      "Epoch 58: val_loss did not improve from 13.27772\n",
      "104/104 [==============================] - 5441s 52s/step - loss: 11.0986 - eval_dice: 0.0257 - val_loss: 13.2878 - val_eval_dice: 0.0167 - lr: 4.0000e-06\n",
      "Epoch 59/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0908 - eval_dice: 0.0229 \n",
      "Epoch 59: val_loss did not improve from 13.27772\n",
      "104/104 [==============================] - 5418s 52s/step - loss: 11.0908 - eval_dice: 0.0229 - val_loss: 13.2955 - val_eval_dice: 0.0165 - lr: 4.0000e-06\n",
      "Epoch 60/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0954 - eval_dice: 0.0236 \n",
      "Epoch 60: val_loss did not improve from 13.27772\n",
      "104/104 [==============================] - 5421s 52s/step - loss: 11.0954 - eval_dice: 0.0236 - val_loss: 13.3219 - val_eval_dice: 0.0167 - lr: 4.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history1 = res_unet_model.fit(\n",
    "                train_dynamic,\n",
    "                epochs=60,\n",
    "                validation_data=val_dynamic,\n",
    "                callbacks=callbacks_list  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [MODEL] - U-Net3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs Directory: C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC\n",
      "Training session id: Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\n"
     ]
    }
   ],
   "source": [
    "logs_sagittal = refactor_path(os.path.join(logs_dir,params['interpolation'],params['subset']))\n",
    "training_session_path = 'Sigma3/GroupNorm/unet3d-{}'.format(datetime.datetime.now().strftime(\"(%Y-%m-%d)/%H.%M.%S\"))\n",
    "print('Logs Directory:',logs_sagittal)\n",
    "print('Training session id:', training_session_path)\n",
    "file_path, callbacks_list = prepareCallbacks(training_session_path, logs_sagittal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet3d input: 19 train KerasTensor(type_spec=TensorSpec(shape=(None, 64, 256, 256, 1), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\") True\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 64, 256, 25  0           []                               \n",
      "                                6, 1)]                                                            \n",
      "                                                                                                  \n",
      " conv3d_35 (Conv3D)             (None, 64, 256, 256  432         ['input_3[0][0]']                \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization (GroupNorm  (None, 64, 256, 256  32         ['conv3d_35[0][0]']              \n",
      " alization)                     , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 64, 256, 256  0           ['group_normalization[0][0]']    \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_36 (Conv3D)             (None, 64, 256, 256  6912        ['re_lu_26[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_1 (GroupNo  (None, 64, 256, 256  32         ['conv3d_36[0][0]']              \n",
      " rmalization)                   , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_1[0][0]']  \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_37 (Conv3D)             (None, 32, 128, 128  13824       ['re_lu_27[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_2 (GroupNo  (None, 32, 128, 128  64         ['conv3d_37[0][0]']              \n",
      " rmalization)                   , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_2[0][0]']  \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_38 (Conv3D)             (None, 32, 128, 128  27648       ['re_lu_28[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_3 (GroupNo  (None, 32, 128, 128  64         ['conv3d_38[0][0]']              \n",
      " rmalization)                   , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_3[0][0]']  \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_39 (Conv3D)             (None, 16, 64, 64,   55296       ['re_lu_29[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_4 (GroupNo  (None, 16, 64, 64,   128        ['conv3d_39[0][0]']              \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_4[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_40 (Conv3D)             (None, 16, 64, 64,   110592      ['re_lu_30[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_5 (GroupNo  (None, 16, 64, 64,   128        ['conv3d_40[0][0]']              \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_5[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_41 (Conv3D)             (None, 8, 32, 32, 1  221184      ['re_lu_31[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_6 (GroupNo  (None, 8, 32, 32, 1  256        ['conv3d_41[0][0]']              \n",
      " rmalization)                   28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_6[0][0]']  \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_42 (Conv3D)             (None, 8, 32, 32, 1  442368      ['re_lu_32[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_7 (GroupNo  (None, 8, 32, 32, 1  256        ['conv3d_42[0][0]']              \n",
      " rmalization)                   28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_7[0][0]']  \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_43 (Conv3D)             (None, 4, 16, 16, 5  1769472     ['re_lu_33[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_8 (GroupNo  (None, 4, 16, 16, 5  1024       ['conv3d_43[0][0]']              \n",
      " rmalization)                   12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)                (None, 4, 16, 16, 5  0           ['group_normalization_8[0][0]']  \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_44 (Conv3D)             (None, 4, 16, 16, 5  7077888     ['re_lu_34[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_9 (GroupNo  (None, 4, 16, 16, 5  1024       ['conv3d_44[0][0]']              \n",
      " rmalization)                   12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)                (None, 4, 16, 16, 5  0           ['group_normalization_9[0][0]']  \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 16, 16, 5  0           ['re_lu_35[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_8 (Conv3DTran  (None, 8, 32, 32, 1  1769600    ['dropout[0][0]']                \n",
      " spose)                         28)                                                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 8, 32, 32, 2  0           ['conv3d_transpose_8[0][0]',     \n",
      "                                56)                               're_lu_33[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_45 (Conv3D)             (None, 8, 32, 32, 1  884736      ['concatenate_4[0][0]']          \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_10 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_45[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_10[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_46 (Conv3D)             (None, 8, 32, 32, 1  442368      ['re_lu_36[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_11 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_46[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_11[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_9 (Conv3DTran  (None, 16, 64, 64,   221248     ['re_lu_37[0][0]']               \n",
      " spose)                         64)                                                               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 16, 64, 64,   0           ['conv3d_transpose_9[0][0]',     \n",
      "                                128)                              're_lu_31[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_47 (Conv3D)             (None, 16, 64, 64,   221184      ['concatenate_5[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_12 (GroupN  (None, 16, 64, 64,   128        ['conv3d_47[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_12[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_48 (Conv3D)             (None, 16, 64, 64,   110592      ['re_lu_38[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_13 (GroupN  (None, 16, 64, 64,   128        ['conv3d_48[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_13[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_10 (Conv3DTra  (None, 32, 128, 128  55328      ['re_lu_39[0][0]']               \n",
      " nspose)                        , 32)                                                             \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 128, 128  0           ['conv3d_transpose_10[0][0]',    \n",
      "                                , 64)                             're_lu_29[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_49 (Conv3D)             (None, 32, 128, 128  55296       ['concatenate_6[0][0]']          \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_14 (GroupN  (None, 32, 128, 128  64         ['conv3d_49[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_14[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_50 (Conv3D)             (None, 32, 128, 128  27648       ['re_lu_40[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_15 (GroupN  (None, 32, 128, 128  64         ['conv3d_50[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_15[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_transpose_11 (Conv3DTra  (None, 64, 256, 256  13840      ['re_lu_41[0][0]']               \n",
      " nspose)                        , 16)                                                             \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 64, 256, 256  0           ['conv3d_transpose_11[0][0]',    \n",
      "                                , 32)                             're_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_51 (Conv3D)             (None, 64, 256, 256  13824       ['concatenate_7[0][0]']          \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_16 (GroupN  (None, 64, 256, 256  32         ['conv3d_51[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_16[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_52 (Conv3D)             (None, 64, 256, 256  6912        ['re_lu_42[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_17 (GroupN  (None, 64, 256, 256  32         ['conv3d_52[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_17[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_53 (Conv3D)             (None, 64, 256, 256  8227        ['re_lu_43[0][0]']               \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 64, 256, 256  0           ['conv3d_53[0][0]']              \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,560,387\n",
      "Trainable params: 13,560,387\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initial input features\n",
    "features = tf.keras.layers.Input(shape=input_shape[params['subset']])\n",
    "print('unet3d input:',yshape[params['subset']][-1],params['mode'],features,params['augment'])\n",
    "output = unet3d_mod(yshape[params['subset']][-1],params['mode'],features,params['normalization'])\n",
    "unet3d_model = tf.keras.Model(inputs=features, outputs=output)\n",
    "unet3d_model.compile(optimizer=Adam(learning_rate=params['lr']),\n",
    "                   loss=make_loss, \n",
    "                   metrics=[eval_dice])\n",
    "unet3d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training session id: Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14 2 groupnorm\n",
      "Epoch 1/60\n",
      "      6/Unknown - 32s 2s/step - loss: 22.0958 - eval_dice: 0.9506WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6946s vs `on_train_batch_end` time: 1.0295s). Check your callbacks.\n",
      "    104/Unknown - 207s 2s/step - loss: 20.1677 - eval_dice: 0.7102\n",
      "Epoch 1: val_loss improved from inf to 18.68508, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 245s 2s/step - loss: 20.1677 - eval_dice: 0.7102 - val_loss: 18.6851 - val_eval_dice: 0.3529 - lr: 1.0000e-04\n",
      "Epoch 2/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.3224 - eval_dice: 0.1922\n",
      "Epoch 2: val_loss improved from 18.68508 to 18.16143, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 18.3224 - eval_dice: 0.1922 - val_loss: 18.1614 - val_eval_dice: 0.1076 - lr: 1.0000e-04\n",
      "Epoch 3/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.0514 - eval_dice: 0.0834\n",
      "Epoch 3: val_loss improved from 18.16143 to 18.00872, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 233s 2s/step - loss: 18.0514 - eval_dice: 0.0834 - val_loss: 18.0087 - val_eval_dice: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 4/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.8847 - eval_dice: 0.0495\n",
      "Epoch 4: val_loss improved from 18.00872 to 17.87551, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 237s 2s/step - loss: 17.8847 - eval_dice: 0.0495 - val_loss: 17.8755 - val_eval_dice: 0.0303 - lr: 1.0000e-04\n",
      "Epoch 5/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.7156 - eval_dice: 0.0359\n",
      "Epoch 5: val_loss improved from 17.87551 to 17.72705, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 242s 2s/step - loss: 17.7156 - eval_dice: 0.0359 - val_loss: 17.7271 - val_eval_dice: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 6/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.3288 - eval_dice: 0.0305\n",
      "Epoch 6: val_loss improved from 17.72705 to 17.46769, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 17.3288 - eval_dice: 0.0305 - val_loss: 17.4677 - val_eval_dice: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 7/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 16.5181 - eval_dice: 0.0281\n",
      "Epoch 7: val_loss improved from 17.46769 to 16.96755, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 244s 2s/step - loss: 16.5181 - eval_dice: 0.0281 - val_loss: 16.9675 - val_eval_dice: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 8/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 15.4102 - eval_dice: 0.0283\n",
      "Epoch 8: val_loss improved from 16.96755 to 16.26853, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 246s 2s/step - loss: 15.4102 - eval_dice: 0.0283 - val_loss: 16.2685 - val_eval_dice: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 9/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.2460 - eval_dice: 0.0248\n",
      "Epoch 9: val_loss improved from 16.26853 to 15.54811, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 237s 2s/step - loss: 14.2460 - eval_dice: 0.0248 - val_loss: 15.5481 - val_eval_dice: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 10/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.5497 - eval_dice: 0.0244\n",
      "Epoch 10: val_loss improved from 15.54811 to 15.25077, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 13.5497 - eval_dice: 0.0244 - val_loss: 15.2508 - val_eval_dice: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 11/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.1323 - eval_dice: 0.0215\n",
      "Epoch 11: val_loss did not improve from 15.25077\n",
      "104/104 [==============================] - 238s 2s/step - loss: 13.1323 - eval_dice: 0.0215 - val_loss: 15.2816 - val_eval_dice: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 12/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.9241 - eval_dice: 0.0237\n",
      "Epoch 12: val_loss improved from 15.25077 to 14.89662, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 240s 2s/step - loss: 12.9241 - eval_dice: 0.0237 - val_loss: 14.8966 - val_eval_dice: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 13/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.6178 - eval_dice: 0.0238\n",
      "Epoch 13: val_loss improved from 14.89662 to 14.73305, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 12.6178 - eval_dice: 0.0238 - val_loss: 14.7330 - val_eval_dice: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 14/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.3215 - eval_dice: 0.0234\n",
      "Epoch 14: val_loss improved from 14.73305 to 14.31841, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 12.3215 - eval_dice: 0.0234 - val_loss: 14.3184 - val_eval_dice: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 15/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.9639 - eval_dice: 0.0225\n",
      "Epoch 15: val_loss did not improve from 14.31841\n",
      "104/104 [==============================] - 238s 2s/step - loss: 11.9639 - eval_dice: 0.0225 - val_loss: 14.4943 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 16/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.8317 - eval_dice: 0.0239\n",
      "Epoch 16: val_loss improved from 14.31841 to 14.14976, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 248s 2s/step - loss: 11.8317 - eval_dice: 0.0239 - val_loss: 14.1498 - val_eval_dice: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 17/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.7194 - eval_dice: 0.0208\n",
      "Epoch 17: val_loss did not improve from 14.14976\n",
      "104/104 [==============================] - 234s 2s/step - loss: 11.7194 - eval_dice: 0.0208 - val_loss: 14.3157 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 18/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5952 - eval_dice: 0.0227\n",
      "Epoch 18: val_loss improved from 14.14976 to 14.10503, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 237s 2s/step - loss: 11.5952 - eval_dice: 0.0227 - val_loss: 14.1050 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 19/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2821 - eval_dice: 0.0237\n",
      "Epoch 19: val_loss improved from 14.10503 to 14.07650, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 11.2821 - eval_dice: 0.0237 - val_loss: 14.0765 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 20/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2136 - eval_dice: 0.0223\n",
      "Epoch 20: val_loss improved from 14.07650 to 13.82341, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 11.2136 - eval_dice: 0.0223 - val_loss: 13.8234 - val_eval_dice: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 21/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2080 - eval_dice: 0.0247\n",
      "Epoch 21: val_loss did not improve from 13.82341\n",
      "104/104 [==============================] - 244s 2s/step - loss: 11.2080 - eval_dice: 0.0247 - val_loss: 13.9363 - val_eval_dice: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 22/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1731 - eval_dice: 0.0233\n",
      "Epoch 22: val_loss did not improve from 13.82341\n",
      "104/104 [==============================] - 251s 2s/step - loss: 11.1731 - eval_dice: 0.0233 - val_loss: 13.9332 - val_eval_dice: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 23/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1599 - eval_dice: 0.0220\n",
      "Epoch 23: val_loss did not improve from 13.82341\n",
      "104/104 [==============================] - 253s 2s/step - loss: 11.1599 - eval_dice: 0.0220 - val_loss: 14.1539 - val_eval_dice: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 24/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9714 - eval_dice: 0.0232\n",
      "Epoch 24: val_loss improved from 13.82341 to 13.68497, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 245s 2s/step - loss: 10.9714 - eval_dice: 0.0232 - val_loss: 13.6850 - val_eval_dice: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 25/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8035 - eval_dice: 0.0216\n",
      "Epoch 25: val_loss did not improve from 13.68497\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.8035 - eval_dice: 0.0216 - val_loss: 13.9222 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 26/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.6267 - eval_dice: 0.0213\n",
      "Epoch 26: val_loss did not improve from 13.68497\n",
      "104/104 [==============================] - 234s 2s/step - loss: 10.6267 - eval_dice: 0.0213 - val_loss: 13.6892 - val_eval_dice: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 27/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.4318 - eval_dice: 0.0227\n",
      "Epoch 27: val_loss improved from 13.68497 to 13.50742, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.4318 - eval_dice: 0.0227 - val_loss: 13.5074 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 28/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.3087 - eval_dice: 0.0220\n",
      "Epoch 28: val_loss did not improve from 13.50742\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.3087 - eval_dice: 0.0220 - val_loss: 13.7987 - val_eval_dice: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 29/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2738 - eval_dice: 0.0219\n",
      "Epoch 29: val_loss improved from 13.50742 to 13.48321, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.2738 - eval_dice: 0.0219 - val_loss: 13.4832 - val_eval_dice: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 30/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2088 - eval_dice: 0.0228\n",
      "Epoch 30: val_loss did not improve from 13.48321\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.2088 - eval_dice: 0.0228 - val_loss: 13.5579 - val_eval_dice: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 31/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1987 - eval_dice: 0.0220\n",
      "Epoch 31: val_loss did not improve from 13.48321\n",
      "104/104 [==============================] - 238s 2s/step - loss: 10.1987 - eval_dice: 0.0220 - val_loss: 13.6002 - val_eval_dice: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 32/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1536 - eval_dice: 0.0219\n",
      "Epoch 32: val_loss did not improve from 13.48321\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.1536 - eval_dice: 0.0219 - val_loss: 13.5272 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 33/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1429 - eval_dice: 0.0219\n",
      "Epoch 33: val_loss did not improve from 13.48321\n",
      "104/104 [==============================] - 244s 2s/step - loss: 10.1429 - eval_dice: 0.0219 - val_loss: 13.6067 - val_eval_dice: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 34/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1285 - eval_dice: 0.0230\n",
      "Epoch 34: val_loss did not improve from 13.48321\n",
      "104/104 [==============================] - 244s 2s/step - loss: 10.1285 - eval_dice: 0.0230 - val_loss: 13.6000 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 35/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.0755 - eval_dice: 0.0210\n",
      "Epoch 35: val_loss improved from 13.48321 to 13.40703, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 237s 2s/step - loss: 10.0755 - eval_dice: 0.0210 - val_loss: 13.4070 - val_eval_dice: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 36/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.0624 - eval_dice: 0.0225\n",
      "Epoch 36: val_loss did not improve from 13.40703\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.0624 - eval_dice: 0.0225 - val_loss: 13.6523 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 37/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.0180 - eval_dice: 0.0198\n",
      "Epoch 37: val_loss improved from 13.40703 to 13.29400, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.0180 - eval_dice: 0.0198 - val_loss: 13.2940 - val_eval_dice: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 38/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.9046 - eval_dice: 0.0211\n",
      "Epoch 38: val_loss improved from 13.29400 to 13.15458, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 9.9046 - eval_dice: 0.0211 - val_loss: 13.1546 - val_eval_dice: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 39/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8990 - eval_dice: 0.0231\n",
      "Epoch 39: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 240s 2s/step - loss: 9.8990 - eval_dice: 0.0231 - val_loss: 13.3279 - val_eval_dice: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 40/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8621 - eval_dice: 0.0211\n",
      "Epoch 40: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 242s 2s/step - loss: 9.8621 - eval_dice: 0.0211 - val_loss: 13.5203 - val_eval_dice: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 41/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8540 - eval_dice: 0.0221\n",
      "Epoch 41: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 244s 2s/step - loss: 9.8540 - eval_dice: 0.0221 - val_loss: 13.5031 - val_eval_dice: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 42/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8741 - eval_dice: 0.0220\n",
      "Epoch 42: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 239s 2s/step - loss: 9.8741 - eval_dice: 0.0220 - val_loss: 13.3771 - val_eval_dice: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 43/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8528 - eval_dice: 0.0216\n",
      "Epoch 43: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 243s 2s/step - loss: 9.8528 - eval_dice: 0.0216 - val_loss: 13.2277 - val_eval_dice: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 44/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8584 - eval_dice: 0.0219\n",
      "Epoch 44: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 237s 2s/step - loss: 9.8584 - eval_dice: 0.0219 - val_loss: 13.2726 - val_eval_dice: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 45/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8477 - eval_dice: 0.0210\n",
      "Epoch 45: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 238s 2s/step - loss: 9.8477 - eval_dice: 0.0210 - val_loss: 13.3687 - val_eval_dice: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 46/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8526 - eval_dice: 0.0235\n",
      "Epoch 46: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 262s 2s/step - loss: 9.8526 - eval_dice: 0.0235 - val_loss: 13.2297 - val_eval_dice: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 47/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7882 - eval_dice: 0.0219\n",
      "Epoch 47: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 255s 2s/step - loss: 9.7882 - eval_dice: 0.0219 - val_loss: 13.1759 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 48/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7512 - eval_dice: 0.0210\n",
      "Epoch 48: val_loss did not improve from 13.15458\n",
      "104/104 [==============================] - 237s 2s/step - loss: 9.7512 - eval_dice: 0.0210 - val_loss: 13.1679 - val_eval_dice: 0.0112 - lr: 2.0000e-05\n",
      "Epoch 49/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7477 - eval_dice: 0.0214\n",
      "Epoch 49: val_loss improved from 13.15458 to 13.09771, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 240s 2s/step - loss: 9.7477 - eval_dice: 0.0214 - val_loss: 13.0977 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 50/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7391 - eval_dice: 0.0208\n",
      "Epoch 50: val_loss improved from 13.09771 to 13.07288, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 9.7391 - eval_dice: 0.0208 - val_loss: 13.0729 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 51/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7310 - eval_dice: 0.0210\n",
      "Epoch 51: val_loss did not improve from 13.07288\n",
      "104/104 [==============================] - 236s 2s/step - loss: 9.7310 - eval_dice: 0.0210 - val_loss: 13.1468 - val_eval_dice: 0.0109 - lr: 2.0000e-05\n",
      "Epoch 52/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7248 - eval_dice: 0.0214\n",
      "Epoch 52: val_loss did not improve from 13.07288\n",
      "104/104 [==============================] - 238s 2s/step - loss: 9.7248 - eval_dice: 0.0214 - val_loss: 13.1225 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 53/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7210 - eval_dice: 0.0213\n",
      "Epoch 53: val_loss did not improve from 13.07288\n",
      "104/104 [==============================] - 242s 2s/step - loss: 9.7210 - eval_dice: 0.0213 - val_loss: 13.1330 - val_eval_dice: 0.0110 - lr: 2.0000e-05\n",
      "Epoch 54/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7208 - eval_dice: 0.0218\n",
      "Epoch 54: val_loss improved from 13.07288 to 13.07122, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14\\cp.ckpt\n",
      "104/104 [==============================] - 250s 2s/step - loss: 9.7208 - eval_dice: 0.0218 - val_loss: 13.0712 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 55/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7118 - eval_dice: 0.0203\n",
      "Epoch 55: val_loss did not improve from 13.07122\n",
      "104/104 [==============================] - 245s 2s/step - loss: 9.7118 - eval_dice: 0.0203 - val_loss: 13.1575 - val_eval_dice: 0.0109 - lr: 2.0000e-05\n",
      "Epoch 56/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7114 - eval_dice: 0.0222\n",
      "Epoch 56: val_loss did not improve from 13.07122\n",
      "104/104 [==============================] - 241s 2s/step - loss: 9.7114 - eval_dice: 0.0222 - val_loss: 13.1415 - val_eval_dice: 0.0108 - lr: 2.0000e-05\n",
      "Epoch 57/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7093 - eval_dice: 0.0213\n",
      "Epoch 57: val_loss did not improve from 13.07122\n",
      "104/104 [==============================] - 251s 2s/step - loss: 9.7093 - eval_dice: 0.0213 - val_loss: 13.1769 - val_eval_dice: 0.0106 - lr: 2.0000e-05\n",
      "Epoch 58/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7004 - eval_dice: 0.0199\n",
      "Epoch 58: val_loss did not improve from 13.07122\n",
      "104/104 [==============================] - 240s 2s/step - loss: 9.7004 - eval_dice: 0.0199 - val_loss: 13.1113 - val_eval_dice: 0.0108 - lr: 2.0000e-05\n",
      "Epoch 59/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7041 - eval_dice: 0.0211\n",
      "Epoch 59: val_loss did not improve from 13.07122\n",
      "104/104 [==============================] - 248s 2s/step - loss: 9.7041 - eval_dice: 0.0211 - val_loss: 13.1338 - val_eval_dice: 0.0109 - lr: 2.0000e-05\n",
      "Epoch 60/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7043 - eval_dice: 0.0223\n",
      "Epoch 60: val_loss did not improve from 13.07122\n",
      "104/104 [==============================] - 255s 2s/step - loss: 9.7043 - eval_dice: 0.0223 - val_loss: 13.1985 - val_eval_dice: 0.0107 - lr: 2.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "print('Training session id:', training_session_path,batch_size,params['normalization'])\n",
    "history1_unet = unet3d_model.fit(\n",
    "                train_dynamic,\n",
    "                epochs=60,\n",
    "                validation_data=val_dynamic, \n",
    "                callbacks=callbacks_list  \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [MODEL] - AttUnet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs Directory: C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC\n",
      "Training session id: Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\n"
     ]
    }
   ],
   "source": [
    "logs_dynamic = refactor_path(os.path.join(logs_dir,params['interpolation'],params['subset']))\n",
    "training_session_path = 'Sigma3/BatchNorm/attunet3d_BN{}'.format(datetime.datetime.now().strftime(\"(%Y-%m-%d)/%H.%M.%S\"))\n",
    "print('Logs Directory:',logs_dynamic)\n",
    "print('Training session id:', training_session_path)\n",
    "file_path, callbacks_list = prepareCallbacks(training_session_path, logs_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttUnet3d input: 19 train KerasTensor(type_spec=TensorSpec(shape=(None, 64, 256, 256, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") True\n",
      "Input shape AFTER concatenate: (None, 8, 32, 32, 128)\n",
      "Input shape AFTER concatenate: (None, 16, 64, 64, 64)\n",
      "Input shape AFTER concatenate: (None, 32, 128, 128, 32)\n",
      "Input shape AFTER concatenate: (None, 64, 256, 256, 16)\n"
     ]
    }
   ],
   "source": [
    "# Initial input features\n",
    "features = tf.keras.layers.Input(shape=input_shape[params['subset']])\n",
    "print('AttUnet3d input:',yshape[params['subset']][-1],params['mode'],features,params['augment'])\n",
    "output = AttUnet3d(yshape[params['subset']][-1],params['mode'],features,params['normalization'])\n",
    "attunet_model = tf.keras.Model(inputs=features, outputs=output)\n",
    "attunet_model.compile(optimizer=Adam(learning_rate=params['lr']),\n",
    "                   loss=make_loss, \n",
    "                   metrics=[eval_dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64, 256, 25  0           []                               \n",
      "                                6, 1)]                                                            \n",
      "                                                                                                  \n",
      " conv3d (Conv3D)                (None, 64, 256, 256  432         ['input_1[0][0]']                \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 256, 256  64         ['conv3d[0][0]']                 \n",
      " alization)                     , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 64, 256, 256  0           ['batch_normalization[0][0]']    \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)              (None, 64, 256, 256  6912        ['re_lu[0][0]']                  \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 256, 256  64         ['conv3d_1[0][0]']               \n",
      " rmalization)                   , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 64, 256, 256  0           ['batch_normalization_1[0][0]']  \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 32, 128, 128  13824       ['re_lu_1[0][0]']                \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 128, 128  128        ['conv3d_2[0][0]']               \n",
      " rmalization)                   , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 32, 128, 128  0           ['batch_normalization_2[0][0]']  \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_3 (Conv3D)              (None, 32, 128, 128  27648       ['re_lu_2[0][0]']                \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 128, 128  128        ['conv3d_3[0][0]']               \n",
      " rmalization)                   , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 32, 128, 128  0           ['batch_normalization_3[0][0]']  \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_4 (Conv3D)              (None, 16, 64, 64,   55296       ['re_lu_3[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 64, 64,   256        ['conv3d_4[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 16, 64, 64,   0           ['batch_normalization_4[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_5 (Conv3D)              (None, 16, 64, 64,   110592      ['re_lu_4[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 64, 64,   256        ['conv3d_5[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 16, 64, 64,   0           ['batch_normalization_5[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_6 (Conv3D)              (None, 8, 32, 32, 1  221184      ['re_lu_5[0][0]']                \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 32, 32, 1  512        ['conv3d_6[0][0]']               \n",
      " rmalization)                   28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 8, 32, 32, 1  0           ['batch_normalization_6[0][0]']  \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_7 (Conv3D)              (None, 8, 32, 32, 1  442368      ['re_lu_6[0][0]']                \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8, 32, 32, 1  512        ['conv3d_7[0][0]']               \n",
      " rmalization)                   28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 8, 32, 32, 1  0           ['batch_normalization_7[0][0]']  \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_8 (Conv3D)              (None, 4, 16, 16, 5  1769472     ['re_lu_7[0][0]']                \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 16, 16, 5  2048       ['conv3d_8[0][0]']               \n",
      " rmalization)                   12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 4, 16, 16, 5  0           ['batch_normalization_8[0][0]']  \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_9 (Conv3D)              (None, 4, 16, 16, 5  7077888     ['re_lu_8[0][0]']                \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 4, 16, 16, 5  2048       ['conv3d_9[0][0]']               \n",
      " rmalization)                   12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 4, 16, 16, 5  0           ['batch_normalization_9[0][0]']  \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 16, 16, 5  0           ['re_lu_9[0][0]']                \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_10 (Conv3D)             (None, 4, 16, 16, 6  65600       ['re_lu_7[0][0]']                \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_11 (Conv3D)             (None, 4, 16, 16, 6  32832       ['dropout[0][0]']                \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4, 16, 16, 6  0           ['conv3d_10[0][0]',              \n",
      "                                4)                                'conv3d_11[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 4, 16, 16, 6  0           ['add[0][0]']                    \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_12 (Conv3D)             (None, 4, 16, 16, 1  65          ['activation[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 4, 16, 16, 1  0           ['conv3d_12[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling3d (UpSampling3D)   (None, 8, 32, 32, 1  0           ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose (Conv3DTransp  (None, 8, 32, 32, 1  1769600    ['dropout[0][0]']                \n",
      " ose)                           28)                                                               \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 8, 32, 32, 1  0           ['up_sampling3d[0][0]',          \n",
      "                                28)                               're_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8, 32, 32, 2  0           ['conv3d_transpose[0][0]',       \n",
      "                                56)                               'multiply[0][0]']               \n",
      "                                                                                                  \n",
      " conv3d_13 (Conv3D)             (None, 8, 32, 32, 1  884736      ['concatenate[0][0]']            \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_13[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 8, 32, 32, 1  0           ['batch_normalization_10[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_14 (Conv3D)             (None, 8, 32, 32, 1  442368      ['re_lu_10[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8, 32, 32, 1  512        ['conv3d_14[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 8, 32, 32, 1  0           ['batch_normalization_11[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_15 (Conv3D)             (None, 8, 32, 32, 3  16416       ['re_lu_5[0][0]']                \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv3d_16 (Conv3D)             (None, 8, 32, 32, 3  4128        ['re_lu_11[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 8, 32, 32, 3  0           ['conv3d_15[0][0]',              \n",
      "                                2)                                'conv3d_16[0][0]']              \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 8, 32, 32, 3  0           ['add_1[0][0]']                  \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv3d_17 (Conv3D)             (None, 8, 32, 32, 1  33          ['activation_2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 8, 32, 32, 1  0           ['conv3d_17[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling3d_1 (UpSampling3D)  (None, 16, 64, 64,   0          ['activation_3[0][0]']           \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " conv3d_transpose_1 (Conv3DTran  (None, 16, 64, 64,   221248     ['re_lu_11[0][0]']               \n",
      " spose)                         64)                                                               \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 16, 64, 64,   0           ['up_sampling3d_1[0][0]',        \n",
      "                                64)                               're_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 16, 64, 64,   0           ['conv3d_transpose_1[0][0]',     \n",
      "                                128)                              'multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv3d_18 (Conv3D)             (None, 16, 64, 64,   221184      ['concatenate_1[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 64, 64,   256        ['conv3d_18[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 16, 64, 64,   0           ['batch_normalization_12[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_19 (Conv3D)             (None, 16, 64, 64,   110592      ['re_lu_12[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 64, 64,   256        ['conv3d_19[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 16, 64, 64,   0           ['batch_normalization_13[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_20 (Conv3D)             (None, 16, 64, 64,   4112        ['re_lu_3[0][0]']                \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " conv3d_21 (Conv3D)             (None, 16, 64, 64,   1040        ['re_lu_13[0][0]']               \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 16, 64, 64,   0           ['conv3d_20[0][0]',              \n",
      "                                16)                               'conv3d_21[0][0]']              \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 64, 64,   0           ['add_2[0][0]']                  \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " conv3d_22 (Conv3D)             (None, 16, 64, 64,   17          ['activation_4[0][0]']           \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 16, 64, 64,   0           ['conv3d_22[0][0]']              \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " up_sampling3d_2 (UpSampling3D)  (None, 32, 128, 128  0          ['activation_5[0][0]']           \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_2 (Conv3DTran  (None, 32, 128, 128  55328      ['re_lu_13[0][0]']               \n",
      " spose)                         , 32)                                                             \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 32, 128, 128  0           ['up_sampling3d_2[0][0]',        \n",
      "                                , 32)                             're_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 128, 128  0           ['conv3d_transpose_2[0][0]',     \n",
      "                                , 64)                             'multiply_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv3d_23 (Conv3D)             (None, 32, 128, 128  55296       ['concatenate_2[0][0]']          \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 128, 128  128        ['conv3d_23[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 32, 128, 128  0           ['batch_normalization_14[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_24 (Conv3D)             (None, 32, 128, 128  27648       ['re_lu_14[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 128, 128  128        ['conv3d_24[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 32, 128, 128  0           ['batch_normalization_15[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_25 (Conv3D)             (None, 32, 128, 128  1032        ['re_lu_1[0][0]']                \n",
      "                                , 8)                                                              \n",
      "                                                                                                  \n",
      " conv3d_26 (Conv3D)             (None, 32, 128, 128  264         ['re_lu_15[0][0]']               \n",
      "                                , 8)                                                              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 128, 128  0           ['conv3d_25[0][0]',              \n",
      "                                , 8)                              'conv3d_26[0][0]']              \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 128, 128  0           ['add_3[0][0]']                  \n",
      "                                , 8)                                                              \n",
      "                                                                                                  \n",
      " conv3d_27 (Conv3D)             (None, 32, 128, 128  9           ['activation_6[0][0]']           \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 128, 128  0           ['conv3d_27[0][0]']              \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " up_sampling3d_3 (UpSampling3D)  (None, 64, 256, 256  0          ['activation_7[0][0]']           \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_3 (Conv3DTran  (None, 64, 256, 256  13840      ['re_lu_15[0][0]']               \n",
      " spose)                         , 16)                                                             \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 64, 256, 256  0           ['up_sampling3d_3[0][0]',        \n",
      "                                , 16)                             're_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 64, 256, 256  0           ['conv3d_transpose_3[0][0]',     \n",
      "                                , 32)                             'multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " conv3d_28 (Conv3D)             (None, 64, 256, 256  13824       ['concatenate_3[0][0]']          \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 256, 256  64         ['conv3d_28[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 64, 256, 256  0           ['batch_normalization_16[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_29 (Conv3D)             (None, 64, 256, 256  6912        ['re_lu_16[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64, 256, 256  64         ['conv3d_29[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 64, 256, 256  0           ['batch_normalization_17[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_30 (Conv3D)             (None, 64, 256, 256  8227        ['re_lu_17[0][0]']               \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 256, 256  0           ['conv3d_30[0][0]']              \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,689,903\n",
      "Trainable params: 13,685,935\n",
      "Non-trainable params: 3,968\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attunet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "    104/Unknown - 195s 2s/step - loss: 20.4251 - eval_dice: 0.7615\n",
      "Epoch 1: val_loss improved from inf to 18.92287, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 230s 2s/step - loss: 20.4251 - eval_dice: 0.7615 - val_loss: 18.9229 - val_eval_dice: 0.4435 - lr: 1.0000e-04\n",
      "Epoch 2/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.4068 - eval_dice: 0.2276\n",
      "Epoch 2: val_loss improved from 18.92287 to 18.22534, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 223s 2s/step - loss: 18.4068 - eval_dice: 0.2276 - val_loss: 18.2253 - val_eval_dice: 0.1278 - lr: 1.0000e-04\n",
      "Epoch 3/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.1032 - eval_dice: 0.0962\n",
      "Epoch 3: val_loss improved from 18.22534 to 18.05827, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 227s 2s/step - loss: 18.1032 - eval_dice: 0.0962 - val_loss: 18.0583 - val_eval_dice: 0.0673 - lr: 1.0000e-04\n",
      "Epoch 4/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.9162 - eval_dice: 0.0583\n",
      "Epoch 4: val_loss improved from 18.05827 to 17.90175, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 17.9162 - eval_dice: 0.0583 - val_loss: 17.9018 - val_eval_dice: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 5/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.5650 - eval_dice: 0.0396\n",
      "Epoch 5: val_loss improved from 17.90175 to 17.58939, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 233s 2s/step - loss: 17.5650 - eval_dice: 0.0396 - val_loss: 17.5894 - val_eval_dice: 0.0222 - lr: 1.0000e-04\n",
      "Epoch 6/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 16.8048 - eval_dice: 0.0298\n",
      "Epoch 6: val_loss improved from 17.58939 to 16.86294, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 230s 2s/step - loss: 16.8048 - eval_dice: 0.0298 - val_loss: 16.8629 - val_eval_dice: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 7/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 15.6480 - eval_dice: 0.0268\n",
      "Epoch 7: val_loss improved from 16.86294 to 16.04043, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 233s 2s/step - loss: 15.6480 - eval_dice: 0.0268 - val_loss: 16.0404 - val_eval_dice: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 8/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.6050 - eval_dice: 0.0245\n",
      "Epoch 8: val_loss improved from 16.04043 to 15.55066, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 231s 2s/step - loss: 14.6050 - eval_dice: 0.0245 - val_loss: 15.5507 - val_eval_dice: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 9/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.2711 - eval_dice: 0.0243\n",
      "Epoch 9: val_loss improved from 15.55066 to 15.47428, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 14.2711 - eval_dice: 0.0243 - val_loss: 15.4743 - val_eval_dice: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 10/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.8301 - eval_dice: 0.0259\n",
      "Epoch 10: val_loss improved from 15.47428 to 15.19387, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 240s 2s/step - loss: 13.8301 - eval_dice: 0.0259 - val_loss: 15.1939 - val_eval_dice: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 11/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.4927 - eval_dice: 0.0245\n",
      "Epoch 11: val_loss improved from 15.19387 to 14.82861, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 238s 2s/step - loss: 13.4927 - eval_dice: 0.0245 - val_loss: 14.8286 - val_eval_dice: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 12/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.2870 - eval_dice: 0.0235\n",
      "Epoch 12: val_loss improved from 14.82861 to 14.77313, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 13.2870 - eval_dice: 0.0235 - val_loss: 14.7731 - val_eval_dice: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 13/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.1504 - eval_dice: 0.0238\n",
      "Epoch 13: val_loss improved from 14.77313 to 14.59290, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 13.1504 - eval_dice: 0.0238 - val_loss: 14.5929 - val_eval_dice: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 14/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.1260 - eval_dice: 0.0235\n",
      "Epoch 14: val_loss did not improve from 14.59290\n",
      "104/104 [==============================] - 235s 2s/step - loss: 13.1260 - eval_dice: 0.0235 - val_loss: 14.7934 - val_eval_dice: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 15/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.9988 - eval_dice: 0.0252\n",
      "Epoch 15: val_loss improved from 14.59290 to 14.38474, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 12.9988 - eval_dice: 0.0252 - val_loss: 14.3847 - val_eval_dice: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 16/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.6124 - eval_dice: 0.0241\n",
      "Epoch 16: val_loss improved from 14.38474 to 14.32743, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 237s 2s/step - loss: 12.6124 - eval_dice: 0.0241 - val_loss: 14.3274 - val_eval_dice: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 17/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.4386 - eval_dice: 0.0226\n",
      "Epoch 17: val_loss improved from 14.32743 to 14.08032, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 12.4386 - eval_dice: 0.0226 - val_loss: 14.0803 - val_eval_dice: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 18/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.3577 - eval_dice: 0.0238\n",
      "Epoch 18: val_loss did not improve from 14.08032\n",
      "104/104 [==============================] - 240s 2s/step - loss: 12.3577 - eval_dice: 0.0238 - val_loss: 14.1555 - val_eval_dice: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 19/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.3045 - eval_dice: 0.0227\n",
      "Epoch 19: val_loss improved from 14.08032 to 13.97174, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 238s 2s/step - loss: 12.3045 - eval_dice: 0.0227 - val_loss: 13.9717 - val_eval_dice: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 20/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.2178 - eval_dice: 0.0222\n",
      "Epoch 20: val_loss improved from 13.97174 to 13.92037, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 12.2178 - eval_dice: 0.0222 - val_loss: 13.9204 - val_eval_dice: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 21/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.1691 - eval_dice: 0.0221\n",
      "Epoch 21: val_loss improved from 13.92037 to 13.84629, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 234s 2s/step - loss: 12.1691 - eval_dice: 0.0221 - val_loss: 13.8463 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 22/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.1225 - eval_dice: 0.0230\n",
      "Epoch 22: val_loss did not improve from 13.84629\n",
      "104/104 [==============================] - 238s 2s/step - loss: 12.1225 - eval_dice: 0.0230 - val_loss: 13.8788 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 23/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0999 - eval_dice: 0.0241\n",
      "Epoch 23: val_loss did not improve from 13.84629\n",
      "104/104 [==============================] - 238s 2s/step - loss: 12.0999 - eval_dice: 0.0241 - val_loss: 13.8935 - val_eval_dice: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 24/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0790 - eval_dice: 0.0234\n",
      "Epoch 24: val_loss did not improve from 13.84629\n",
      "104/104 [==============================] - 236s 2s/step - loss: 12.0790 - eval_dice: 0.0234 - val_loss: 13.9599 - val_eval_dice: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 25/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0669 - eval_dice: 0.0232\n",
      "Epoch 25: val_loss did not improve from 13.84629\n",
      "104/104 [==============================] - 238s 2s/step - loss: 12.0669 - eval_dice: 0.0232 - val_loss: 13.9179 - val_eval_dice: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 26/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0616 - eval_dice: 0.0251\n",
      "Epoch 26: val_loss improved from 13.84629 to 13.73499, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 12.0616 - eval_dice: 0.0251 - val_loss: 13.7350 - val_eval_dice: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 27/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0337 - eval_dice: 0.0233\n",
      "Epoch 27: val_loss did not improve from 13.73499\n",
      "104/104 [==============================] - 236s 2s/step - loss: 12.0337 - eval_dice: 0.0233 - val_loss: 13.8350 - val_eval_dice: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 28/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.9733 - eval_dice: 0.0239\n",
      "Epoch 28: val_loss did not improve from 13.73499\n",
      "104/104 [==============================] - 244s 2s/step - loss: 11.9733 - eval_dice: 0.0239 - val_loss: 13.7763 - val_eval_dice: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 29/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.9361 - eval_dice: 0.0234\n",
      "Epoch 29: val_loss did not improve from 13.73499\n",
      "104/104 [==============================] - 236s 2s/step - loss: 11.9361 - eval_dice: 0.0234 - val_loss: 13.7752 - val_eval_dice: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 30/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.9293 - eval_dice: 0.0247\n",
      "Epoch 30: val_loss did not improve from 13.73499\n",
      "104/104 [==============================] - 238s 2s/step - loss: 11.9293 - eval_dice: 0.0247 - val_loss: 13.8868 - val_eval_dice: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 31/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.9148 - eval_dice: 0.0242\n",
      "Epoch 31: val_loss did not improve from 13.73499\n",
      "104/104 [==============================] - 244s 2s/step - loss: 11.9148 - eval_dice: 0.0242 - val_loss: 13.8454 - val_eval_dice: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 32/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.8388 - eval_dice: 0.0243\n",
      "Epoch 32: val_loss improved from 13.73499 to 13.60819, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 250s 2s/step - loss: 11.8388 - eval_dice: 0.0243 - val_loss: 13.6082 - val_eval_dice: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 33/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.7569 - eval_dice: 0.0249\n",
      "Epoch 33: val_loss improved from 13.60819 to 13.47123, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 11.7569 - eval_dice: 0.0249 - val_loss: 13.4712 - val_eval_dice: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 34/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4171 - eval_dice: 0.0228\n",
      "Epoch 34: val_loss improved from 13.47123 to 13.29720, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 11.4171 - eval_dice: 0.0228 - val_loss: 13.2972 - val_eval_dice: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 35/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1297 - eval_dice: 0.0232\n",
      "Epoch 35: val_loss improved from 13.29720 to 13.13407, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 240s 2s/step - loss: 11.1297 - eval_dice: 0.0232 - val_loss: 13.1341 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 36/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0714 - eval_dice: 0.0236\n",
      "Epoch 36: val_loss did not improve from 13.13407\n",
      "104/104 [==============================] - 247s 2s/step - loss: 11.0714 - eval_dice: 0.0236 - val_loss: 13.1776 - val_eval_dice: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 37/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9919 - eval_dice: 0.0233\n",
      "Epoch 37: val_loss did not improve from 13.13407\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.9919 - eval_dice: 0.0233 - val_loss: 13.2270 - val_eval_dice: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 38/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9925 - eval_dice: 0.0230\n",
      "Epoch 38: val_loss did not improve from 13.13407\n",
      "104/104 [==============================] - 253s 2s/step - loss: 10.9925 - eval_dice: 0.0230 - val_loss: 13.3207 - val_eval_dice: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 39/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9116 - eval_dice: 0.0234\n",
      "Epoch 39: val_loss did not improve from 13.13407\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.9116 - eval_dice: 0.0234 - val_loss: 13.2777 - val_eval_dice: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 40/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8844 - eval_dice: 0.0213\n",
      "Epoch 40: val_loss did not improve from 13.13407\n",
      "104/104 [==============================] - 237s 2s/step - loss: 10.8844 - eval_dice: 0.0213 - val_loss: 13.2211 - val_eval_dice: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 41/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8570 - eval_dice: 0.0221\n",
      "Epoch 41: val_loss did not improve from 13.13407\n",
      "104/104 [==============================] - 236s 2s/step - loss: 10.8570 - eval_dice: 0.0221 - val_loss: 13.2605 - val_eval_dice: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 42/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8515 - eval_dice: 0.0234\n",
      "Epoch 42: val_loss improved from 13.13407 to 13.08825, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 245s 2s/step - loss: 10.8515 - eval_dice: 0.0234 - val_loss: 13.0883 - val_eval_dice: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 43/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8328 - eval_dice: 0.0238\n",
      "Epoch 43: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.8328 - eval_dice: 0.0238 - val_loss: 13.1141 - val_eval_dice: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 44/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8424 - eval_dice: 0.0229\n",
      "Epoch 44: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.8424 - eval_dice: 0.0229 - val_loss: 13.2673 - val_eval_dice: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 45/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8209 - eval_dice: 0.0215\n",
      "Epoch 45: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 237s 2s/step - loss: 10.8209 - eval_dice: 0.0215 - val_loss: 13.3030 - val_eval_dice: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 46/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8131 - eval_dice: 0.0235\n",
      "Epoch 46: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.8131 - eval_dice: 0.0235 - val_loss: 13.2839 - val_eval_dice: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 47/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8071 - eval_dice: 0.0231\n",
      "Epoch 47: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.8071 - eval_dice: 0.0231 - val_loss: 13.1426 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 48/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7840 - eval_dice: 0.0226\n",
      "Epoch 48: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.7840 - eval_dice: 0.0226 - val_loss: 13.1761 - val_eval_dice: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 49/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7871 - eval_dice: 0.0235\n",
      "Epoch 49: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.7871 - eval_dice: 0.0235 - val_loss: 13.1884 - val_eval_dice: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 50/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7692 - eval_dice: 0.0222\n",
      "Epoch 50: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.7692 - eval_dice: 0.0222 - val_loss: 13.1368 - val_eval_dice: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 51/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7596 - eval_dice: 0.0209\n",
      "Epoch 51: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.7596 - eval_dice: 0.0209 - val_loss: 13.1594 - val_eval_dice: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 52/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7691 - eval_dice: 0.0226\n",
      "Epoch 52: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.7691 - eval_dice: 0.0226 - val_loss: 13.2890 - val_eval_dice: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 53/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7594 - eval_dice: 0.0227\n",
      "Epoch 53: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 238s 2s/step - loss: 10.7594 - eval_dice: 0.0227 - val_loss: 13.1288 - val_eval_dice: 0.0160 - lr: 2.0000e-05\n",
      "Epoch 54/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.7112 - eval_dice: 0.0220\n",
      "Epoch 54: val_loss did not improve from 13.08825\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.7112 - eval_dice: 0.0220 - val_loss: 13.1354 - val_eval_dice: 0.0144 - lr: 2.0000e-05\n",
      "Epoch 55/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.6868 - eval_dice: 0.0220\n",
      "Epoch 55: val_loss improved from 13.08825 to 13.06451, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.6868 - eval_dice: 0.0220 - val_loss: 13.0645 - val_eval_dice: 0.0126 - lr: 2.0000e-05\n",
      "Epoch 56/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.5444 - eval_dice: 0.0212\n",
      "Epoch 56: val_loss improved from 13.06451 to 12.91812, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 10.5444 - eval_dice: 0.0212 - val_loss: 12.9181 - val_eval_dice: 0.0119 - lr: 2.0000e-05\n",
      "Epoch 57/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.4142 - eval_dice: 0.0232\n",
      "Epoch 57: val_loss did not improve from 12.91812\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.4142 - eval_dice: 0.0232 - val_loss: 13.0141 - val_eval_dice: 0.0118 - lr: 2.0000e-05\n",
      "Epoch 58/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.3348 - eval_dice: 0.0235\n",
      "Epoch 58: val_loss improved from 12.91812 to 12.89550, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.3348 - eval_dice: 0.0235 - val_loss: 12.8955 - val_eval_dice: 0.0116 - lr: 2.0000e-05\n",
      "Epoch 59/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2969 - eval_dice: 0.0212\n",
      "Epoch 59: val_loss did not improve from 12.89550\n",
      "104/104 [==============================] - 238s 2s/step - loss: 10.2969 - eval_dice: 0.0212 - val_loss: 12.8994 - val_eval_dice: 0.0113 - lr: 2.0000e-05\n",
      "Epoch 60/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2815 - eval_dice: 0.0217\n",
      "Epoch 60: val_loss did not improve from 12.89550\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.2815 - eval_dice: 0.0217 - val_loss: 12.8989 - val_eval_dice: 0.0115 - lr: 2.0000e-05\n",
      "Epoch 61/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2655 - eval_dice: 0.0216\n",
      "Epoch 61: val_loss did not improve from 12.89550\n",
      "104/104 [==============================] - 238s 2s/step - loss: 10.2655 - eval_dice: 0.0216 - val_loss: 12.9036 - val_eval_dice: 0.0115 - lr: 2.0000e-05\n",
      "Epoch 62/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2579 - eval_dice: 0.0223\n",
      "Epoch 62: val_loss improved from 12.89550 to 12.86700, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.2579 - eval_dice: 0.0223 - val_loss: 12.8670 - val_eval_dice: 0.0113 - lr: 2.0000e-05\n",
      "Epoch 63/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2467 - eval_dice: 0.0219\n",
      "Epoch 63: val_loss did not improve from 12.86700\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.2467 - eval_dice: 0.0219 - val_loss: 12.9040 - val_eval_dice: 0.0114 - lr: 2.0000e-05\n",
      "Epoch 64/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2389 - eval_dice: 0.0199\n",
      "Epoch 64: val_loss improved from 12.86700 to 12.79043, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/BatchNorm/attunet3d_BN(2024-05-28)/22.45.30\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.2389 - eval_dice: 0.0199 - val_loss: 12.7904 - val_eval_dice: 0.0112 - lr: 2.0000e-05\n",
      "Epoch 65/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2279 - eval_dice: 0.0220\n",
      "Epoch 65: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.2279 - eval_dice: 0.0220 - val_loss: 12.8984 - val_eval_dice: 0.0112 - lr: 2.0000e-05\n",
      "Epoch 66/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2252 - eval_dice: 0.0221\n",
      "Epoch 66: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.2252 - eval_dice: 0.0221 - val_loss: 12.8766 - val_eval_dice: 0.0110 - lr: 2.0000e-05\n",
      "Epoch 67/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2141 - eval_dice: 0.0216\n",
      "Epoch 67: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.2141 - eval_dice: 0.0216 - val_loss: 12.9416 - val_eval_dice: 0.0116 - lr: 2.0000e-05\n",
      "Epoch 68/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2226 - eval_dice: 0.0222\n",
      "Epoch 68: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.2226 - eval_dice: 0.0222 - val_loss: 12.9137 - val_eval_dice: 0.0112 - lr: 2.0000e-05\n",
      "Epoch 69/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2169 - eval_dice: 0.0223\n",
      "Epoch 69: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.2169 - eval_dice: 0.0223 - val_loss: 12.8914 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 70/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2109 - eval_dice: 0.0217\n",
      "Epoch 70: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.2109 - eval_dice: 0.0217 - val_loss: 12.8904 - val_eval_dice: 0.0110 - lr: 2.0000e-05\n",
      "Epoch 71/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2037 - eval_dice: 0.0212\n",
      "Epoch 71: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.2037 - eval_dice: 0.0212 - val_loss: 12.9232 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 72/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2058 - eval_dice: 0.0219\n",
      "Epoch 72: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 246s 2s/step - loss: 10.2058 - eval_dice: 0.0219 - val_loss: 12.9241 - val_eval_dice: 0.0112 - lr: 2.0000e-05\n",
      "Epoch 73/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2064 - eval_dice: 0.0224\n",
      "Epoch 73: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.2064 - eval_dice: 0.0224 - val_loss: 12.8886 - val_eval_dice: 0.0111 - lr: 2.0000e-05\n",
      "Epoch 74/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2056 - eval_dice: 0.0209\n",
      "Epoch 74: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 238s 2s/step - loss: 10.2056 - eval_dice: 0.0209 - val_loss: 12.9070 - val_eval_dice: 0.0109 - lr: 2.0000e-05\n",
      "Epoch 75/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1963 - eval_dice: 0.0221\n",
      "Epoch 75: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.1963 - eval_dice: 0.0221 - val_loss: 12.8866 - val_eval_dice: 0.0111 - lr: 4.0000e-06\n",
      "Epoch 76/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1876 - eval_dice: 0.0200\n",
      "Epoch 76: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.1876 - eval_dice: 0.0200 - val_loss: 12.9432 - val_eval_dice: 0.0110 - lr: 4.0000e-06\n",
      "Epoch 77/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1910 - eval_dice: 0.0214\n",
      "Epoch 77: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 245s 2s/step - loss: 10.1910 - eval_dice: 0.0214 - val_loss: 12.9181 - val_eval_dice: 0.0111 - lr: 4.0000e-06\n",
      "Epoch 78/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1929 - eval_dice: 0.0213\n",
      "Epoch 78: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.1929 - eval_dice: 0.0213 - val_loss: 12.9109 - val_eval_dice: 0.0111 - lr: 4.0000e-06\n",
      "Epoch 79/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1853 - eval_dice: 0.0211\n",
      "Epoch 79: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.1853 - eval_dice: 0.0211 - val_loss: 12.9074 - val_eval_dice: 0.0110 - lr: 4.0000e-06\n",
      "Epoch 80/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1848 - eval_dice: 0.0210\n",
      "Epoch 80: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.1848 - eval_dice: 0.0210 - val_loss: 12.9285 - val_eval_dice: 0.0110 - lr: 4.0000e-06\n",
      "Epoch 81/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1875 - eval_dice: 0.0207\n",
      "Epoch 81: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.1875 - eval_dice: 0.0207 - val_loss: 12.9138 - val_eval_dice: 0.0110 - lr: 4.0000e-06\n",
      "Epoch 82/90\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1847 - eval_dice: 0.0212\n",
      "Epoch 82: val_loss did not improve from 12.79043\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.1847 - eval_dice: 0.0212 - val_loss: 12.9200 - val_eval_dice: 0.0109 - lr: 4.0000e-06\n",
      "Epoch 82: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history1 = attunet_model.fit(\n",
    "                train_dynamic,\n",
    "                epochs=90,\n",
    "                validation_data=val_dynamic,\n",
    "                callbacks=callbacks_list  \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [MODEL] - Residual Attention U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs Directory: C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC\n",
      "Training session id: Sigma3/BatchNorm/resattunet3d_(2024-07-29)/16.49.34\n"
     ]
    }
   ],
   "source": [
    "logs_axial = refactor_path(os.path.join(logs_dir,params['interpolation'],params['subset']))\n",
    "training_session_path = 'Sigma3/BatchNorm/resattunet3d_{}'.format(datetime.datetime.now().strftime(\"(%Y-%m-%d)/%H.%M.%S\"))\n",
    "print('Logs Directory:',logs_axial)\n",
    "print('Training session id:', training_session_path)\n",
    "file_path, callbacks_list = prepareCallbacks(training_session_path, logs_axial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttResUnet3d input: 19 train KerasTensor(type_spec=TensorSpec(shape=(None, 64, 256, 256, 1), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\") True\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 64, 256, 25  0           []                               \n",
      "                                6, 1)]                                                            \n",
      "                                                                                                  \n",
      " conv3d_89 (Conv3D)             (None, 64, 256, 256  432         ['input_5[0][0]']                \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_44 (GroupN  (None, 64, 256, 256  32         ['conv3d_89[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_70 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_44[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_90 (Conv3D)             (None, 64, 256, 256  6912        ['re_lu_70[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_45 (GroupN  (None, 64, 256, 256  32         ['conv3d_90[0][0]']              \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_71 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_45[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_91 (Conv3D)             (None, 32, 128, 128  13824       ['re_lu_71[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_46 (GroupN  (None, 32, 128, 128  64         ['conv3d_91[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_72 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_46[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_92 (Conv3D)             (None, 32, 128, 128  27648       ['re_lu_72[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_47 (GroupN  (None, 32, 128, 128  64         ['conv3d_92[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_93 (Conv3D)             (None, 32, 128, 128  512         ['re_lu_71[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_73 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_47[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_48 (GroupN  (None, 32, 128, 128  64         ['conv3d_93[0][0]']              \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 32, 128, 128  0           ['re_lu_73[0][0]',               \n",
      "                                , 32)                             'group_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_74 (ReLU)                (None, 32, 128, 128  0           ['add_24[0][0]']                 \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_94 (Conv3D)             (None, 16, 64, 64,   55296       ['re_lu_74[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_49 (GroupN  (None, 16, 64, 64,   128        ['conv3d_94[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_75 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_49[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_95 (Conv3D)             (None, 16, 64, 64,   110592      ['re_lu_75[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_50 (GroupN  (None, 16, 64, 64,   128        ['conv3d_95[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_96 (Conv3D)             (None, 16, 64, 64,   2048        ['re_lu_74[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_76 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_50[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_51 (GroupN  (None, 16, 64, 64,   128        ['conv3d_96[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 16, 64, 64,   0           ['re_lu_76[0][0]',               \n",
      "                                64)                               'group_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_77 (ReLU)                (None, 16, 64, 64,   0           ['add_25[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_97 (Conv3D)             (None, 8, 32, 32, 1  221184      ['re_lu_77[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_52 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_97[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_78 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_52[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_98 (Conv3D)             (None, 8, 32, 32, 1  442368      ['re_lu_78[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_53 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_98[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_99 (Conv3D)             (None, 8, 32, 32, 1  8192        ['re_lu_77[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_79 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_53[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_54 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_99[0][0]']              \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 32, 32, 1  0           ['re_lu_79[0][0]',               \n",
      "                                28)                               'group_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_80 (ReLU)                (None, 8, 32, 32, 1  0           ['add_26[0][0]']                 \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_100 (Conv3D)            (None, 4, 16, 16, 5  1769472     ['re_lu_80[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_55 (GroupN  (None, 4, 16, 16, 5  1024       ['conv3d_100[0][0]']             \n",
      " ormalization)                  12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_81 (ReLU)                (None, 4, 16, 16, 5  0           ['group_normalization_55[0][0]'] \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_101 (Conv3D)            (None, 4, 16, 16, 5  7077888     ['re_lu_81[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_56 (GroupN  (None, 4, 16, 16, 5  1024       ['conv3d_101[0][0]']             \n",
      " ormalization)                  12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_102 (Conv3D)            (None, 4, 16, 16, 5  65536       ['re_lu_80[0][0]']               \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " re_lu_82 (ReLU)                (None, 4, 16, 16, 5  0           ['group_normalization_56[0][0]'] \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_57 (GroupN  (None, 4, 16, 16, 5  1024       ['conv3d_102[0][0]']             \n",
      " ormalization)                  12)                                                               \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 4, 16, 16, 5  0           ['re_lu_82[0][0]',               \n",
      "                                12)                               'group_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_83 (ReLU)                (None, 4, 16, 16, 5  0           ['add_27[0][0]']                 \n",
      "                                12)                                                               \n",
      "                                                                                                  \n",
      " conv3d_103 (Conv3D)            (None, 4, 16, 16, 6  65600       ['re_lu_80[0][0]']               \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_104 (Conv3D)            (None, 4, 16, 16, 6  32832       ['re_lu_83[0][0]']               \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 4, 16, 16, 6  0           ['conv3d_103[0][0]',             \n",
      "                                4)                                'conv3d_104[0][0]']             \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 4, 16, 16, 6  0           ['add_28[0][0]']                 \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_105 (Conv3D)            (None, 4, 16, 16, 1  65          ['activation_19[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 4, 16, 16, 1  0           ['conv3d_105[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling3d_8 (UpSampling3D)  (None, 8, 32, 32, 1  0          ['activation_20[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose_20 (Conv3DTra  (None, 8, 32, 32, 1  1769600    ['re_lu_83[0][0]']               \n",
      " nspose)                        28)                                                               \n",
      "                                                                                                  \n",
      " multiply_8 (Multiply)          (None, 8, 32, 32, 1  0           ['up_sampling3d_8[0][0]',        \n",
      "                                28)                               're_lu_80[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 8, 32, 32, 2  0           ['conv3d_transpose_20[0][0]',    \n",
      "                                56)                               'multiply_8[0][0]']             \n",
      "                                                                                                  \n",
      " conv3d_106 (Conv3D)            (None, 8, 32, 32, 1  884736      ['concatenate_12[0][0]']         \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_58 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_106[0][0]']             \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_84 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_58[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_107 (Conv3D)            (None, 8, 32, 32, 1  442368      ['re_lu_84[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_59 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_107[0][0]']             \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_21 (Conv3DTra  (None, 8, 32, 32, 1  65536      ['re_lu_83[0][0]']               \n",
      " nspose)                        28)                                                               \n",
      "                                                                                                  \n",
      " re_lu_85 (ReLU)                (None, 8, 32, 32, 1  0           ['group_normalization_59[0][0]'] \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_60 (GroupN  (None, 8, 32, 32, 1  256        ['conv3d_transpose_21[0][0]']    \n",
      " ormalization)                  28)                                                               \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 8, 32, 32, 1  0           ['re_lu_85[0][0]',               \n",
      "                                28)                               'group_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_86 (ReLU)                (None, 8, 32, 32, 1  0           ['add_29[0][0]']                 \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv3d_108 (Conv3D)            (None, 8, 32, 32, 3  16416       ['re_lu_77[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv3d_109 (Conv3D)            (None, 8, 32, 32, 3  4128        ['re_lu_86[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 8, 32, 32, 3  0           ['conv3d_108[0][0]',             \n",
      "                                2)                                'conv3d_109[0][0]']             \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 8, 32, 32, 3  0           ['add_30[0][0]']                 \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv3d_110 (Conv3D)            (None, 8, 32, 32, 1  33          ['activation_21[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 32, 32, 1  0           ['conv3d_110[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling3d_9 (UpSampling3D)  (None, 16, 64, 64,   0          ['activation_22[0][0]']          \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " conv3d_transpose_22 (Conv3DTra  (None, 16, 64, 64,   221248     ['re_lu_86[0][0]']               \n",
      " nspose)                        64)                                                               \n",
      "                                                                                                  \n",
      " multiply_9 (Multiply)          (None, 16, 64, 64,   0           ['up_sampling3d_9[0][0]',        \n",
      "                                64)                               're_lu_77[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 64, 64,   0           ['conv3d_transpose_22[0][0]',    \n",
      "                                128)                              'multiply_9[0][0]']             \n",
      "                                                                                                  \n",
      " conv3d_111 (Conv3D)            (None, 16, 64, 64,   221184      ['concatenate_13[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_61 (GroupN  (None, 16, 64, 64,   128        ['conv3d_111[0][0]']             \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_87 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_61[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_112 (Conv3D)            (None, 16, 64, 64,   110592      ['re_lu_87[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_62 (GroupN  (None, 16, 64, 64,   128        ['conv3d_112[0][0]']             \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_23 (Conv3DTra  (None, 16, 64, 64,   8192       ['re_lu_86[0][0]']               \n",
      " nspose)                        64)                                                               \n",
      "                                                                                                  \n",
      " re_lu_88 (ReLU)                (None, 16, 64, 64,   0           ['group_normalization_62[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " group_normalization_63 (GroupN  (None, 16, 64, 64,   128        ['conv3d_transpose_23[0][0]']    \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 64, 64,   0           ['re_lu_88[0][0]',               \n",
      "                                64)                               'group_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_89 (ReLU)                (None, 16, 64, 64,   0           ['add_31[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_113 (Conv3D)            (None, 16, 64, 64,   4112        ['re_lu_74[0][0]']               \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " conv3d_114 (Conv3D)            (None, 16, 64, 64,   1040        ['re_lu_89[0][0]']               \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 64, 64,   0           ['conv3d_113[0][0]',             \n",
      "                                16)                               'conv3d_114[0][0]']             \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 64, 64,   0           ['add_32[0][0]']                 \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " conv3d_115 (Conv3D)            (None, 16, 64, 64,   17          ['activation_23[0][0]']          \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 64, 64,   0           ['conv3d_115[0][0]']             \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " up_sampling3d_10 (UpSampling3D  (None, 32, 128, 128  0          ['activation_24[0][0]']          \n",
      " )                              , 1)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_24 (Conv3DTra  (None, 32, 128, 128  55328      ['re_lu_89[0][0]']               \n",
      " nspose)                        , 32)                                                             \n",
      "                                                                                                  \n",
      " multiply_10 (Multiply)         (None, 32, 128, 128  0           ['up_sampling3d_10[0][0]',       \n",
      "                                , 32)                             're_lu_74[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 32, 128, 128  0           ['conv3d_transpose_24[0][0]',    \n",
      "                                , 64)                             'multiply_10[0][0]']            \n",
      "                                                                                                  \n",
      " conv3d_116 (Conv3D)            (None, 32, 128, 128  55296       ['concatenate_14[0][0]']         \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_64 (GroupN  (None, 32, 128, 128  64         ['conv3d_116[0][0]']             \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_90 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_64[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_117 (Conv3D)            (None, 32, 128, 128  27648       ['re_lu_90[0][0]']               \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_65 (GroupN  (None, 32, 128, 128  64         ['conv3d_117[0][0]']             \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_transpose_25 (Conv3DTra  (None, 32, 128, 128  2048       ['re_lu_89[0][0]']               \n",
      " nspose)                        , 32)                                                             \n",
      "                                                                                                  \n",
      " re_lu_91 (ReLU)                (None, 32, 128, 128  0           ['group_normalization_65[0][0]'] \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_66 (GroupN  (None, 32, 128, 128  64         ['conv3d_transpose_25[0][0]']    \n",
      " ormalization)                  , 32)                                                             \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 32, 128, 128  0           ['re_lu_91[0][0]',               \n",
      "                                , 32)                             'group_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_92 (ReLU)                (None, 32, 128, 128  0           ['add_33[0][0]']                 \n",
      "                                , 32)                                                             \n",
      "                                                                                                  \n",
      " conv3d_118 (Conv3D)            (None, 32, 128, 128  1032        ['re_lu_71[0][0]']               \n",
      "                                , 8)                                                              \n",
      "                                                                                                  \n",
      " conv3d_119 (Conv3D)            (None, 32, 128, 128  264         ['re_lu_92[0][0]']               \n",
      "                                , 8)                                                              \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 32, 128, 128  0           ['conv3d_118[0][0]',             \n",
      "                                , 8)                              'conv3d_119[0][0]']             \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 128, 128  0           ['add_34[0][0]']                 \n",
      "                                , 8)                                                              \n",
      "                                                                                                  \n",
      " conv3d_120 (Conv3D)            (None, 32, 128, 128  9           ['activation_25[0][0]']          \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 32, 128, 128  0           ['conv3d_120[0][0]']             \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " up_sampling3d_11 (UpSampling3D  (None, 64, 256, 256  0          ['activation_26[0][0]']          \n",
      " )                              , 1)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_26 (Conv3DTra  (None, 64, 256, 256  13840      ['re_lu_92[0][0]']               \n",
      " nspose)                        , 16)                                                             \n",
      "                                                                                                  \n",
      " multiply_11 (Multiply)         (None, 64, 256, 256  0           ['up_sampling3d_11[0][0]',       \n",
      "                                , 16)                             're_lu_71[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 64, 256, 256  0           ['conv3d_transpose_26[0][0]',    \n",
      "                                , 32)                             'multiply_11[0][0]']            \n",
      "                                                                                                  \n",
      " conv3d_121 (Conv3D)            (None, 64, 256, 256  13824       ['concatenate_15[0][0]']         \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_67 (GroupN  (None, 64, 256, 256  32         ['conv3d_121[0][0]']             \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_93 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_67[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_122 (Conv3D)            (None, 64, 256, 256  6912        ['re_lu_93[0][0]']               \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_68 (GroupN  (None, 64, 256, 256  32         ['conv3d_122[0][0]']             \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_transpose_27 (Conv3DTra  (None, 64, 256, 256  512        ['re_lu_92[0][0]']               \n",
      " nspose)                        , 16)                                                             \n",
      "                                                                                                  \n",
      " re_lu_94 (ReLU)                (None, 64, 256, 256  0           ['group_normalization_68[0][0]'] \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " group_normalization_69 (GroupN  (None, 64, 256, 256  32         ['conv3d_transpose_27[0][0]']    \n",
      " ormalization)                  , 16)                                                             \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 64, 256, 256  0           ['re_lu_94[0][0]',               \n",
      "                                , 16)                             'group_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_95 (ReLU)                (None, 64, 256, 256  0           ['add_35[0][0]']                 \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_123 (Conv3D)            (None, 64, 256, 256  8227        ['re_lu_95[0][0]']               \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 64, 256, 256  0           ['conv3d_123[0][0]']             \n",
      "                                , 19)                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,840,463\n",
      "Trainable params: 13,840,463\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "features = tf.keras.layers.Input(shape=input_shape[params['subset']])\n",
    "print('AttResUnet3d input:',yshape[params['subset']][-1],params['mode'],features,params['augment'])\n",
    "output = resAtt_unet3d(yshape[params['subset']][-1],params['mode'],features,params['normalization'])\n",
    "attres_unet_model = tf.keras.Model(inputs=features, outputs=output)\n",
    "attres_unet_model.compile(optimizer=Adam(learning_rate=params['lr']), # type: ignore\n",
    "                   loss=make_loss, \n",
    "                   metrics=[eval_dice])\n",
    "attres_unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "    104/Unknown - 5164s 47s/step - loss: 19.0376 - eval_dice: 0.4053"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history1 = attres_unet_model.fit(\n",
    "                train_dynamic,\n",
    "                epochs=60,\n",
    "                validation_data=val_dynamic,\n",
    "                callbacks=callbacks_list  # callbacks\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-(2024-07-26)/11.10.14/cp.ckpt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was saved during training, reconstruction must be made with the weights. Reusing the model of training (same model)\n",
    "\n",
    "\n",
    "Example of file path for log analysis: '**logs/Linear Interpolation/DATASET_DYNAMIC/BatchNorm/residual_unet3d_2023-11-20/11.53.28/cp.ckpt**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_t = 'logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/unet3d-GN(2024-01-18)/21.54.47/cp.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/attunet3d_GN(2024-04-09)/16.19.19/cp.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#current_model = res_unet_model \n",
    "#current_model = attres_unet_model\n",
    "#current_model = unet3d_model\n",
    "current_model = attunet_model\n",
    "current_file_path = file_path\n",
    "current_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1 - Learning Curves (Current model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history1.history\n",
    "#history_dict = history1_unet.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAJOCAYAAADlMzAmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB9XklEQVR4nO3de1zT9fcH8NcYAnL3DgiC9/stU1NDMU1FI5UsU1O01DIsrexi5bWL3X5pmWllaWVqqXjJvCsomabmvczUUBFR8wIIipfx/v3x/m4w2GD3zzZez8djj22ffS5nY8rhfTsqIYQAERERETk1D6UDICIiIqKyMWkjIiIicgFM2oiIiIhcAJM2IiIiIhfApI2IiIjIBTBpIyIiInIBTNqIiIiIXACTNiIiIiIXwKSNiIiIyAUwaSNyM8OHD0dUVJRFx06dOhUqlcq2ATmZ06dPQ6VSYeHChQ6/tkqlwtSpU3XPFy5cCJVKhdOnT5d5bFRUFIYPH27TeKz5rhCR4zFpI3IQlUpl0i0lJUXpUMu9559/HiqVCidPnjS6zxtvvAGVSoXDhw87MDLznT9/HlOnTsXBgweVDkVHmzh/9NFHSodC5FI8lQ6AqLz4/vvv9Z5/99132Lx5c4ntjRs3tuo6X331FQoKCiw69s0338Rrr71m1fXdwZAhQzB79mwsXrwYkydPNrjPkiVL0Lx5c7Ro0cLi6wwdOhSPP/44vL29LT5HWc6fP49p06YhKioKrVq10nvNmu8KETkekzYiB3niiSf0nu/evRubN28usb24GzduwNfX1+TrVKhQwaL4AMDT0xOenvxvoX379qhXrx6WLFliMGnbtWsX0tLS8N5771l1HbVaDbVabdU5rGHNd4WIHI/do0ROJCYmBs2aNcMff/yBzp07w9fXF6+//joAYPXq1ejTpw/CwsLg7e2NunXr4q233oJGo9E7R/FxSkW7or788kvUrVsX3t7eaNu2Lfbu3at3rKExbSqVCmPHjsWqVavQrFkzeHt7o2nTptiwYUOJ+FNSUnDvvffCx8cHdevWxRdffGHyOLnU1FQ8+uijqFWrFry9vREREYEXXngBN2/eLPH+/P39kZGRgX79+sHf3x/VqlXDhAkTSnwWWVlZGD58OIKCghAcHIyEhARkZWWVGQsgW9v+/vtv7N+/v8RrixcvhkqlwqBBg3D79m1MnjwZbdq0QVBQEPz8/BAdHY3k5OQyr2FoTJsQAm+//TbCw8Ph6+uLrl274s8//yxx7NWrVzFhwgQ0b94c/v7+CAwMRGxsLA4dOqTbJyUlBW3btgUAjBgxQtcFrx3PZ2hMW15eHl566SVERETA29sbDRs2xEcffQQhhN5+5nwvLHXp0iU89dRTqFGjBnx8fNCyZUt8++23JfZbunQp2rRpg4CAAAQGBqJ58+b45JNPdK/fuXMH06ZNQ/369eHj44MqVarg/vvvx+bNm20WK5Ej8E9qIidz5coVxMbG4vHHH8cTTzyBGjVqAJC/4P39/fHiiy/C398f27Ztw+TJk5GTk4MPP/ywzPMuXrwY169fx9NPPw2VSoUPPvgA8fHx+Pfff8tscfn111+RlJSEZ599FgEBAfj000/xyCOP4OzZs6hSpQoA4MCBA+jVqxdCQ0Mxbdo0aDQaTJ8+HdWqVTPpfS9btgw3btzAmDFjUKVKFezZswezZ8/GuXPnsGzZMr19NRoNevbsifbt2+Ojjz7Cli1b8H//93+oW7cuxowZA0AmP3379sWvv/6KZ555Bo0bN8bKlSuRkJBgUjxDhgzBtGnTsHjxYtxzzz161/7pp58QHR2NWrVq4fLly5g/fz4GDRqEUaNG4fr16/j666/Rs2dP7Nmzp0SXZFkmT56Mt99+G71790bv3r2xf/9+9OjRA7dv39bb799//8WqVavw6KOPonbt2rh48SK++OILdOnSBX/99RfCwsLQuHFjTJ8+HZMnT8bo0aMRHR0NAOjYsaPBawsh8PDDDyM5ORlPPfUUWrVqhY0bN+Lll19GRkYGZs6cqbe/Kd8LS928eRMxMTE4efIkxo4di9q1a2PZsmUYPnw4srKyMG7cOADA5s2bMWjQIHTr1g3vv/8+AODYsWPYuXOnbp+pU6dixowZGDlyJNq1a4ecnBzs27cP+/fvx4MPPmhVnEQOJYhIEYmJiaL4P8EuXboIAGLevHkl9r9x40aJbU8//bTw9fUV+fn5um0JCQkiMjJS9zwtLU0AEFWqVBFXr17VbV+9erUAIH7++WfdtilTppSICYDw8vISJ0+e1G07dOiQACBmz56t2xYXFyd8fX1FRkaGbtuJEyeEp6dniXMaYuj9zZgxQ6hUKnHmzBm99wdATJ8+XW/f1q1bizZt2uier1q1SgAQH3zwgW7b3bt3RXR0tAAgFixYUGZMbdu2FeHh4UKj0ei2bdiwQQAQX3zxhe6ct27d0jvu2rVrokaNGuLJJ5/U2w5ATJkyRfd8wYIFAoBIS0sTQghx6dIl4eXlJfr06SMKCgp0+73++usCgEhISNBty8/P14tLCPmz9vb21vts9u7da/T9Fv+uaD+zt99+W2+/AQMGCJVKpfcdMPV7YYj2O/nhhx8a3WfWrFkCgFi0aJFu2+3bt0WHDh2Ev7+/yMnJEUIIMW7cOBEYGCju3r1r9FwtW7YUffr0KTUmIlfA7lEiJ+Pt7Y0RI0aU2F6xYkXd4+vXr+Py5cuIjo7GjRs38Pfff5d53oEDB6JSpUq659pWl3///bfMY7t37466devqnrdo0QKBgYG6YzUaDbZs2YJ+/fohLCxMt1+9evUQGxtb5vkB/feXl5eHy5cvo2PHjhBC4MCBAyX2f+aZZ/SeR0dH672XdevWwdPTU9fyBsgxZM8995xJ8QByHOK5c+ewY8cO3bbFixfDy8sLjz76qO6cXl5eAICCggJcvXoVd+/exb333muwa7U0W7Zswe3bt/Hcc8/pdSmPHz++xL7e3t7w8JD/hWs0Gly5cgX+/v5o2LCh2dfVWrduHdRqNZ5//nm97S+99BKEEFi/fr3e9rK+F9ZYt24dQkJCMGjQIN22ChUq4Pnnn0dubi62b98OAAgODkZeXl6pXZ3BwcH4888/ceLECavjIlISkzYiJ1OzZk1dElDUn3/+if79+yMoKAiBgYGoVq2abhJDdnZ2meetVauW3nNtAnft2jWzj9Uerz320qVLuHnzJurVq1diP0PbDDl79iyGDx+OypUr68apdenSBUDJ9+fj41Oi27VoPABw5swZhIaGwt/fX2+/hg0bmhQPADz++ONQq9VYvHgxACA/Px8rV65EbGysXgL87bffokWLFrrxUtWqVcMvv/xi0s+lqDNnzgAA6tevr7e9WrVqetcDZII4c+ZM1K9fH97e3qhatSqqVauGw4cPm33dotcPCwtDQECA3nbtjGZtfFplfS+scebMGdSvX1+XmBqL5dlnn0WDBg0QGxuL8PBwPPnkkyXG1U2fPh1ZWVlo0KABmjdvjpdfftnpl2ohMoRJG5GTKdripJWVlYUuXbrg0KFDmD59On7++Wds3rxZN4bHlGUbjM1SFMUGmNv6WFNoNBo8+OCD+OWXX/Dqq69i1apV2Lx5s27AfPH356gZl9WrV8eDDz6IFStW4M6dO/j5559x/fp1DBkyRLfPokWLMHz4cNStWxdff/01NmzYgM2bN+OBBx6w63Ia7777Ll588UV07twZixYtwsaNG7F582Y0bdrUYct42Pt7YYrq1avj4MGDWLNmjW48XmxsrN7Yxc6dO+PUqVP45ptv0KxZM8yfPx/33HMP5s+f77A4iWyBExGIXEBKSgquXLmCpKQkdO7cWbc9LS1NwagKVa9eHT4+PgYXoy1tgVqtI0eO4J9//sG3336LYcOG6bZbM7svMjISW7duRW5url5r2/Hjx806z5AhQ7BhwwasX78eixcvRmBgIOLi4nSvL1++HHXq1EFSUpJel+aUKVMsihkATpw4gTp16ui2//fffyVar5YvX46uXbvi66+/1tuelZWFqlWr6p6bU+EiMjISW7ZswfXr1/Va27Td79r4HCEyMhKHDx9GQUGBXmuboVi8vLwQFxeHuLg4FBQU4Nlnn8UXX3yBSZMm6Vp6K1eujBEjRmDEiBHIzc1F586dMXXqVIwcOdJh74nIWmxpI3IB2haNoi0Yt2/fxueff65USHrUajW6d++OVatW4fz587rtJ0+eLDEOytjxgP77E0LoLdtgrt69e+Pu3buYO3eubptGo8Hs2bPNOk+/fv3g6+uLzz//HOvXr0d8fDx8fHxKjf3333/Hrl27zI65e/fuqFChAmbPnq13vlmzZpXYV61Wl2jRWrZsGTIyMvS2+fn5AYBJS5307t0bGo0Gn332md72mTNnQqVSmTw+0RZ69+6NCxcu4Mcff9Rtu3v3LmbPng1/f39d1/mVK1f0jvPw8NAteHzr1i2D+/j7+6NevXq614lcBVvaiFxAx44dUalSJSQkJOhKLH3//fcO7YYqy9SpU7Fp0yZ06tQJY8aM0f3yb9asWZkllBo1aoS6detiwoQJyMjIQGBgIFasWGHV2Ki4uDh06tQJr732Gk6fPo0mTZogKSnJ7PFe/v7+6Nevn25cW9GuUQB46KGHkJSUhP79+6NPnz5IS0vDvHnz0KRJE+Tm5pp1Le16czNmzMBDDz2E3r1748CBA1i/fr1e65n2utOnT8eIESPQsWNHHDlyBD/88INeCx0A1K1bF8HBwZg3bx4CAgLg5+eH9u3bo3bt2iWuHxcXh65du+KNN97A6dOn0bJlS2zatAmrV6/G+PHj9SYd2MLWrVuRn59fYnu/fv0wevRofPHFFxg+fDj++OMPREVFYfny5di5cydmzZqlawkcOXIkrl69igceeADh4eE4c+YMZs+ejVatWunGvzVp0gQxMTFo06YNKleujH379mH58uUYO3asTd8Pkd0pM2mViIwt+dG0aVOD++/cuVPcd999omLFiiIsLEy88sorYuPGjQKASE5O1u1nbMkPQ8sroNgSFMaW/EhMTCxxbGRkpN4SFEIIsXXrVtG6dWvh5eUl6tatK+bPny9eeukl4ePjY+RTKPTXX3+J7t27C39/f1G1alUxatQo3RISRZerSEhIEH5+fiWONxT7lStXxNChQ0VgYKAICgoSQ4cOFQcOHDB5yQ+tX375RQAQoaGhJZbZKCgoEO+++66IjIwU3t7eonXr1mLt2rUlfg5ClL3khxBCaDQaMW3aNBEaGioqVqwoYmJixNGjR0t83vn5+eKll17S7depUyexa9cu0aVLF9GlSxe9665evVo0adJEt/yK9r0bivH69evihRdeEGFhYaJChQqifv364sMPP9RbgkT7Xkz9XhSn/U4au33//fdCCCEuXrwoRowYIapWrSq8vLxE8+bNS/zcli9fLnr06CGqV68uvLy8RK1atcTTTz8tMjMzdfu8/fbbol27diI4OFhUrFhRNGrUSLzzzjvi9u3bpcZJ5GxUQjjRn+pE5Hb69evH5RaIiGyAY9qIyGaKl5w6ceIE1q1bh5iYGGUCIiJyI2xpIyKbCQ0NxfDhw1GnTh2cOXMGc+fOxa1bt3DgwIESa48REZF5OBGBiGymV69eWLJkCS5cuABvb2906NAB7777LhM2IiIbYEsbERERkQvgmDYiIiIiF8CkjYiIiMgFuP2YtoKCApw/fx4BAQFmlXMhIiIisjchBK5fv46wsDC9km2GuH3Sdv78eURERCgdBhEREZFR6enpCA8PL3Uft0/atKVO0tPTERgYqHA0RERERIVycnIQERGhy1dK4/ZJm7ZLNDAwkEkbEREROSVThnBxIgIRERGRC2DSRkREROQCmLQRERERuQC3H9NGRERkioKCAty+fVvpMMjNVKhQAWq12ibnYtJGRETl3u3bt5GWloaCggKlQyE3FBwcjJCQEKvXi2XSRkRE5ZoQApmZmVCr1YiIiChzgVMiUwkhcOPGDVy6dAkAEBoaatX5mLQREVG5dvfuXdy4cQNhYWHw9fVVOhxyMxUrVgQAXLp0CdWrV7eqq5R/ThARUbmm0WgAAF5eXgpHQu5K+8fAnTt3rDoPkzYiIiKYtrgpkSVs9d1i0kZERETkApi0EREREQAgKioKs2bNMnn/lJQUqFQqZGVl2S0mKsSkjYiIyAY0GiAlBViyRN7/b6icXahUqlJvU6dOtei8e/fuxejRo03ev2PHjsjMzERQUJBF1zMVk0OJs0eJiIislJQEjBsHnDtXuC08HPjkEyA+3vbXy8zM1D3+8ccfMXnyZBw/fly3zd/fX/dYCAGNRgNPz7J/5VerVs2sOLy8vBASEmLWMWQ5RVvaZsyYgbZt2yIgIADVq1dHv3799L50AJCfn4/ExERUqVIF/v7+eOSRR3Dx4kWFIiYiItKXlAQMGKCfsAFARobcnpRk+2uGhITobkFBQVCpVLrnf//9NwICArB+/Xq0adMG3t7e+PXXX3Hq1Cn07dsXNWrUgL+/P9q2bYstW7bonbd496hKpcL8+fPRv39/+Pr6on79+lizZo3u9eItYAsXLkRwcDA2btyIxo0bw9/fH7169dJLMu/evYvnn38ewcHBqFKlCl599VUkJCSgX79+Fn8e165dw7Bhw1CpUiX4+voiNjYWJ06c0L1+5swZxMXFoVKlSvDz80PTpk2xbt063bFDhgxBtWrVULFiRdSvXx8LFiywOBZ7UjRp2759OxITE7F7925s3rwZd+7cQY8ePZCXl6fb54UXXsDPP/+MZcuWYfv27Th//jzi7fFnCxEREQAhgLw80245OcDzz8tjDJ0HkC1wOTmmnc/QeSz12muv4b333sOxY8fQokUL5Obmonfv3ti6dSsOHDiAXr16IS4uDmfPni31PNOmTcNjjz2Gw4cPo3fv3hgyZAiuXr1qdP8bN27go48+wvfff48dO3bg7NmzmDBhgu71999/Hz/88AMWLFiAnTt3IicnB6tWrbLqvQ4fPhz79u3DmjVrsGvXLggh0Lt3b90SG4mJibh16xZ27NiBI0eO4P3339e1Rk6aNAl//fUX1q9fj2PHjmHu3LmoWrWqVfHYjXAily5dEgDE9u3bhRBCZGVliQoVKohly5bp9jl27JgAIHbt2mXSObOzswUAkZ2dbZeY794VIjlZiMWL5f3du3a5DBER2cnNmzfFX3/9JW7evCmEECI3VwiZPjn+lptrfvwLFiwQQUFBuufJyckCgFi1alWZxzZt2lTMnj1b9zwyMlLMnDlT9xyAePPNN3XPc3NzBQCxfv16vWtdu3ZNFwsAcfLkSd0xc+bMETVq1NA9r1Gjhvjwww91z+/evStq1aol+vbtazTO4tcp6p9//hEAxM6dO3XbLl++LCpWrCh++uknIYQQzZs3F1OnTjV47ri4ODFixAij17aF4t+xoszJU5xqIkJ2djYAoHLlygCAP/74A3fu3EH37t11+zRq1Ai1atXCrl27DJ7j1q1byMnJ0bvZS1ISEBUFdO0KDB4s76Oi7NMUTkREZI57771X73lubi4mTJiAxo0bIzg4GP7+/jh27FiZLW0tWrTQPfbz80NgYKCuLJMhvr6+qFu3ru55aGiobv/s7GxcvHgR7dq1072uVqvRpk0bs95bUceOHYOnpyfat2+v21alShU0bNgQx44dAwA8//zzePvtt9GpUydMmTIFhw8f1u07ZswYLF26FK1atcIrr7yC3377zeJY7M1pkraCggKMHz8enTp1QrNmzQAAFy5cgJeXF4KDg/X2rVGjBi5cuGDwPDNmzEBQUJDuFhERYZd4lRjDQERE9ufrC+Tmmnb737CoMq1bZ9r5bFlFy8/PT+/5hAkTsHLlSrz77rtITU3FwYMH0bx5c9y+fbvU81SoUEHvuUqlQkFBgVn7C1v2+1pg5MiR+PfffzF06FAcOXIE9957L2bPng0AiI2NxZkzZ/DCCy/g/Pnz6Natm153rjNxmqQtMTERR48exdKlS606z8SJE5Gdna27paen2yjCQhqNHKNQ2hiG8ePtO92biIjsQ6UC/PxMu/XoIWeJGlvwXqUCIiLkfqacz55FGXbu3Inhw4ejf//+aN68OUJCQnD69Gn7XdCAoKAg1KhRA3v37tVt02g02L9/v8XnbNy4Me7evYvff/9dt+3KlSs4fvw4mjRpotsWERGBZ555BklJSXjppZfw1Vdf6V6rVq0aEhISsGjRIsyaNQtffvmlxfHYk1Ms+TF27FisXbsWO3bsQHh4uG57SEgIbt++jaysLL3WtosXLxqdYuzt7Q1vb2+7xpuaWrKFrSghgPR0uV9MjF1DISIiBanVclmPAQNkwlX0j3ltAjZrltxPafXr10dSUhLi4uKgUqkwadKkUlvM7OW5557DjBkzUK9ePTRq1AizZ8/GtWvXTCr1dOTIEQQEBOieq1QqtGzZEn379sWoUaPwxRdfICAgAK+99hpq1qyJvn37AgDGjx+P2NhYNGjQANeuXUNycjIaN24MAJg8eTLatGmDpk2b4tatW1i7dq3uNWejaEubEAJjx47FypUrsW3bNtSuXVvv9TZt2qBChQrYunWrbtvx48dx9uxZdOjQwdHh6hSZuWyT/YiIyHXFxwPLlwM1a+pvDw+X251lwYOPP/4YlSpVQseOHREXF4eePXvinnvucXgcr776KgYNGoRhw4ahQ4cO8Pf3R8+ePeHj41PmsZ07d0br1q11N+1YuAULFqBNmzZ46KGH0KFDBwghsG7dOl1XrUajQWJiIho3boxevXqhQYMG+PzzzwHIteYmTpyIFi1aoHPnzlCr1Vb3+tmLSijY0fzss89i8eLFWL16NRo2bKjbHhQUhIoVKwKQAwTXrVuHhQsXIjAwEM899xwAmDxQMCcnB0FBQcjOzkZgYKBN4k5JkZMOypKczJY2IiJnl5+fj7S0NNSuXdukxMEYjUb2sGRmAqGhQHS0c7SwObuCggI0btwYjz32GN566y2lw7GL0r5j5uQpinaPzp07FwAQUyyzWbBgAYYPHw4AmDlzJjw8PPDII4/g1q1b6Nmzpy47Vkp0tPwLKiPD8Lg2lUq+Hh3t+NiIiEgZajX/UDfFmTNnsGnTJnTp0gW3bt3CZ599hrS0NAwePFjp0JyeokmbKY18Pj4+mDNnDubMmeOAiEzjSmMYiIiInImHhwcWLlyICRMmQAiBZs2aYcuWLU47jsyZOMVEBFekHcNgqNbcrFnOM4aBiIjImURERGDnzp1Kh+GSnGbJD1cUHw+cPg2sWCGfq1TA338zYSMiIiLbY9JmJbUa6N8fCAiQ3aRnzigdEREREbkjJm02oFIB9evLxydOKBsLERERuScmbTbCpI2IiIjsiUmbjdSrJ+9PnlQ2DiIiInJPTNpshC1tREREZE9M2myESRsREbmamJgYjB8/Xvc8KioKs2bNKvUYlUqFVatWWX1tW52nPGHSZiPapC09HcjPVzYWIiJSgEYj6xwuWSLvNRq7XSouLg69evUy+FpqaipUKhUOHz5s9nn37t2L0aNHWxuenqlTp6JVq1YltmdmZiI2Ntam1ypu4cKFCA4Otus1HIlJm41UrQoEBcllP06dUjoaIiJyqKQkICpKFqYePFjeR0XJ7Xbw1FNPYfPmzThXdHX3/1mwYAHuvfdetGjRwuzzVqtWDb6+vrYIsUwhISHw9vZ2yLXcBZM2G+GyH0RE5VRSkqxrWDyBysiQ2+2QuD300EOoVq0aFi5cqLc9NzcXy5Ytw1NPPYUrV65g0KBBqFmzJnx9fdG8eXMsWbKk1PMW7x49ceIEOnfuDB8fHzRp0gSbN28uccyrr76KBg0awNfXF3Xq1MGkSZNw584dALKla9q0aTh06BBUKhVUKpUu5uLdo0eOHMEDDzyAihUrokqVKhg9ejRyc3N1rw8fPhz9+vXDRx99hNDQUFSpUgWJiYm6a1ni7Nmz6Nu3L/z9/REYGIjHHnsMFy9e1L1+6NAhdO3aFQEBAQgMDESbNm2wb98+ALKGalxcHCpVqgQ/Pz80bdoU69atszgWU7CMlQ3Vrw/s28ekjYjIpQkB3Lhh2r4aDfD88/pFqIueR6WS9Q67dzetILWvb2ER61J4enpi2LBhWLhwId544w2o/nfMsmXLoNFoMGjQIOTm5qJNmzZ49dVXERgYiF9++QVDhw5F3bp10a5duzKvUVBQgPj4eNSoUQO///47srOz9ca/aQUEBGDhwoUICwvDkSNHMGrUKAQEBOCVV17BwIEDcfToUWzYsAFbtmwBAAQFBZU4R15eHnr27IkOHTpg7969uHTpEkaOHImxY8fqJabJyckIDQ1FcnIyTp48iYEDB6JVq1YYNWpUme/H0PvTJmzbt2/H3bt3kZiYiIEDByIlJQUAMGTIELRu3Rpz586FWq3GwYMHUaFCBQBAYmIibt++jR07dsDPzw9//fUX/P39zY7DLMLNZWdnCwAiOzvb7teaPFkIQIhRo+x+KSIispGbN2+Kv/76S9y8eVNuyM2V/5krccvNNTnuY8eOCQAiOTlZty06Olo88cQTRo/p06ePeOmll3TPu3TpIsaNG6d7HhkZKWbOnCmEEGLjxo3C09NTZGRk6F5fv369ACBWrlxp9BoffvihaNOmje75lClTRMuWLUvsV/Q8X375pahUqZLILfL+f/nlF+Hh4SEuXLgghBAiISFBREZGirt37+r2efTRR8XAgQONxrJgwQIRFBRk8LVNmzYJtVotzp49q9v2559/CgBiz549QgghAgICxMKFCw0e37x5czF16lSj1y6qxHesCHPyFHaP2hC7R4mIyFEaNWqEjh074ptvvgEAnDx5EqmpqXjqqacAABqNBm+99RaaN2+OypUrw9/fHxs3bsTZs2dNOv+xY8cQERGBsLAw3bYOHTqU2O/HH39Ep06dEBISAn9/f7z55psmX6PotVq2bAk/Pz/dtk6dOqGgoADHjx/XbWvatCnURVosQ0NDcenSJbOuVfSaERERiIiI0G1r0qQJgoODcezYMQDAiy++iJEjR6J79+547733cKrIoPXnn38eb7/9Njp16oQpU6ZYNPHDXEzabIhJGxGRG/D1BXJzTbuZOoZp3TrTzmfmJICnnnoKK1aswPXr17FgwQLUrVsXXbp0AQB8+OGH+OSTT/Dqq68iOTkZBw8eRM+ePXH79m1zPxGjdu3ahSFDhqB3795Yu3YtDhw4gDfeeMOm1yhK2zWppVKpUFBQYJdrAXLm659//ok+ffpg27ZtaNKkCVauXAkAGDlyJP79918MHToUR44cwb333ovZs2fbLRaASZtNaasiZGSYPhyCiIicjEoF+PmZduvRAwgPNz4OTaUCIiLkfqacz4TxbEU99thj8PDwwOLFi/Hdd9/hySef1I1v27lzJ/r27YsnnngCLVu2RJ06dfDPP/+YfO7GjRsjPT0dmZmZum27d+/W2+e3335DZGQk3njjDdx7772oX78+zpw5o7ePl5cXNGUsf9K4cWMcOnQIeXl5um07d+6Eh4cHGjZsaHLM5tC+v/T0dN22v/76C1lZWWjSpIluW4MGDfDCCy9g06ZNiI+Px4IFC3SvRURE4JlnnkFSUhJeeuklfPXVV3aJVYtJmw1VqQJUqiQfs5wVEVE5oFYDn3wiHxdPuLTPZ80ybRKCBfz9/TFw4EBMnDgRmZmZGD58uO61+vXrY/Pmzfjtt99w7NgxPP3003ozI8vSvXt3NGjQAAkJCTh06BBSU1Pxxhtv6O1Tv359nD17FkuXLsWpU6fw6aef6lqitKKiopCWloaDBw/i8uXLuHXrVolrDRkyBD4+PkhISMDRo0eRnJyM5557DkOHDkWNGjXM+1CK0Wg0OHjwoN7t2LFj6N69O5o3b44hQ4Zg//792LNnD4YNG4YuXbrg3nvvxc2bNzF27FikpKTgzJkz2LlzJ/bu3YvGjRsDAMaPH4+NGzciLS0N+/fvR3Jysu41e2HSZmPaLlImbURE5UR8PLB8OVCzpv728HC5PT7erpd/6qmncO3aNfTs2VNv/Nmbb76Je+65Bz179kRMTAxCQkLQr18/k8/r4eGBlStX4ubNm2jXrh1GjhyJd955R2+fhx9+GC+88ALGjh2LVq1a4bfffsOkSZP09nnkkUfQq1cvdO3aFdWqVTO47Iivry82btyIq1evom3bthgwYAC6deuGzz77zLwPw4Dc3Fy0bt1a7xYXFweVSoXVq1ejUqVK6Ny5M7p37446dergxx9/BACo1WpcuXIFw4YNQ4MGDfDYY48hNjYW06ZNAyCTwcTERDRu3Bi9evVCgwYN8Pnnn1sdb2lUQhiap+w+cnJyEBQUhOzsbAQGBtr9ek88AfzwA/Dee8Crr9r9ckREZKX8/HykpaWhdu3a8PHxsfxEGg2QmgpkZgKhoUB0tN1a2Mi1lPYdMydP4TptNsbJCERE5ZRaDcTEKB0FuTF2j9oYkzYiIiKyByZtNsakjYiIiOyBSZuNaZO2zEy55A4RERGRLTBps7HgYKBqVfmYM0iJiIjIVpi02QG7SImIXI+bL6ZACrJV1QbOHrWDevWAXbuYtBERuYIKFSpApVLhv//+Q7Vq1XQVBYisJYTA7du38d9//8HDwwNeXl5WnY9Jmx2wpY2IyHWo1WqEh4fj3LlzOH36tNLhkBvy9fVFrVq14OFhXQcnkzY7YNJGRORa/P39Ub9+fdy5c0fpUMjNqNVqeHp62qQFl0mbHbCUFRGR61Gr1VCzggE5MU5EsANt0nbxIpCTo2wsRERE5B6YtNlBYCBQvbp8zNY2IiIisgUmbXbCcW1ERERkS0za7IRJGxEREdkSkzY7YdJGREREtsSkzU7q1ZP3TNqIiIjIFpi02Qlb2oiIiMiWmLTZibal7fJlICtL0VCIiIjIDTBps5OAACAkRD5maxsRERFZi0mbHbGLlIiIiGyFSZsdsZwVERER2QqTNjtiSxsRERHZCpM2O2LSRkRERLbCpM2OmLQRERGRrTBps6O6deX91avyRkRERGQpJm125OcHhIXJx2xtIyIiImswabMzdpESERGRLTBpszMmbURERGQLTNrsjEkbERER2QKTNjtj0kZERES2wKTNzoombUIoGwsRERG5LiZtdqZd9iM7G7h8WdlYiIiIyHUxabOzihWBiAj5mDVIiYiIyFJM2hyA49qIiIjIWkzaHKBePXnPpI2IiIgsxaTNAdjSRkRERNZi0uYATNqIiIjIWkzaHIDLfhAREZG1mLQ5QJ06gEoFXL8OXLqkdDRERETkipi0OYCPD1CrlnzMLlIiIiKyBJM2B+G4NiIiIrIGkzYHYdJGRERE1mDS5iBM2oiIiMgaTNocRJu0sZQVERERWYJJm4MUrYrAZT+IiIjIXEzaHKROHcDDA8jLAy5cUDoaIiIicjVM2hzEywuIjJSPOa6NiIiIzMWkzYE4GYGIiIgsxaTNgZi0ERERkaWYtDkQkzYiIiKyFJM2B2LSRkRERJZi0uZARddqKyhQNhYiIiJyLUzaHCgqClCrgZs3gfPnlY6GiIiIXAmTNgeqUAGoXVs+ZhcpERERmYNJm4NpKyOwnBURERGZg0mbg3EyAhEREVmCSZuDMWkjIiIiS3gqHYDL02iA1FQgMxMIDQWio+VsAyOYtBEREZElmLRZIykJGDcOOHeucFt4OPDJJ0B8vMFDtEnbqVNy2Q8PtnUSERGRCZgyWCopCRgwQD9hA4CMDLk9KcngYZGRgKcnkJ9f8lAiIiIiY5i0WUKjkS1sQpR8Tbtt/Hi5XzGenkCdOvIxu0iJiIjIVEzaLJGaWnozmRBAerrczwCOayMiIiJzMWmzRGamVfsxaSMiIiJzMWmzRGioVftpF9hl0kZERESmYtJmiehoOUtUpTK+T0iI3M8AtrQRERGRuZi0WUKtlst6AMYTt7w84NAhgy9pk7Z//zU4V4GIiIioBCZtloqPB5YvB2rW1N9esyZQty5w/TrQtSuwc2eJQ2vVAry8gNu35XwFIiIiorIwabNGfDxw+jSQnAwsXizvz5wB9u+XXaM5OUCPHsCWLXqHqdVA7dry8ZdfAikpbHEjIiKi0qmEMLTYmPvIyclBUFAQsrOzERgY6LgL37ghk7qNG2Wz2rJlwMMPA5Dr7g4ZIhfY1SqjkAIRERG5IXPyFLa02YuvL7B6tczCbt+W90uW6AopFE3YgDILKRAREVE5x6TNnry9gR9/BIYOBTQaiCFDsGvkfAgBeECDLkjB41iCLkiBSsj+USOFFIiIiKicY8F4e/P0BBYuBPz9oZo7Fx9eG4XG2IkHsQURKKyqkI5wjBOfYGV6PFJTgZgYxSImIiIiJ8SWNkfw8ADmzMFfca8AAJ7EQoRDvwxWTWRgOQagP5JMLrhARERE5QeTNkdRqXDp+XeQDTnIsPjqbh6Q80FmYTxCq7N/lIiIiPQxaXOgaNWvCEKO0dc9IFAL6dj7cSry8hwYGBERETk9RZO2HTt2IC4uDmFhYVCpVFi1apXe67m5uRg7dizCw8NRsWJFNGnSBPPmzVMmWBtQXzKt33P/uky0bg3s2WPngIiIiMhlKJq05eXloWXLlpgzZ47B11988UVs2LABixYtwrFjxzB+/HiMHTsWa9ascXCkNmJiofnufrtx/kQuOnYE3noLuHtXbtfc1uDgrBT89twSHJyVAs1tdqMSERGVF06zuK5KpcLKlSvRr18/3bZmzZph4MCBmDRpkm5bmzZtEBsbi7ffftuk8yq2uK4hGg0QFSUXZSvjY8+tEIw5d0bjM4xFRIcITG6WhBbfjEOYpnACw3l1OM6++Anu+4Ar8hIREbkit1lct2PHjlizZg0yMjIghEBycjL++ecf9OjRw+gxt27dQk5Ojt7NaZRWaF6lkrdRo4D69eF/Jwuv4gOkoTbe2xWNnl89ghCN/ozTEE0G2n04ALtf4Yq8RERE7s6pk7bZs2ejSZMmCA8Ph5eXF3r16oU5c+agc+fORo+ZMWMGgoKCdLeIiAgHRmwCY4Xmw8Pl9i+/BP7+G1izBujaFZ7QoDN+hQolf1jaGacRH49nVykREZGbc/qkbffu3VizZg3++OMP/N///R8SExOxpVgB9qImTpyI7Oxs3S09Pd2BEZvIUKH5tLTCwqMeHkBcHLBtG46/9GWpp/KAQE1NOo58nmr/uImIiEgxTlsR4ebNm3j99dexcuVK9OnTBwDQokULHDx4EB999BG6d+9u8Dhvb294e3s7MlTLqNUmlT24csvfpNPdOMUVeYmIiNyZ07a03blzB3fu3IGHh36IarUaBQUFCkXleL51TZtx6n/4N+DmTTtHQ0REREpRNGnLzc3FwYMHcfDgQQBAWloaDh48iLNnzyIwMBBdunTByy+/jJSUFKSlpWHhwoX47rvv0L9/fyXDdqjmz0bjvDocBSVqKEjaOagtdnyGq5Xq4MKrM4EbN/R30miAlBRgyRJ5z4r0RERELkfRJT9SUlLQtWvXEtsTEhKwcOFCXLhwARMnTsSmTZtw9epVREZGYvTo0XjhhRegKj770ginWvLDQrtfSUK7DwcAKJx8AECXyP0SNhotzq9HJM4CALJ9qkO8OAHBE8cAmzZBjBsH1bnCmaciPByqTz4pHENHREREijAnT3GaddrsxR2SNkAmbrU+1l+nLUMdgfQXZ+G+D+Jx7NBtbBvxPWIPvIM6SAMA3PQKgM/t6xDQb1ItgAoqAKoVy5m4ERERKYhJWxHukrQBsiLCkc9TceNUJnzrhqL5s9FQe6n19vn91zvYNnIxBhx/C/Vxyui5CqBCfpVw+F5Mk5MiiIiIyOGYtBXhTkmbqYQAdr+zFR0mGZ5hW5RmSzLU3WLsHxQRERGV4DYVEcgyKhUQdOuSSfseT+FSIURERK6ASZubyoRpS4WYuh8REREpi0mbm1LHRCMdpS8VcgE1gOhoxwZGREREFmHS5qaiY9SYXkUWpy+euAkAKgD+uI5lE/fj/HnHx0dERETmYdLmptRqIPbLeDyK5ciAfnH6DNTEX2gEf9zA+/u746mmu7F+vUKBEhERkUmYtLmx+HhgyIp43F/zNGKQjEFYjBgk4/7wMzixaC9u3NsZQcjBj1k98HbvnXj5ZeD2baWjJiIiIkO45Ec5oNEAqalAZiYQGiqHsanVAPLyoOkTB/X2ZOTCD72xDvltO2PpUqBOnVKOIyIiIpvgOm1FMGkrw40bQN++wJYtyIMv+uAXHAiMwciRwE8/AUWqXyE8HGD1KyIiItth0lYEkzYT3LwJ9O8PbNyIfI+K6FPwM7ahGzygQTRSEYpMZCIUvyIaBSo1lrP6FRERkU0waSuCSZuJ8vOBAQOAX37BTfjgQ0zACCxEBAqb2tIRjvH4BHsj4pHG6ldERERWY0UEMp+PD7BiBS53fBgVkY9JeBvhRRI2AKiJDCzDANybnoTUVIXiJCIiKqeYtFEhb29sG70UN+ADFVBiWV4PyEbZWRiPCxkah4dHRERUnjFpIz0Nrv0OX+Qbfd0DArWQjkb/samNiIjIkZi0kZ4W1UwrIG/qfkRERGQbTNpIj0dN0wrIm7ofERER2QaTNtIXHQ2Eh0OUUmgeEREsNE9ERORgTNpIn1oNfPIJVCpAqAwnbreeeJLrfRARETkYkzYqKT4eWL4cqpr6hebzVXJWqcdHHwApKYqERkREVF4xaSPD4uOB06eB5GRg8WIgORlJX17BL+iNCnduQvTpAy7WRkRE5DisiEAm02iAe5vlY8bf/dALGwE/P2DjRqBTJ6VDIyIickmsiEB2oVYDk97xQX+sxDaP7kBeHhAbC+zerXRoREREbo9JG5mlf3+gaZuKeKhgNU5EdAWuXwd69gT27lU6NCIiIrfGpI3MolIB774L3IQv2l/8GbfadwZycoAePYA//pB9qCkpwJIl8l7DcldERES2wKSNzPbgg0CXLsC12354uckvckxbVpbcWLMm0LUrMHiwvI+KApKSlA6ZiIjI5TFpI7OpVMA778jHn3/nj1OfrQcaNJBj3C5e1N85IwMYMICJGxERkZWYtJFFOnUCeveWvZ9T3vcFcnMN76idnDx+PLtKiYiIrMCkjSz29tvyPmNpKnD+vPEdhQDS07muGxERkRWYtJHFWrcGHnsMCEGmaQdkmrgfERERlcCkjawybRpwURVq2s6hJu5HREREJTBpI6s0agTUHhaNdISjAIYLzEOlAiIigOhoxwZHRETkRpi0kdUmT1PjJfUnAABhLHGbNUuWVCAiIiKLMGkjq0VGAiHPxmMAluOSV039F319geXLZQF6IiIishiTNrKJ118HNvrGI+z2aSx6KhlHHpkKABAVKgBxccoGR0RE5AaYtJFNhITIEqQFUGPo1zFoteJN/IeqUGVnY8eMnUqHR0RE5PKYtJFNJCUBq1YVPi+AGuvQGwCwd8paFkQgIiKyEpM2sppGA4wbV1j8QGstHgIAPISfWRCBiIjISkzayGqpqcC5cyW3b0RP3EYFNMQ/8En/hwURiIiIrMCkjaxmrNDBdQRiO7oAAB7CWhZEICIisgKTNrJaaYUOtF2kcfiZBRGIiIiswKSNrBYdDYSHy8IHxf0MudxHNFIR3TzLsYERERG5ESZtZDW1GvhEFkQokbiloQ7+RBN4QgP15g2OD46IiMhNMGkjm4iPl4UPahYriODtDXj2k12k+PlnxwdGRETkJpi0kc3ExwOnTwPJycBHH8ltBQVArWf/VxFh/Xrg7l3F4iMiInJlTNrIptRqICYGePFFWZP0zh0gJf8+oHJl4No14LfflA6RiIjIJTFpI7tQqYBeveTj9Zs9gd6yOgK7SImIiCzDpI3sRpu0bdiAwqLxa9cqFg8REZErY9JGdvPAA4CnJ3DiBJDWoKd88vffwMmTSodGRETkcpi0kd0EBgKdOsnH638LAjp3lk/YRUpERGQ2Jm1kV+wiJSIisg0mbWRX2qRt2zbgVo//rde2YweQna1cUERERC6ISRvZVcuWQEgIkJcH7LxQD2jUSK7VtnGj0qERERG5FCZtZFcqFdCzp3y8YQOAh1gdgYiIyBJM2sjuYmPlvd64tnXrWB2BiIjIDEzayO66dwc8PIAjR4CMyI5ApUrA1avA7t1Kh0ZEROQymLSR3VWpArRrJx9v2OJZ2PTGLlIiIiKTMWkjhzC49AeTNiIiIpMxaSOH0CZtmzcDd7v3kpXljx0DTp1SNjAiIiIXwaSNHOLee4HKleXybL8fDwaio+ULXGiXiIjIJEzayCHUaqBHD/mYXaRERETmY9JGDmNwXNv27UBOjmIxERERuQombeQw2pa2ffuAS0H1gQYNWB2BiIjIREzayGFCQ4FWreTjzZvBLlIiIiIzMGkjh9LrItWWtFq3DtBoFIuJiIjIFTBpI4fSJm0bNwIFHToBwcHAlSusjkBERFQGJm3kUB06AAEBwH//AQeOViisjsClP4iIiErFpI0cyssL6NZNPtbrIuW4NiIiolIxaSOH0xvXFhsrF3H7808gLU3RuIiIiJwZkzZyuJ495f2uXUCWqhJw//1yA7tIiYiIjGLSRg4XFQU0aiQnjG7disIu0m+/BZYsAVJSOJuUiIioGCZtpAi9LlJvb/nkjz+AwYOBrl1lZpeUpFR4RERETodJGylCm7RhZRLEuHEld8jIAAYMYOJGRET0P0zaSBGdOwO+3hpMvjIOEKLkDtpt48ezq5SIiAhM2kghFSsCiS1SEYFzUBnbSQggPR1ITXVkaERERE6JSRsppmujTNN2zDRxPyIiIjfGpI0U07R7qGk7hpq4HxERkRtj0kaKiRgcjfPqcBQY6yBVqYCICCA62rGBEREROSEmbaQYlacaP3f7BAAgDCVuQgCzZsmKCUREROUckzZSVMiz8RiA5bigrml4h5pGthMREZUzTNpIUQ88APzsGY9wzWlkLEoGFi8GkpOBIUPkDqNGAXfuKBskERGRE2DSRooKCJClRwugxqqsGGDQICAmRnaLVq0KHDkCfPihwlESEREpj0kbKU5bHWHRoiKlRytVBWbOlC9Mnw6cOKFYfERERM6ASRsprkIFeb97d7HSoxWHAD16ALduAU8/bbhyAhERUTnBpI0UlZQETJhQcntGBjDgURU29J0ryyckJwMLFjg+QCIiIifBpI0Uo9EA48ooPTr6vToomDpdPpkwAbh40XEBEhEROREmbaSY1FTg3Dnjr+tKj7YZD9xzD3DtmiwgT0REVA4xaSPFmFpS9PwlT+Crr+Qiu0uXAuvW2TcwIiIiJ8SkjRRjaknR0FDIlrYXXpAbxowBcnPtFhcREZEzYtJGiomOBsLDZYlRQ0qUHp06VU4rPXsWePNNB0VJRETkHJi0kWLUauATWXq0ROKmfa5XetTPD/jiC/n400+BPXscESYREZFTYNJGioqPB5YvL1litEYNuT0+vtgBPXoATzwhZymMHAls2VJkRV6No8ImIiJyOJUQ7r1iaU5ODoKCgpCdnY3AwEClwyEjNBo5m/T552Xlqv/7P+DFF43s/N9/QJ06Jce1hYfLprsSmR4REZFzMidPYUsbOQW1WpYcHTZMPt+2rZSdU1MNT0TIyAAGDJAr9hIREbkZRZO2HTt2IC4uDmFhYVCpVFi1alWJfY4dO4aHH34YQUFB8PPzQ9u2bXH27FnHB0sO0b27vE9JAW7fNrCDdkVeQ7SNxuPHs6uUiIjcjqJJW15eHlq2bIk5c+YYfP3UqVO4//770ahRI6SkpODw4cOYNGkSfHx8HBwpOUqLFkC1akBenqxFWoLJK/Km2i1GIiIiJXgqefHY2FjExsYaff2NN95A79698cEHH+i21a1b1xGhkUI8PGRr25Ilco5B587FdjB1RV5T9yMiInIRTjumraCgAL/88gsaNGiAnj17onr16mjfvr3BLtSibt26hZycHL0buRZtF+nmzQZeNGtFXiIiIvfhtEnbpUuXkJubi/feew+9evXCpk2b0L9/f8THx2P79u1Gj5sxYwaCgoJ0t4iICAdGTbbw4IPyfs8eIDu72Itmr8hLRETkHpw2aSsoKAAA9O3bFy+88AJatWqF1157DQ899BDmzZtn9LiJEyciOztbd0tPT3dUyGQjERFAw4ZAQQGQnFzsxdJW5AXkmDa9FXmJiIjcg9MmbVWrVoWnpyeaNGmit71x48alzh719vZGYGCg3o1cT6ldpMZW5AWAdu24ThsREbklp03avLy80LZtWxw/flxv+z///IPIyEiFoiJH0XaRbtliZIf4eOD0adkUt3gx8O23cvvevcDffzsiRCIiIodSdPZobm4uTp48qXuelpaGgwcPonLlyqhVqxZefvllDBw4EJ07d0bXrl2xYcMG/Pzzz0hJSVEuaHKImBjZw/nPP7I+fK1aBnbSrsirlZQErF4tyyl89ZWDIiUiInIMRctYpaSkoGvXriW2JyQkYOHChQCAb775BjNmzMC5c+fQsGFDTJs2DX379jX5Gixj5bo6dgR27QLmzweeesqEA377DejUCfDykq1wnEFKREROzpw8hbVHyWlNmQJMnw48/rhct80k998P7NwJvPYaMGOGXeMjIiKyFmuPklvQTkbYskXOJDXJK6/I+7lzAa7RR0REboRJGzmt++4D/P2By5eBw4dNPOihh4BGjeQCbxzXRkREboRJGzmtChUK5xkYXPrDEA8P4OWX5eNZs4xUnSciInI9TNrIqZW6XpsxQ4bISQjnzgFLl9olLiIiIkezKGlLT0/HuXPndM/37NmD8ePH48svv7RZYERA4XptqalAfr6JB3l7A+PGyccffCCrJBAREbk4i5K2wYMHI/l/9YUuXLiABx98EHv27MEbb7yB6dOn2zRAKt8aNwbCwmTCtnOnGQc+/TQQEAD8+Sewfr3d4iMiInIUi5K2o0ePol27dgCAn376Cc2aNcNvv/2GH374Qbe+GpEtqFQWdpEGB8vEDZCtbURERC7OoqTtzp078Pb2BgBs2bIFDz/8MACgUaNGyMzMtF10RDChpJUx48bJ2QzbtwN79tg8LiIiIkeyKGlr2rQp5s2bh9TUVGzevBm9evUCAJw/fx5VqlSxaYBE3brJ+/37gStXzDgwPFxOSgCADz+0eVxERESOZFHS9v777+OLL75ATEwMBg0ahJYtWwIA1qxZo+s2JbKV0FCgWTM5n2DrVjMPnjBB3q9YARSpc0tERORqLEraYmJicPnyZVy+fBnffPONbvvo0aMxb948mwVHpGVxF2nTpkCfPjLj+7//s3lcREREjmJR0nbz5k3cunULlSpVAgCcOXMGs2bNwvHjx1G9enWbBkgE6E9GMHsFD21pqwULgIsXbRoXERGRo1iUtPXt2xffffcdACArKwvt27fH//3f/6Ffv36YO3euTQMkAoDOneWcgtOngX//NfPg6GigfXvg1i3gs8/sER4REZHdWZS07d+/H9HR0QCA5cuXo0aNGjhz5gy+++47fPrppzYNkAiQNUg7dJCPzVr6A5Drhmhb2+bMAXJzbRobERGRI1iUtN24cQMBAQEAgE2bNiE+Ph4eHh647777cObMGZsGSKSlHddmdtIGAH37AvXrA9euAV9/bdO4iIiIHMGipK1evXpYtWoV0tPTsXHjRvTo0QMAcOnSJQQGBto0QCItbdK2bRug0Zh5sFpdOJP044+BO3dsGhsREZG9WZS0TZ48GRMmTEBUVBTatWuHDv/rt9q0aRNat25t0wCJtNq0AYKCgKws4I8/LDjBsGFA9erA2bPAtGnAkiVASooFGSAREZHjWZS0DRgwAGfPnsW+ffuwceNG3fZu3bph5syZNguOqChPT+CBB+Rji7pIfXwKV+p95x1g8GCga1cgKgpISrJVmERERHZhUdIGACEhIWjdujXOnz+Pc+fOAQDatWuHRo0a2Sw4ouIsXq8NkInZ0qUlt2dkAAMGMHEjIiKnZlHSVlBQgOnTpyMoKAiRkZGIjIxEcHAw3nrrLRQUFNg6RiId7XptO3cCeXlmHKjRyFqkhhZ5024bP55dpURE5LQsStreeOMNfPbZZ3jvvfdw4MABHDhwAO+++y5mz56NSZMm2TpGIp169YDISDmPIDXVjANTU4H/tQgbJASQnm7mSYmIiBzH05KDvv32W8yfPx8PP/ywbluLFi1Qs2ZNPPvss3jnnXdsFiBRUSqVbG37+ms5rq1XLxMPzMy07X5EREQOZlFL29WrVw2OXWvUqBGuXr1qdVBEpbFovbbQUNvuR0RE5GAWJW0tW7bEZwbKAX322Wdo0aKF1UERlUY7AfTIEeDCBRMPio4GwsNlU50hKhUQESH3IyIickIWdY9+8MEH6NOnD7Zs2aJbo23Xrl1IT0/HunXrbBogUXFVqwKtWwMHDgBbtwJDhphwkFoNfPKJnCWqUpWckCAEMHOm3I+IiMgJWdTS1qVLF/zzzz/o378/srKykJWVhfj4ePz555/4/vvvbR0jUQkWdZHGxwPLlwM1axp+3dDMUiIiIiehEsJ2v6kOHTqEe+65BxonWjYhJycHQUFByM7OZoktN7Jli0zcataUkz6N9XoapNHIWaKZmXIM29atwNtvy5MdOwb8r64uERGRvZmTp1jUPUqktE6dAG9vuS7u338DjRubcbBaDcTEFD6/7z5Z0urUKWDKFFmblIiIyMlYXBGBSEkVKwL33y8fv/eelSVEfXyAOXPk408/BQ4etEGEREREtsWkjVxSUhKwb598/N13Nigh2rMn8OijMvMbMwZgZQ8iInIyZnWPxsfHl/p6VlaWNbEQmSQpSU4CLT4aU1tCdPlyOefAbDNnAhs2ALt3A/PnA6NH2yReIiIiWzCrpS0oKKjUW2RkJIYNG2avWInsW0K0Zk3grbfk49deAy5dsjRMIiIim7Pp7FFnxNmj7iUlRXaFliU5WX+ugcnu3gXatpXj2oYNA7791oKTEBERmcacPIVj2sil2L2EqKcnMG+eXEPku++A7dstPBEREZFtMWkjl+KQEqLt2wNPPy0fjxkD3L5txcmIiIhsg0kbuRSHlRB9912genW52C7XbSMiIifApI1ciraEKGA8cZs1ywYlRCtVAj76SD6ePh1IS7PyhERERNZh0kYup7QSorNmWbjchyFPPCFnM9y8CTz/PGuTEhGRopi0kUuKjwdOn5azRBcvlsPQAMCmSwWqVMDcuUCFCsDatXKBuJQUWfLKqhIMRERE5mPSRi5LW0J00CDgmWfktmXLbHyRRo2Al1+WjwcOlOuNDB5sgxIMRERE5mHSRm6hb1/ZIHb0qCwgb1PNmsn74i1r2hIMTNyIiMgBmLSRW6hUCejeXT5evtyGJ9ZogFdeMfya1SUYiIiITMekjdzGo4/Ke5t2kaamAufOGX9dCCA9Xe5HRERkR0zayG307SsLGhw+DPzzj41OavcSDERERKZh0kZuo3JloFs3+dhmrW0OKcFARERUNiZt5FZs3kVaVgkGQE5j9eA/JSIisi/+piG30q+fzKEOHQJOnLDBCUsrwaB9rtEADzwAvPceUFBgg4sSERGVxKSN3EqVKoVdpDabRWqsBEN4OLBoETBkiEzcJk4E+vQB/vvPRhcmIiIqpBLCvWvz5OTkICgoCNnZ2QgMDFQ6HHKA+fOBUaOA1q2B/ftteGKNRs4SzcyUY9iio2VLnBDAN98AY8cC+flAWJismtC5s/FjiIiIYF6ewqSN3M7ly0BIiMyXTp4E6tZ10IWPHpWD6v7+W45xGziw5JIh4eGyu9VmBVKJiMiVmZOnsHuU3E7VqrLKFGCHslaladYM2LcPSEiQY9uWLCm5xhurKBARkYWYtJFbsstCu6bw8wO+/lqWaDCEVRSIiMhCTNrILfXvL4eO7d8P/Puvgy+emgpcu2b8dVZRICIiCzBpI7dUrRoQEyMf27QWqSlYRYGIiOyASRu5rQED5L3Du0hZRYGIiOyASRu5rfh4OYlz3z4gLc2BFzalikKNGnI/IiIiEzFpI7dVvTrQpYt87NAu0tKqKGjl5zs4kyQiIlfHpI3cmmKzSI1VUahZE4iKArKzgQcflEuAEBERmYCL65Jbu3hRFigoKABOnwYiIx0cgKGKCJcvA/ffL1f+bdoU2LEDqFzZwYEREZEz4OK6RP9To4asJgUoMIsUkF2lMTHAoEHyXq2WQW3eLFvd/vwT6N0byM1VIDgiInIlTNrI7Sk2i7Q0UVHApk2yhe3332V36q1bSkdFREROjEkbub1HHpHzAX7/HTh7VuloimjSBFi/XlZR2LwZGDqUVRKIiMgoJm3k9kJCClfXUKSLtDTt2gGrVgFeXrIpcMyYwlJXRERERTBpo3JBsVmkpujeHVi8WC4q99VXwOuvyxa3lBRZdD4lhS1wRETE2aNUPmRmynH/Qsgu0ogIpSMyYP58YNQo+TgoSC4LohUeLtd+i49XJjYiIrILzh4lKiY0VK6yAQArVigbi1EjR8pxbYB+wgbI9dwGDACSkhwfFxEROQUmbVRuOOUs0qI0GiA52fBr2gbx8ePZVUpEVE4xaaNy45FH5P1vvwHnzikbi0GpqaUHJgSQni73M4Zj4YiI3BaTNio3atYEOnWSj52yizQz07T93n4b2LatZEKWlCTXf+vaFRg8WN5HRbFLlYjITTBpo3JFO4t0/nwnbIwKDTVtv61bgW7dZH2uxERg+3a5lsmAASVb6jgWjojIbXD2KJUrX30FjB6tv81pJmZqNLJlLCPD8FptKhVQtSoQFyfXdrt6tfA1Dw9ZYNUQlUq+ybQ0WUaLiIicBmePEhmQlAQ8/XTJ7U7TGKVWy+wRkIlWUdrn8+YBX38NXLggqymMGCErKhhL2ADTxsIREZHTY9JG5YJGA4wbZ7gBy6kmZsbHy67OmjX1t4eHy+3a5sAKFYBevYBvvgHmzjXt3KaOmSMiIqfkqXQARI5gzsTMmBiHhWVYfDzQt68MJjNTjnWLjjbetWnqSsGmjpkjIiKnxKSNygVTG5mcpjFKrTY9e4yOli1xpY2FCw8vLMBKREQuid2jVC6Y2sjkko1RpY2F05o1i5MQiIhcHJM2Khe0jVHGchqVSvYyumxjlLGxcADQuDHQr5/DQyIiItti0kblQrlojIqPB06flqWwFi8GFi0CfH2Bv/4qfPNEROSyuE4blStJSXIWadFJCT4+wA8/OME6bfbwxRfAM88A3t7A/v1AkyZKR0REREVwnTYiI4o2Rn3wgdym0QDduysalv2MHi2XBrl1Cxg6FLhzR+mIiIjIQkzaqNzRTsycMAFo0EDmMevWKR2VnahUcjHeSpVkS9vbbysdERERWYhJG5VbKhXwyCPysVMWkLeVsLDCBXjfeQfYs0fZeIiIyCJM2qhc045jW7cOuHlT2VjsauBA4PHHZV/wsGHAjRtKR0RERGZi0kblWps2QK1aMofZuFHpaOxszhzZ6nb8ODBxotLREBGRmZi0UbmmUhW2tileMN7eKleWtUoB4NNPga1blY2HiIjMwqSNyj3tuLY1a4Dbt5WNxe569gTGjJGPhw8HsrIsP5dGA6SkAEuWyHuNxvr4iIjIKCZtVO516ADUqAFkZ8ulQNzehx8CdevKxerGjbPsHElJQFQU0LUrMHiwvI+KKgfNlUREymHSRuWeWg307y8fu/UsUi0/P+C77wAPD3m/bJl5LWZJScCAAforFAOyYP2AAUzciIjshBURiABs3gz06AFUqwZkZrp4OStTvf46MGOGTN4KCgq3h4fLsleGSkRoNLJFrXjCpqVSyePT0lz3Q9RogNRU+UUIDZUFaV31vRCR02NFBCIzxcTI9Wf/+w/49Velo3GQVq3kfdGEDSjZYnbtGrBzJzB/PjBokPGEDQCEANLTZdJjS44aP8duXyJyYp5KB0DkDCpUAB5+GPj2W/n7uUsXpSOyM40GeOklw69pG98HDwaCgoBLl8w/f2am5bEVZ6hgbGmtgdZcZ8CAwvevpU1ily930wK1ROQq2NJG9D/aWaRJSSUbn9xOamrpLWaArFeqTdgiImT/sfZDKktoqHXxaTlq/JxGIxNDQ6NFtNvGj+cMWSJSFJM2ov958EHA31/mB/v2KR2NnZnaEvbWW0BODnD2rFx9+McfZSuXSmX8mGrV5DgwazkykSoribVXty8RkRmYtBH9j48P0KePfOz2s0hNbQm7/34gIKDwuVotuyUB44nblSsyubOWIxMpU5NYW3b7WsMV1shzhRiJXIyiSduOHTsQFxeHsLAwqFQqrFq1yui+zzzzDFQqFWbNmuWw+Kj8KVodwa3nVUdHl95iplLJLlFDLWbx8XJ8V82a+tvDw+X+BQXAkCGy6oI1HJlImZrE2qrb1xquMFnCFWIkckGKJm15eXlo2bIl5syZU+p+K1euxO7duxEWFuagyKi86t0b8PYGTp4EjhxROho7Kq3FTPt81izjS13ExwOnT8vViBcvlvenT8sWleefl/uMGwe8+aZl2e+NG8Databta4tEKjpadusaU1oS60iusEaeK8RI5KqEkwAgVq5cWWL7uXPnRM2aNcXRo0dFZGSkmDlzplnnzc7OFgBEdna2bQIlt/fww0IAQkyZonQkDrBihRDh4fINa28REXK7pQoKhHj77cLzjR4txN27ph//889CREXpx2TsVrOmeec25o8/hPDxKf1a1nwmtnD3bsmfVdGbSiV/drb4PNw5RiInY06e4tRj2goKCjB06FC8/PLLaNq0qUnH3Lp1Czk5OXo3InNoJ0i6/bg2wHCLWVqadUtbqFTAG28A8+bJx19+CQwcKGejljbO6exZWZoiLk7GVKsW8Oqr8hzGunF9fIC8PMtjBYATJ4BevYD8fKBZs5LdvoBscWzUyLrrWMsVJku4QoxELsyp12l7//334enpiee13S0mmDFjBqZNm2bHqMjdxcUBnp7A0aPAP/8ADRooHZGdqdVydWFbe/ppoEoVOb5txQrg+HHg6lXg/PnCfcLDgf/7P5mkTZsmu0U9PeUacpMmyZJb7dqVXKctJATIzQVOnQIeeghYv17ua67z5+VSJv/9B9xzj0xa/fwKKyKEhMj4fvkFGDVKbvew4d+6plRfEAI4eBD44APTzqnkZAlXm9BB5Goc0PJnEhTrHt23b5+oUaOGyMjI0G0zpXs0Pz9fZGdn627p6ensHiWz9eghe3NmzFA6EjewdWvZXY/aW+fOQhw9WvIcd+8KkZwsxOLF8v7uXdmlGRQkj+veXYibN82L6+pVIZo1k8fXry/ExYuG9zt7Vgh/f7nfnDlmvvlSGOqaDg8v7IZNTxfivfeEaNrUtM9Oe0tOtl2M5kpOdv4YiZyMOd2jTpu0zZw5U6hUKqFWq3U3AMLDw0NERkaafF6OaSNLzJsnf7e0bat0JG7g7l0hqlcv/Ze4h4cQ33wjx8OZ47ffhPDzk+d46CEhbt0y7bi8PCE6dpTHhYYKkZZW+v6zZ8t9AwJkMmWtFSvk+C5DY74AmUwWfd3bW4hHHhGiShXDxxUdj+jMY9q0t9df57g2ov9xizFtQ4cOxeHDh3Hw4EHdLSwsDC+//DI2btyodHjk5vr1k8Oo9u6VQ63ICqmpZZfCKigAatcufdFeQzp0kLNMfXzk/RNPAHfvln7MnTtyFuNvvwHBwcCmTXI5itKMGSOvdf06kJho3XowpiwafPSofBwdLccEXrggl1n58kv5urHP6bnnlC1ur1YDU6YYfq1ozO++C/TsaVmJNKJyTNGkLTc3V5eQAUBaWhoOHjyIs2fPokqVKmjWrJnerUKFCggJCUHDhg2VDJvKgRo15LqyALBypbKxuDx7j3OKiZE/pAoVgGXLgCefNF6HrKAAGDFCjoGrWFGOVWvWrOxrqNXAV1/Ja6xZY90sFVNKiAFyYsiOHXIsXXCw3GZsjTwfH3n/+edy3KCS/v1X3nt56W8PD5ef2w8/AL6+wNatQOvWwK+/Oj5GIhelaNK2b98+tG7dGq1btwYAvPjii2jdujUmT56sZFhEAMrZLFJ7csTCtb16AT/9JJOr778Hnn1WtlQVna2anCzLXv3wg5zssHw50LGj6ddo2hSYOFE+HjsWuHbNslitHYRvaMbvuXNAnTpy+xNPKFc8NzcXmDtXPtbGVnxW8uDBsgm7cWM5ESQmRk72KP7zcrcqCu783shxHNBdqyiOaSNLnTlTOMzowgWlo3Fh2nFOxsZi2XLtriVLCq/z0EPGx1ctWmTZ+fPzhWjUSJ7jqacsO4e9BusfOFA44WPqVMtis9YnnxRO7Cjr53n9uhCDBhW+33bthAgLMz4xw5WVNemEyjW3GNNGpLRatYC2beX/sKVUWKOyWFt9wRyPPw58/bV8vHat8W7IihUtO7+3t+wmBeR1kpPNP0dubulj9yytvtCqlVwbD5DLp2zYYH5s1rh7F5g5Uz5+8cWyf57+/rLV8/PPZcvnnj36y8EA7lFFgRUiyIaYtBGVQrvGLLtIrVRavdLly61bzLe4YcMKx4AZolLJblJLu6fuv19OTACA0aOBmzdNP3bePDnLRTvhwNZJbEKCXB9PCNkNefq0+eewVFKSvF7VqvJnYAqVSn6GlSsbfl37OZX183LWrkdTJp1Y812kcodJG1EptLlEcrLy47tdnj2qLxiSmgpkZRl/3Rar8s+YAYSFySK1b71V9v4FBcDLL8tkT6ORydWPP9onif3kE9lEfO2abMnJz7f8XKYSAvjoI/k4MVFONDBVWbOLtT+v9esNv+7MxelZIYJszKkrIhAprUEDObnw6FHg55/l71qygr2qLxTliFX5g4Jkt16/frJSwcCBQMuWhve9eRMYOrSwuXb6dODNN2Ur0yOPlF0RwVze3jLxu+ce4I8/5DIg2i5de0lNlZMLfHzkJBBzmPpziIuTkxfuu6/w9s8/wGOPlWzJ0nY9lpUAm1KRwhqsEEE2xpY2ojJo/893hj/cyQSOmK0KAH37yqRLowFGjjTcxXXpkmz5WbFCLoGxaJEsz6XtBtUmsYMGyXtbJQy1asnWTJUKmD8f+OYb25zXGG0rW0ICUL26ecea83M4dgxYsEB2AbdsaThhA0zrenREC52p783cz4zKLZUQ1qwS6fxycnIQFBSE7OxsBAYGKh0OuaDDh+XvB29vWaIyIEDpiKhUGo385ZuRYfgXukoluyHT0qxPkjIzZetPdrZMXNq0KWy1qVoVePhheZ1KleRsls6drbueud5+WyaJ3t5yMeGWLW3fsvT33/IzUKlkUmXuOpqm/rx+/x3Yt0/e794t348p4wmnTpXJdb16hevZaScHFL+eNpm21ThLjUZ+D0rrrgeATp2Ab78F6ta1/pra69qzBZFsyqw8xe5zWRXGJT/IWgUFQtStK2fpT56sX/6SnJS2TFTxZUa022y51MJXX+mXoCpekqpOHSH+/tt21zOHRiOXPgGEqFbNPktqjBolz9W3r+XnsOTntWiReTVZPTzkzyI2trCWrL2XoNm/XwhPT+PXAQqXafH1FeLzz80v5VYclxdxOS5Ze9RemLSRLfTrV/L/XP4/6OQM/fKKiLD9D2358tKThW++se31zHX1qhA1ahhPHKxJYi9elHVRASFSU62L09yfl6nr3TVuLERQkHkJniXr5BWXkyNEvXryXPfea/y9paUJERNTuL17d7lIpNbduzIWU/5aLK2mra3/WCGbMSdPYfcoURmSkgqrIxRl654UsgN7dxNpu/aMzRC0ZVespTQaOUv14kXDr1sT45QpcmJFu3ayy9Lc2rGGYjX152VON7iHh3z/x4/LihnatfxKs3ixHGtoCSGAIUPkEiQREcDBg3LyirH3VlAAfPYZ8Nprsss3MFDOAg4IkOPyin6/wsPla8X/03GF7yIZxO7RItjSRtbQLubviJ4UckH2qm7gCjHm5QlRpYo89qef7BF52SzpVnXEz2z+fHkOtVqInTtNP+74cSE6dCg9LkPvraBAiNWrnf+7SAaxIgKRjXCZJSqVKyzpYK8Yv/0WuHIFqF0b6N/f/LhswZJFm6Oj5eultQrWrGl+RQqto0flMisA8M475tW3bdBA/mcyY4bxfbTp19ChcsZxgwaAn5+czWwKLi/i0rhOG1EpXOF3MinIUcuLWMMeMWo0wMcfy8cvvCDLUCklPl4mLKZ2q2rLqg0YIBM3Q12r3t5yxmeVKubFkpcn1+y7eRPo2VMuqGwutVquQVeWGzeA7dvNP7+S30WyGlvaiErhCr+TSUFltdpYWkfUlkxpWTI3xjVrZDWISpWAESOsj9Fa5q53Z6yFrkYNOY7s33/leYyNAzTm+eeBv/6S/yF8950cS2cJU/8KTEyUidupU7KmbVk/Z09PjmdzcUzaiErhCr+TSUHaVhvA9nVEbaW0GLUiImTBd1NpF9MdM0YWfndFhsqqZWTICRWhobKbs3Pn0sdHFLVokVzE2MNDns+aBXNN/StwwAAZY506sou0rJ/z3btAly7AhAnm1cy1B0fWi3XW2rSWcMAYO0VxIgJZy9hYZ+2YYM6iJ4ctL2INQzFWriwHywNy2Ylr18o+z86dcn8vLyHOn7d72Io4cUKIWrXk+6xdW4h//y19/+PHhfDzk/tPnWr99bUzoAz9p1PWDChj38VvvxUiIaFwW4MGQvz2m21iNXVJktJitNcaStZcy5L3ZgGu01YEkzayBUP/7gEhGjaU65cSOeo/eKsYinHLFiECAuQXunlzIc6dK/0c8fFy3yefdETEyjl9unBV7Zo1jS+QfPOmEC1bFia+tvq5W7NAdGnfxbVrhQgNlefy8BBiwgQhbtwo+zhjMZqbEDlyLTlrruXAxJJJWxFM2shWiv5/tnRp4R/Wc+YoHRmRlQ4cECIkpLBV5uhRw/udOFH4S9DYPu4kI0MuzgsIUb26EIcPl0xsnnlGvl6tmtzfluzVgnv1qhDDhun/9TljhnlJiiUJkS3WUDI1sbTmWg5epJhJWxFM2shePv1U/jsOCBAiPV3paIislJYmf3kDQgQHC7FjR8l9EhPl6717Ozw8xVy6VNiS5u8vkzdDScCGDfa5vj1bcH/+ubDVzdQ14bQxmZIQ3bkju9z/+Ud2q7/1lnVryZnT+rVqlWnXqltXiI4dZSWKhx8WYuDAwr/ILU0szcSKCEWwIgLZi0Yj6zz//rtccWDlSusXhCdS1JUrQFwcsGuXXPbihx+Afv3kchr//CPXH7t9G9i2DejaVeloHefaNVn14eRJ4/usWOGapVEuXwYiI+USIsYEBsqfvXaJlLQ0OdmiLGq1ZYP+W7UCYmOBli2BFi2A+vXljOUBA+T1i9L+pzt1KlCxIrB3L7Bvn4zRnpKT5QxjGzAnT2HSRmSFI0eAe+6Rk7KWLzdc7orIpdy4IZfOWLNGPg8OlmuWaVWoIGfhlacvu0YD1KoFnD9v+HVXLhGVkmL/BNzfH6hWTf4h8Pff5h/v5SWTtTt3bB/bBx8A9erJ7/2NG/IPlO+/L/s4a8qcFWNOnsIlP4is0Lw58Oqr8vHYsfq/24hckq+vbDV68EH5vPiX+s4d4NFHZVHe8iI11XjCBsiEwlVLo5i6JtyDD8r/5J5/3vSEfelSmQhdvy7Xvjt6tOw1lKpXB2bPBkaPlosM+/nJ1l1TErbOnWU1iS1bZAuiKes1vfiirOgxZAgwahTw5JOmvTelFue0Waesk+KYNrK3mzfl7HlAiNGjlY6GyAZYdFff4sWmjY9avFjpSM1nSS1Wa5ckMWdWrEYjxMyZln3+lszAtea9WYi1R4kcyMcH+Oor+fjLL4EdO5SNh8hqLLqrz51Lo1iygrg1i0qbWy/Ww0OOcTNF8c/fktq0Tr5gNpM2Ihvo3Fm2rAPyPj9f2XiIrMKiu/rcuTSKpUmKJQlR0WOLV6NISzN+jDWfv7nXsva92RknIhDZSFYW0LgxcOEC8OabwFtvKR0RkYVMHZxuwxl0Ti8pSc5eBPRnMGoTCYV/mVstKQkYN06/hTUiQiZspb0vjUa2uGZmypau6Gj7tEIp8fk76L1x9mgRTNrIkZYvl2O0PT2BAweAZs2UjojIAhoNEBUla3Ea+hXhyrMlrWFpYuMqHJWAWcpNP38mbUUwaSNHEkJORFq9Wk58+vVX5/o/j8hk7t6yZClnT2zcnRt+/kzaimDSRo6WkSG7Sa9fl0NFWrRwq/9fqDxx05YNImfCpK0IJm2khM8/BxITCxcQ1woPl4kcf9+Ry3DDlg0iZ2JOnuLpoJiIypXq1eV98T+JMjJkj1N57VkiF6RWl5/JBkROjkt+ENmYRgO88ILh17RJ3PjxlpXkIyKi8otJG5GNcV1SIiKyByZtRDbGdUmJiMgemLQR2Zg7V7whIiLlMGkjsjF3rnhDRETKYdJGZGOllfID5Jg2BesNExGRi2LSRmQHxuoNAzJZa9zY8TEREZFrY9JGZCfx8cDp07Km9uLFwLZtQJ8+cqmP0aOBggKlIyQiIlfCxXWJ7Kj4uqR16wJNmsiapPPny+SNiIjIFGxpI3KgWrWAd96Rj195hct+EBGR6Zi0ETnY2LHAvfcC2dmyFjcREZEpmLQROZhaDXz1lbxftgz4+WelIyIiIlfApI1IAa1aAS+9JB8nJgLXrysaDhERuQAmbUQKmTIFqF1b1iGdNEnpaIiIyNkxaSNSiK8vMG+efPzpp8CePcrGQ0REzo1JG5GCevQAnnhCVkkYPRq4c0fpiIiIyFkxaSNS2McfA5UrA4cOATNnKh0NERE5KyZtRAqrVk0mbgAwdSpw6pSi4RARkZNi0kbkBIYNAx54ALh5ExgzRnaXEhERFcUyVkROQKWSkxKaNwc2bwa+/15WT8jMBEJDgehoua4bERGVX2xpI3IS9esDkyfLxyNGAF27AoMHy/uoKCApSdHwiIhIYUzaiJxIvXryvqBAf3tGBjBgABM3IqLyjEkbkZPQaAqrJBSnHeM2frzcj4iIyh8mbUROIjUVOHfO+OtCyOoJqamOi4mIiJwHkzYiJ5GZadv9iIjIvTBpI3ISoaG23Y+IiNwLkzYiJxEdDYSHy+U/jKlaVe5HRETlD5M2IiehVgOffCIfG0vcrl4FfvjBcTEREZHzYNJG5ETi44Hly4GaNfW3h4cDXbrIpUASEgrLXhERUfnBpI3IycTHA6dPA8nJwOLF8v70aWDbNuDFF+U+L70EvP46y10REZUnLGNF5ITUaiAmpuT2jz6SBeYnTgRmzAAuXwbmzmWJKyKi8oBJG5ELUamA114DqlQBnnkG+Oor4MoVOc7Nx0cuvJuaypqlRETuiN2jRC5o1Chg2TLAy0uWturdG1i0SNYoZc1SIiL3pBLCvUfF5OTkICgoCNnZ2QgMDFQ6HCKb2rYN6NsXyM01/Lp2Fury5XKsHBERORdz8hS2tBG5sAceALZsATyM/EtmzVIiIvfBpI3Ixd28KZcCMYY1S4mI3AOTNiIXx5qlRETlA5M2IhfHmqVEROUDkzYiF1dWzVKVCoiIYM1SIiJXx6SNyMWZUrN01iyu10ZE5OqYtBG5AWM1SwGgRg2ge3fHx0RERLbFpI3ITRSvWbp6tewWvXABGDmSdUqJiFwdy1gRuZHiNUtr1JBj2ZYtAzp3BsaOVSw0IiKyElvaiNxY+/bABx/Ixy++COzbp2w8RERkOSZtRG5u3Digf3/gzh3gsceArCylIyIiIkswaSNycyoV8M03QO3aQFoaMGIEx7cREbkiJm1E5UBwsBzX5uUFrFollwAhIiLXwqSNqJxo0wb4+GP5+JVXgN27lY2HiIjMw6SNqBx59lng0UeBu3eBgQOBq1eVjoiIiEzFpI2oHFGpgPnzgXr1gLNngYQEOUEhJQVYskTeazRKR0lERIZwnTaiciYwUI5vu+8+YO1aoFo1IDu78PXwcFkWKz5euRiJiKgktrQRlUOtWgHDh8vHRRM2AMjIAAYMAJKSHB0VERGVhkkbUTmk0QC//GL4Ne1yIOPHs6uUiMiZMGkjKodSU4Fz54y/LgSQni73IyIi58Ckjagcysy07X5ERGR/nIhAVA6Fhpq2X1oaUFAAeBj4806jkS1xmZnyfNHRsmA9ERHZB1vaiMqh6Gg5S1SlKn2/N94AGjcGPv8cyMsr3J6UBERFAV27AoMHy/uoKE5eICKyJyZtROWQWi2X9QBKJm4qlbz17QsEBQH//AMkJgIREcBrrwFffSVnlxYfE8dZp0RE9sWkjaicio8Hli8HatbU3x4eLrevWiUTs9mz5WK8164B778PjB5tuOA8Z50SEdmXSghD//26j5ycHAQFBSE7OxuBgYFKh0PkdEwZm6bRyIV4p0wBDh0q+5zJyUBMjF3CJSJyK+bkKZyIQFTOqdVlJ1hqtewuvXFDjmEri7FZp5y8QERkOSZtRGQyU2edTpsmZ5727y8nMgByrNu4cfpj4Vgyi4jIdOweJSKTaTRylmhGhuFxbYY0bCgTt1WrSr6mnQSxfDkTNyIqn8zJUzgRgYhMZsqs0/nzgS+/BGJjAS8v4PhxwwkbwMkLRETmUDRp27FjB+Li4hAWFgaVSoVVRf5nv3PnDl599VU0b94cfn5+CAsLw7Bhw3D+/HnlAiaiMmedPvUUMGoUsG4d8N9/wKRJpZ+PJbOIiEyjaNKWl5eHli1bYs6cOSVeu3HjBvbv349JkyZh//79SEpKwvHjx/Hwww8rECkRFRUfD5w+LWeJLl4s79PSSnZxBgYWjmkrC0tmERGVTtGJCLGxsYiNjTX4WlBQEDZv3qy37bPPPkO7du1w9uxZ1KpVyxEhEpERpsw6BUyfvGDqfkRE5ZVLzR7Nzs6GSqVCcHCw0X1u3bqFW7du6Z7n5OQ4IDIiMkZbMqu0yQtVq8r9iIjIOJeZiJCfn49XX30VgwYNKnV2xYwZMxAUFKS7RUREODBKIiqutMkLWllZHNNGRFQWl0ja7ty5g8ceewxCCMydO7fUfSdOnIjs7GzdLT093UFREpExpU1eaNsWuHsXePhh4MABZeIjInIFTt89qk3Yzpw5g23btpW5hom3tze8vb0dFB0RmSo+XlZVKF4R4fZtuTzI9u1Az57Azp1A/fpKR0tE5HycOmnTJmwnTpxAcnIyqlSponRIRGQFQ5MXKlYEVq+W2w8eBB58UCZuxVvliIjKO0WTttzcXJw8eVL3PC0tDQcPHkTlypURGhqKAQMGYP/+/Vi7di00Gg0uXLgAAKhcuTK8vLyUCpuIbCwoCNiwAbj/fuDkSdnitmMHULmy0pERETkPRctYpaSkoGvXriW2JyQkYOrUqahdu7bB45KTkxFjyloDYBkrIldy+jTQsaPsPu3QAdi8GfDzUzoqIiL7MSdPYe1RInIqR4/KsW5ZWUCvXrLrlA3rROSuWHuUiFxWs2bAL7/IsW4bNgDDhwN37gApKcCSJfLe1DqlGo1lxxEROSO2tBGRU1q/Xi4Dcvcu4O8P5OYWvhYeLtd+K142q6ikJGDcOODcOfOOIyJyJLa0EZHLi40Fxo6Vj4smbICsrjBggEzMDElKkq8XTdhMOY6IyJmxpY2InJJGA0RFlUy8iqpRA/jtN6BKFSAgAPDwKPs4lUq2uKWlySVIDF23+FpyhvYjIrIFtrQRkctLTS09YQOAixeBunWB4GDA0xOoVEmu71bacUIA6emGy2YlJcmEr2tXYPBgeR8VxZY5InIOTr24LhGVX5mZpu2nVsvWMSHkjFNTTZwou2CbNAGaNgWOHAEef7xkUXttl+ry5RwLR0TKYtJGRE4pNNS0/bZsAe67TyZsWVnAtm1AYmLZx+3eLW9lEUJ2qY4fL8twGesqZbcqEdkbx7QRkVPSjk3LyCjZ+gUYH5tmynFVqgATJgDHjgF//SVb2fLzy47p/feBp56Sxxdl6UxVJnpExMV1i2DSRuS6tLNAAf0ETKWS98a6LM097ocfgCeeMD2uOnWAdu2Atm2BmzeBSZNKJoimxOjoJUmYJBI5HyZtRTBpI3JthpKbiAhg1izz12kzdlxKipx0UJaaNWULnjlCQoC9e4Hq1QsrO2iTSnMTPS1Lki+2BhI5JyZtRTBpI3J9liYOph5nTldsTg6wb59MxNatA3buNP19+PsDlSvLeO7cMbxPWUuSWJJ8WZokWtMayGSPyDRM2opg0kZEprCkK3bJErk0iD3cdx9w//1A48ZyhmvjxsDWreYnX3fuyIT0/HnD1zGWJFrTGshWPSLTMWkrgkkbEZnK3K5YU7tVt24FWrUCrlyRid6UKZbF5+EBFBQYf93XF+jSBbh2Dbh6VV7v6lXDrYfF9ewJtG8v32/NmsCIEXIdPENKaw1UolWPyJUxaSuCSRsRmcOc1h5LZriamuiNHy/Pr53haqylTEkjRgDNmgF+fvLm4wOMGQNcvmx4f3u06hG5OiZtRTBpIyJ7Mrdb1dKlTL7+Ghg5sux4Ro+WiwZXrixvx48XxleaJ58EKlQAzp6VS6CUVY3CGt27A82by8kZVasCr70mWwUNKWuMH5GrY9JWBJM2IrI3c7tVLRk/Z2oLXXIyEBNT+NyerYEPPQQEBgJ5efJ25gxw4kTZx1mi+PsqypFj4TjujmyNSVsRTNqIyBHM/WVubqJnaQud9lqOaA00Ndl75hkgIAC4dAk4eBA4dKjsY+LjZYtcmzZybF/R9+aoGa4cd0f2YFaeItxcdna2ACCys7OVDoWISM/du0IkJwuxeLG8v3u39P1XrBBCpZI3mU7Jm3bbihWlHxsern9cRITxYyy51t278hrFjyl6bESE/vtMTja8r7FbjRpCPPmkEElJQixaZPhaln4e4eFlfx6WXIuoNObkKWxpIyJyIZYuNgzYvzVQe4ytW/UqVZJdo5s2Abm5pb/HosfZaoarNkZj4/w47o6swe7RIpi0EZG7cfYxXPYa43f7NrBjB7B2LfDTTzKmsjzwAFC3rpzd6u8vl0X54AMgK8v4MUFBQGKirEeblwecOgVs2VL2tZxl3B25FiZtRTBpIyJyPHu36i1eDAwZYvOwrfLoo8CECXLcXfElTTgWjoxh0lYEkzYiItdgTqJn6qSHxER5rtxceTt8WLbWleXBB4HWrWXL3IULwLx5pr+PSpVkC9+DD8rFkBMTuQYdGcekrQgmbURE7sfeM1yLdnWacq3gYKBzZ3lcTo5p74Fj4QgwL0/xKPVVIiIiJ6RWy+5FoLDVSkv7fNaskslQdLRMlIofU/TYiAi5nznXmj8fWLVKLhL822/AtGmyWkRphADS02XrojEajUw0lyyR9xpN6ee09BhyDUzaiIjIJcXHy+7FmjX1t4eHG+92tDTZM/Vanp5Ahw7A5MnA66+b9j5efBGYPRv4+2/9lrykJNnC17UrMHiwvI+KktuNseQYch3sHiUiIpfmiBmullzL1K7YoiIi5Fi44GBg5kzzxsKxhqtr4pi2Ipi0ERGRIfZehsOUsXDVq8vkcetWGcvt26adu0YNOX6uUiVZSszLC6hdm2vJuSImbUUwaSMiIqWYs9jwjRsycfv6a2DZMvOu4+EhZ6qWhWvJOR9ORCAiInIC5oy78/UFevYE+vc37dwVKxYmf6YkbADw3Xclx84BHAvnKtjSRkREZGf2GAuXnAx06SKrNmzYIBf3NVVIiGxx69pVdsk+/7xlY+EsbZ1jq14hdo8WwaSNiIhciSVr0JV1DCDHvrVpA+zaJUt0maKsGq6WVHqwpkKEJcmesyeWZuUpNi5W73Sys7MFAJGdna10KERERCZZsUIIlUreZBomb9ptK1ZYfszNm0KkpAgxdaoQLVvq72vsNmqUEMuXC3HokBB5eYXXKr5fafEVjdHc47THhofrHxcebvtjrDnOEubkKWxpIyIickKWLEti7jFLlsgxbOYqa+JD1arAggVAhQpyX5VKpj5DhgD//Wf4mLJa9cxdzsTSJVAcvXQKu0eLYNJGRESuyt7dgaaOn+veXZbnOnECuHbN7LdhluBguRRKUJC8BQbKMXs3bhg/plIl4K23Ct+nEMAbb5Qea5UqwJw5ckFktVrehACeegq4fNnwMfZYOoVJWxFM2oiIiAyzZPzcl18CTz9d9rmjomQCJoRslbt2zfg6cq6mtKVTzGVOnuJpm0sSERGRq9GW9RowoLALU8tYWa8GDUw794IF+omNqa168+fLa2RnA1lZcuHhhQvLPq5tW5lgamu6/vFH2cc0aiRb3AoKZAL7338yQS1LZmbZ+9gDW9qIiIjKOXPGwlnSOmfNceYsgaJNEi05xprjrMHu0SKYtBEREZXNnLFw5lR6sPY4eyyBYuvE0hqsiEBERERmUatl69GgQfK+tKTEnEoP1h6n7cIFCpM7LWNduJYcY81xjsKWNiIiIrKIIxeudcQSKNYeZwl2jxbBpI2IiMg9lPeKCEzaiIiIiBTCMW1EREREboZJGxEREZELYNJGRERE5AKYtBERERG5ACZtRERERC6ASRsRERGRC2DSRkREROQCmLQRERERuQAmbUREREQugEkbERERkQtg0kZERETkApi0EREREbkAJm1ERERELoBJGxEREZELYNJGRERE5AI8lQ7A3oQQAICcnByFIyEiIiLSp81PtPlKadw+abt+/ToAICIiQuFIiIiIiAy7fv06goKCSt1HJUxJ7VxYQUEBzp8/j4CAAKhUqjL3z8nJQUREBNLT0xEYGOiACJ0bPw99/Dz08fPQx89DHz8Pffw89PHzkIQQuH79OsLCwuDhUfqoNbdvafPw8EB4eLjZxwUGBpbrL1Fx/Dz08fPQx89DHz8Pffw89PHz0MfPA2W2sGlxIgIRERGRC2DSRkREROQCmLQV4+3tjSlTpsDb21vpUJwCPw99/Dz08fPQx89DHz8Pffw89PHzMJ/bT0QgIiIicgdsaSMiIiJyAUzaiIiIiFwAkzYiIiIiF8CkrYg5c+YgKioKPj4+aN++Pfbs2aN0SIqYOnUqVCqV3q1Ro0ZKh+UwO3bsQFxcHMLCwqBSqbBq1Sq914UQmDx5MkJDQ1GxYkV0794dJ06cUCZYBynrMxk+fHiJ70yvXr2UCdbOZsyYgbZt2yIgIADVq1dHv379cPz4cb198vPzkZiYiCpVqsDf3x+PPPIILl68qFDE9mXK5xETE1Pi+/HMM88oFLF9zZ07Fy1atNCtPdahQwesX79e93p5+m4AZX8e5em7YQtM2v7nxx9/xIsvvogpU6Zg//79aNmyJXr27IlLly4pHZoimjZtiszMTN3t119/VTokh8nLy0PLli0xZ84cg69/8MEH+PTTTzFv3jz8/vvv8PPzQ8+ePZGfn+/gSB2nrM8EAHr16qX3nVmyZIkDI3Sc7du3IzExEbt378bmzZtx584d9OjRA3l5ebp9XnjhBfz8889YtmwZtm/fjvPnzyM+Pl7BqO3HlM8DAEaNGqX3/fjggw8Uiti+wsPD8d577+GPP/7Avn378MADD6Bv3774888/AZSv7wZQ9ucBlJ/vhk0IEkII0a5dO5GYmKh7rtFoRFhYmJgxY4aCUSljypQpomXLlkqH4RQAiJUrV+qeFxQUiJCQEPHhhx/qtmVlZQlvb2+xZMkSBSJ0vOKfiRBCJCQkiL59+yoSj9IuXbokAIjt27cLIeT3oUKFCmLZsmW6fY4dOyYAiF27dikVpsMU/zyEEKJLly5i3LhxygWlsEqVKon58+eX+++GlvbzEILfDXOxpQ3A7du38ccff6B79+66bR4eHujevTt27dqlYGTKOXHiBMLCwlCnTh0MGTIEZ8+eVTokp5CWloYLFy7ofVeCgoLQvn37cvtd0UpJSUH16tXRsGFDjBkzBleuXFE6JIfIzs4GAFSuXBkA8Mcff+DOnTt635FGjRqhVq1a5eI7Uvzz0Prhhx9QtWpVNGvWDBMnTsSNGzeUCM+hNBoNli5diry8PHTo0KHcfzeKfx5a5fG7YSm3rz1qisuXL0Oj0aBGjRp622vUqIG///5boaiU0759eyxcuBANGzZEZmYmpk2bhujoaBw9ehQBAQFKh6eoCxcuAIDB74r2tfKoV69eiI+PR+3atXHq1Cm8/vrriI2Nxa5du6BWq5UOz24KCgowfvx4dOrUCc2aNQMgvyNeXl4IDg7W27c8fEcMfR4AMHjwYERGRiIsLAyHDx/Gq6++iuPHjyMpKUnBaO3nyJEj6NChA/Lz8+Hv74+VK1eiSZMmOHjwYLn8bhj7PIDy992wFpM2KiE2Nlb3uEWLFmjfvj0iIyPx008/4amnnlIwMnJWjz/+uO5x8+bN0aJFC9StWxcpKSno1q2bgpHZV2JiIo4ePVquxnyWxtjnMXr0aN3j5s2bIzQ0FN26dcOpU6dQt25dR4dpdw0bNsTBgweRnZ2N5cuXIyEhAdu3b1c6LMUY+zyaNGlS7r4b1mL3KICqVatCrVaXmMFz8eJFhISEKBSV8wgODkaDBg1w8uRJpUNRnPb7wO9K6erUqYOqVau69Xdm7NixWLt2LZKTkxEeHq7bHhISgtu3byMrK0tvf3f/jhj7PAxp3749ALjt98PLywv16tVDmzZtMGPGDLRs2RKffPJJuf1uGPs8DHH374a1mLRBfqHatGmDrVu36rYVFBRg69atev3u5VVubi5OnTqF0NBQpUNRXO3atRESEqL3XcnJycHvv//O70oR586dw5UrV9zyOyOEwNixY7Fy5Ups27YNtWvX1nu9TZs2qFChgt535Pjx4zh79qxbfkfK+jwMOXjwIAC45ffDkIKCAty6davcfTeM0X4ehpS374bZlJ4J4SyWLl0qvL29xcKFC8Vff/0lRo8eLYKDg8WFCxeUDs3hXnrpJZGSkiLS0tLEzp07Rffu3UXVqlXFpUuXlA7NIa5fvy4OHDggDhw4IACIjz/+WBw4cECcOXNGCCHEe++9J4KDg8Xq1avF4cOHRd++fUXt2rXFzZs3FY7cfkr7TK5fvy4mTJggdu3aJdLS0sSWLVvEPffcI+rXry/y8/OVDt3mxowZI4KCgkRKSorIzMzU3W7cuKHb55lnnhG1atUS27ZtE/v27RMdOnQQHTp0UDBq+ynr8zh58qSYPn262Ldvn0hLSxOrV68WderUEZ07d1Y4cvt47bXXxPbt20VaWpo4fPiweO2114RKpRKbNm0SQpSv74YQpX8e5e27YQtM2oqYPXu2qFWrlvDy8hLt2rUTu3fvVjokRQwcOFCEhoYKLy8vUbNmTTFw4EBx8uRJpcNymOTkZAGgxC0hIUEIIZf9mDRpkqhRo4bw9vYW3bp1E8ePH1c2aDsr7TO5ceOG6NGjh6hWrZqoUKGCiIyMFKNGjXLbP3gMfQ4AxIIFC3T73Lx5Uzz77LOiUqVKwtfXV/Tv319kZmYqF7QdlfV5nD17VnTu3FlUrlxZeHt7i3r16omXX35ZZGdnKxu4nTz55JMiMjJSeHl5iWrVqolu3brpEjYhytd3Q4jSP4/y9t2wBZUQQjiuXY+IiIiILMExbUREREQugEkbERERkQtg0kZERETkApi0EREREbkAJm1ERERELoBJGxEREZELYNJGRERE5AKYtBERERG5ACZtRER2olKpsGrVKqXDICI3waSNiNzS8OHDoVKpStx69eqldGhERBbxVDoAIiJ76dWrFxYsWKC3zdvbW6FoiIisw5Y2InJb3t7eCAkJ0btVqlQJgOy6nDt3LmJjY1GxYkXUqVMHy5cv1zv+yJEjeOCBB1CxYkVUqVIFo0ePRm5urt4+33zzDZo2bQpvb2+EhoZi7Nixeq9fvnwZ/fv3h6+vL+rXr481a9boXrt27RqGDBmCatWqoWLFiqhfv36JJJOISItJGxGVW5MmTcIjjzyCQ4cOYciQIXj88cdx7NgxAEBeXh569uyJSpUqYe/evVi2bBm2bNmil5TNnTsXiYmJGD16NI4cOYI1a9agXr16eteYNm0aHnvsMRw+fBi9e/fGkCFDcPXqVd31//rrL6xfvx7Hjh3D3LlzUbVqVcd9AETkWgQRkRtKSEgQarVa+Pn56d3eeecdIYQQAMQzzzyjd0z79u3FmDFjhBBCfPnll6JSpUoiNzdX9/ovv/wiPDw8xIULF4QQQoSFhYk33njDaAwAxJtvvql7npubKwCI9evXCyGEiIuLEyNGjLDNGyYit8cxbUTktrp27Yq5c+fqbatcubLucYcOHfRe69ChAw4ePAgAOHbsGFq2bAk/Pz/d6506dUJBQQGOHz8OlUqF8+fPo1u3bqXG0KJFC91jPz8/BAYG4tKlSwCAMWPG4JFHHsH+/fvRo0cP9OvXDx07drTovRKR+2PSRkRuy8/Pr0R3pa1UrFjRpP0qVKig91ylUqGgoAAAEBsbizNnzmDdunXYvHkzunXrhsTERHz00Uc2j5eIXB/HtBFRubV79+4Szxs3bgwAaNy4MQ4dOoS8vDzd6zt37oSHhwcaNmyIgIAAREVFYevWrVbFUK1aNSQkJGDRokWYNWsWvvzyS6vOR0Tuiy1tROS2bt26hQsXLuht8/T01A32X7ZsGe69917cf//9+OGHH7Bnzx58/fXXAIAhQ4ZgypQpSEhIwNSpU/Hff//hueeew9ChQ1GjRg0AwNSpU/HMM8+gevXqiI2NxfXr17Fz504899xzJsU3efJktGnTBk2bNsWtW7ewdu1aXdJIRFQckzYiclsbNmxAaGio3raGDRvi77//BiBndi5duhTPPvssQkNDsWTJEjRp0gQA4Ovri40bN2LcuHFo27YtfH198cgjj+Djjz/WnSshIQH5+fmYOXMmJkyYgKpVq2LAgAEmx+fl5YWJEyfi9OnTqFixIqKjo7F06VIbvHMickcqIYRQOggiIkdTqVRYuXIl+vXrp3QoREQm4Zg2IiIiIhfApI2IiIjIBXBMGxGVSxwZQkSuhi1tRERERC6ASRsRERGRC2DSRkREROQCmLQRERERuQAmbUREREQugEkbERERkQtg0kZERETkApi0EREREbkAJm1ERERELuD/AZI+j/dOCe4kAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "visualize_learning_curves(epochs,train_loss,val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation - Dynamic Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params\n",
    "params['normalization'] =  'groupnorm'\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'augment': True, 'mode': 'train', 'seed': 42, 'subset': 'DATASET_DYNAMIC', 'interpolation': 'Linear Interpolation', 'normalization': 'groupnorm', 'total_train_samples': 0, 'total_val_samples': 0, 'lr': 0.0001, 'loss': 'dice+ce', 'dropout': 0.2, 'batch_size': 2, 'norm_params_minmax': (0.0, 1072.0), 'norm_params_meanstd': None}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_cross_validation(subset_dir, test_subjects,fold_idx, n_folds):\n",
    "    tfrecords_paths = create_subject_tfrpaths(subset_dir,test_subjects)\n",
    "    #subject_paths_array = np.array(subject_paths)\n",
    "\n",
    "    # perform cross-validation\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    folds = list(kf.split(tfrecords_paths))\n",
    "\n",
    "    # get the current fold data\n",
    "    train_indices, val_indices = folds[fold_idx]\n",
    "    train_paths = [tfrecords_paths[i] for i in train_indices]\n",
    "    val_paths = [tfrecords_paths[i] for i in val_indices]\n",
    "\n",
    "    return train_paths, val_paths\n",
    "\n",
    "def train_fold(fold_idx, n_folds, params, tfrecords_dir, test_subjects, batch_size, seed):\n",
    "    data_dir = data_dir = os.path.join(tfrecords_dir,params['subset'])\n",
    "    logs_and_model_path = refactor_path(os.path.join(logs_dir,params['interpolation'],params['subset']))\n",
    "    test_paths = parse_test_tfrecords(data_dir, test_subjects[params['subset']])\n",
    "    \n",
    "    train_paths, val_paths = subset_cross_validation(data_dir, test_subjects[params['subset']], fold_idx, n_folds)\n",
    "    \n",
    "    normalization_params_train = get_norm_params(train_paths,0)\n",
    "    print(f\"Fold {fold_idx + 1}/{n_folds}\")\n",
    "    print(\"Number of training tuple paths:\", len(train_paths))\n",
    "    print(\"Number of validation tuple paths:\", len(val_paths))\n",
    "    print(\"Number of test tuple paths:\", len(test_paths))\n",
    "    print(\"Normalization parameters training:\", normalization_params_train)\n",
    "    print(\"Layer Normalization: \", params['normalization'])\n",
    "    \n",
    "    params['total_train_samples'] = len(train_paths)\n",
    "    params['total_val_samples'] = len(val_paths)\n",
    "    \n",
    "    train_d = train_tfr_fn(train_paths, normalization_params_train, 0, batch_size, seed, params)\n",
    "    val_ds = val_tfr_fn(val_paths, normalization_params_train, batch_size)\n",
    "    \n",
    "    training_session_path = f'Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_{fold_idx + 1}_{datetime.datetime.now().strftime(\"(%Y-%m-%d)/%H.%M.%S\")}'\n",
    "    file_path, callbacks_list = prepareCallbacks(training_session_path, logs_and_model_path)\n",
    "    \n",
    "    features = tf.keras.layers.Input(shape=input_shape[params['subset']])\n",
    "    output = unet3d_mod(yshape[params['subset']][-1], params['mode'], features, params['normalization'])\n",
    "    unet3d_model = tf.keras.Model(inputs=features, outputs=output)\n",
    "    unet3d_model.compile(optimizer=Adam(learning_rate=params['lr']),\n",
    "                         loss=make_loss, \n",
    "                         metrics=[eval_dice])\n",
    "    \n",
    "    history = unet3d_model.fit(\n",
    "        train_d,\n",
    "        epochs=70,\n",
    "        validation_data=val_ds, \n",
    "        callbacks=callbacks_list\n",
    "    )    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_val(n_folds, params, resampled_dir, test_subjects,batch_size,seed):\n",
    "    history_reg = []\n",
    "    for fold_id in range(n_folds):\n",
    "        # fold_idx, n_folds, params, log_and_model_path, resampled_dir, test_subjects, batch_size, seed\n",
    "        history = train_fold(fold_id,n_folds, params, resampled_dir, test_subjects, batch_size, seed)\n",
    "        history_reg.append(history)\n",
    "    return history_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Number of training tuple paths: 209\n",
      "Number of validation tuple paths: 53\n",
      "Number of test tuple paths: 68\n",
      "Normalization parameters training: (0.0, 1849.0938)\n",
      "Layer Normalization:  groupnorm\n",
      "WARNING:tensorflow:From c:\\Users\\Eduardo\\miniconda3\\envs\\dataortho_env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Epoch 1/70\n",
      "    104/Unknown - 210s 2s/step - loss: 19.3585 - eval_dice: 0.5363\n",
      "Epoch 1: val_loss improved from inf to 18.37395, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 246s 2s/step - loss: 19.3585 - eval_dice: 0.5363 - val_loss: 18.3740 - val_eval_dice: 0.2082 - lr: 1.0000e-04\n",
      "Epoch 2/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.2038 - eval_dice: 0.1330\n",
      "Epoch 2: val_loss improved from 18.37395 to 18.08030, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 238s 2s/step - loss: 18.2038 - eval_dice: 0.1330 - val_loss: 18.0803 - val_eval_dice: 0.0850 - lr: 1.0000e-04\n",
      "Epoch 3/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.0182 - eval_dice: 0.0717\n",
      "Epoch 3: val_loss improved from 18.08030 to 17.91803, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 18.0182 - eval_dice: 0.0717 - val_loss: 17.9180 - val_eval_dice: 0.0388 - lr: 1.0000e-04\n",
      "Epoch 4/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.8350 - eval_dice: 0.0461\n",
      "Epoch 4: val_loss improved from 17.91803 to 17.67877, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 17.8350 - eval_dice: 0.0461 - val_loss: 17.6788 - val_eval_dice: 0.0286 - lr: 1.0000e-04\n",
      "Epoch 5/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.4715 - eval_dice: 0.0355\n",
      "Epoch 5: val_loss improved from 17.67877 to 17.22865, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 227s 2s/step - loss: 17.4715 - eval_dice: 0.0355 - val_loss: 17.2286 - val_eval_dice: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 6/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 16.9101 - eval_dice: 0.0312\n",
      "Epoch 6: val_loss improved from 17.22865 to 16.44746, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 227s 2s/step - loss: 16.9101 - eval_dice: 0.0312 - val_loss: 16.4475 - val_eval_dice: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 7/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 16.1803 - eval_dice: 0.0274\n",
      "Epoch 7: val_loss improved from 16.44746 to 15.88861, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 228s 2s/step - loss: 16.1803 - eval_dice: 0.0274 - val_loss: 15.8886 - val_eval_dice: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 8/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 15.4330 - eval_dice: 0.0268\n",
      "Epoch 8: val_loss improved from 15.88861 to 15.20392, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 237s 2s/step - loss: 15.4330 - eval_dice: 0.0268 - val_loss: 15.2039 - val_eval_dice: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 9/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.9138 - eval_dice: 0.0263\n",
      "Epoch 9: val_loss improved from 15.20392 to 14.66592, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 14.9138 - eval_dice: 0.0263 - val_loss: 14.6659 - val_eval_dice: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 10/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.2083 - eval_dice: 0.0255\n",
      "Epoch 10: val_loss improved from 14.66592 to 13.98560, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 14.2083 - eval_dice: 0.0255 - val_loss: 13.9856 - val_eval_dice: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 11/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.5684 - eval_dice: 0.0249\n",
      "Epoch 11: val_loss improved from 13.98560 to 13.47798, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 226s 2s/step - loss: 13.5684 - eval_dice: 0.0249 - val_loss: 13.4780 - val_eval_dice: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 12/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.1408 - eval_dice: 0.0244\n",
      "Epoch 12: val_loss improved from 13.47798 to 13.23061, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 228s 2s/step - loss: 13.1408 - eval_dice: 0.0244 - val_loss: 13.2306 - val_eval_dice: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 13/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.8958 - eval_dice: 0.0263\n",
      "Epoch 13: val_loss improved from 13.23061 to 13.03968, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 244s 2s/step - loss: 12.8958 - eval_dice: 0.0263 - val_loss: 13.0397 - val_eval_dice: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 14/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.6431 - eval_dice: 0.0253\n",
      "Epoch 14: val_loss improved from 13.03968 to 12.82418, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 234s 2s/step - loss: 12.6431 - eval_dice: 0.0253 - val_loss: 12.8242 - val_eval_dice: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 15/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.5256 - eval_dice: 0.0240\n",
      "Epoch 15: val_loss improved from 12.82418 to 12.78596, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 231s 2s/step - loss: 12.5256 - eval_dice: 0.0240 - val_loss: 12.7860 - val_eval_dice: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 16/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.4130 - eval_dice: 0.0232\n",
      "Epoch 16: val_loss improved from 12.78596 to 12.67278, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 12.4130 - eval_dice: 0.0232 - val_loss: 12.6728 - val_eval_dice: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 17/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.3291 - eval_dice: 0.0254\n",
      "Epoch 17: val_loss improved from 12.67278 to 12.63710, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 12.3291 - eval_dice: 0.0254 - val_loss: 12.6371 - val_eval_dice: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 18/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.2373 - eval_dice: 0.0232\n",
      "Epoch 18: val_loss improved from 12.63710 to 12.46099, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 12.2373 - eval_dice: 0.0232 - val_loss: 12.4610 - val_eval_dice: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 19/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0026 - eval_dice: 0.0237\n",
      "Epoch 19: val_loss improved from 12.46099 to 12.31539, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 232s 2s/step - loss: 12.0026 - eval_dice: 0.0237 - val_loss: 12.3154 - val_eval_dice: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 20/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.8471 - eval_dice: 0.0226\n",
      "Epoch 20: val_loss improved from 12.31539 to 12.27973, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 228s 2s/step - loss: 11.8471 - eval_dice: 0.0226 - val_loss: 12.2797 - val_eval_dice: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 21/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.7706 - eval_dice: 0.0232\n",
      "Epoch 21: val_loss did not improve from 12.27973\n",
      "104/104 [==============================] - 232s 2s/step - loss: 11.7706 - eval_dice: 0.0232 - val_loss: 12.2906 - val_eval_dice: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 22/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.6950 - eval_dice: 0.0240\n",
      "Epoch 22: val_loss did not improve from 12.27973\n",
      "104/104 [==============================] - 235s 2s/step - loss: 11.6950 - eval_dice: 0.0240 - val_loss: 12.4033 - val_eval_dice: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 23/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.6497 - eval_dice: 0.0245\n",
      "Epoch 23: val_loss did not improve from 12.27973\n",
      "104/104 [==============================] - 237s 2s/step - loss: 11.6497 - eval_dice: 0.0245 - val_loss: 12.4265 - val_eval_dice: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 24/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5808 - eval_dice: 0.0239\n",
      "Epoch 24: val_loss did not improve from 12.27973\n",
      "104/104 [==============================] - 240s 2s/step - loss: 11.5808 - eval_dice: 0.0239 - val_loss: 12.3801 - val_eval_dice: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 25/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5441 - eval_dice: 0.0252\n",
      "Epoch 25: val_loss did not improve from 12.27973\n",
      "104/104 [==============================] - 246s 2s/step - loss: 11.5441 - eval_dice: 0.0252 - val_loss: 12.2885 - val_eval_dice: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 26/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5384 - eval_dice: 0.0229\n",
      "Epoch 26: val_loss improved from 12.27973 to 12.23759, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 11.5384 - eval_dice: 0.0229 - val_loss: 12.2376 - val_eval_dice: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 27/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3674 - eval_dice: 0.0237\n",
      "Epoch 27: val_loss improved from 12.23759 to 11.96230, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 11.3674 - eval_dice: 0.0237 - val_loss: 11.9623 - val_eval_dice: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 28/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0974 - eval_dice: 0.0221\n",
      "Epoch 28: val_loss improved from 11.96230 to 11.79801, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 11.0974 - eval_dice: 0.0221 - val_loss: 11.7980 - val_eval_dice: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 29/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0611 - eval_dice: 0.0236\n",
      "Epoch 29: val_loss improved from 11.79801 to 11.75337, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 239s 2s/step - loss: 11.0611 - eval_dice: 0.0236 - val_loss: 11.7534 - val_eval_dice: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 30/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0104 - eval_dice: 0.0236\n",
      "Epoch 30: val_loss did not improve from 11.75337\n",
      "104/104 [==============================] - 236s 2s/step - loss: 11.0104 - eval_dice: 0.0236 - val_loss: 11.8136 - val_eval_dice: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 31/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0043 - eval_dice: 0.0240\n",
      "Epoch 31: val_loss did not improve from 11.75337\n",
      "104/104 [==============================] - 250s 2s/step - loss: 11.0043 - eval_dice: 0.0240 - val_loss: 11.8345 - val_eval_dice: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 32/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9667 - eval_dice: 0.0226\n",
      "Epoch 32: val_loss improved from 11.75337 to 11.71864, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 10.9667 - eval_dice: 0.0226 - val_loss: 11.7186 - val_eval_dice: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 33/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9590 - eval_dice: 0.0248\n",
      "Epoch 33: val_loss did not improve from 11.71864\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.9590 - eval_dice: 0.0248 - val_loss: 11.7745 - val_eval_dice: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 34/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8539 - eval_dice: 0.0226\n",
      "Epoch 34: val_loss improved from 11.71864 to 11.59187, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 244s 2s/step - loss: 10.8539 - eval_dice: 0.0226 - val_loss: 11.5919 - val_eval_dice: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 35/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.6429 - eval_dice: 0.0231\n",
      "Epoch 35: val_loss improved from 11.59187 to 11.36285, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 240s 2s/step - loss: 10.6429 - eval_dice: 0.0231 - val_loss: 11.3629 - val_eval_dice: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 36/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.5688 - eval_dice: 0.0235\n",
      "Epoch 36: val_loss improved from 11.36285 to 11.34780, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.5688 - eval_dice: 0.0235 - val_loss: 11.3478 - val_eval_dice: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 37/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.5239 - eval_dice: 0.0226\n",
      "Epoch 37: val_loss improved from 11.34780 to 11.29063, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 238s 2s/step - loss: 10.5239 - eval_dice: 0.0226 - val_loss: 11.2906 - val_eval_dice: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 38/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.5333 - eval_dice: 0.0225\n",
      "Epoch 38: val_loss did not improve from 11.29063\n",
      "104/104 [==============================] - 236s 2s/step - loss: 10.5333 - eval_dice: 0.0225 - val_loss: 11.3864 - val_eval_dice: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 39/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.5069 - eval_dice: 0.0228\n",
      "Epoch 39: val_loss did not improve from 11.29063\n",
      "104/104 [==============================] - 242s 2s/step - loss: 10.5069 - eval_dice: 0.0228 - val_loss: 11.3916 - val_eval_dice: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 40/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.4630 - eval_dice: 0.0214\n",
      "Epoch 40: val_loss did not improve from 11.29063\n",
      "104/104 [==============================] - 235s 2s/step - loss: 10.4630 - eval_dice: 0.0214 - val_loss: 11.3583 - val_eval_dice: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 41/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.5132 - eval_dice: 0.0232\n",
      "Epoch 41: val_loss did not improve from 11.29063\n",
      "104/104 [==============================] - 245s 2s/step - loss: 10.5132 - eval_dice: 0.0232 - val_loss: 11.3476 - val_eval_dice: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 42/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.3763 - eval_dice: 0.0215\n",
      "Epoch 42: val_loss improved from 11.29063 to 11.18497, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 244s 2s/step - loss: 10.3763 - eval_dice: 0.0215 - val_loss: 11.1850 - val_eval_dice: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 43/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.3622 - eval_dice: 0.0215\n",
      "Epoch 43: val_loss improved from 11.18497 to 11.18003, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 234s 2s/step - loss: 10.3622 - eval_dice: 0.0215 - val_loss: 11.1800 - val_eval_dice: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 44/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.3171 - eval_dice: 0.0217\n",
      "Epoch 44: val_loss did not improve from 11.18003\n",
      "104/104 [==============================] - 241s 2s/step - loss: 10.3171 - eval_dice: 0.0217 - val_loss: 11.2761 - val_eval_dice: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 45/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2611 - eval_dice: 0.0223\n",
      "Epoch 45: val_loss did not improve from 11.18003\n",
      "104/104 [==============================] - 236s 2s/step - loss: 10.2611 - eval_dice: 0.0223 - val_loss: 11.1965 - val_eval_dice: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 46/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.2125 - eval_dice: 0.0222\n",
      "Epoch 46: val_loss did not improve from 11.18003\n",
      "104/104 [==============================] - 239s 2s/step - loss: 10.2125 - eval_dice: 0.0222 - val_loss: 11.2213 - val_eval_dice: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 47/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.1606 - eval_dice: 0.0207\n",
      "Epoch 47: val_loss improved from 11.18003 to 11.07653, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 234s 2s/step - loss: 10.1606 - eval_dice: 0.0207 - val_loss: 11.0765 - val_eval_dice: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 48/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.9982 - eval_dice: 0.0222 \n",
      "Epoch 48: val_loss improved from 11.07653 to 10.94955, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 238s 2s/step - loss: 9.9982 - eval_dice: 0.0222 - val_loss: 10.9496 - val_eval_dice: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 49/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.9234 - eval_dice: 0.0210\n",
      "Epoch 49: val_loss improved from 10.94955 to 10.89925, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 243s 2s/step - loss: 9.9234 - eval_dice: 0.0210 - val_loss: 10.8993 - val_eval_dice: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 50/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.9001 - eval_dice: 0.0214\n",
      "Epoch 50: val_loss did not improve from 10.89925\n",
      "104/104 [==============================] - 238s 2s/step - loss: 9.9001 - eval_dice: 0.0214 - val_loss: 10.9688 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 51/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8859 - eval_dice: 0.0221\n",
      "Epoch 51: val_loss improved from 10.89925 to 10.84843, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 235s 2s/step - loss: 9.8859 - eval_dice: 0.0221 - val_loss: 10.8484 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 52/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8606 - eval_dice: 0.0225\n",
      "Epoch 52: val_loss did not improve from 10.84843\n",
      "104/104 [==============================] - 240s 2s/step - loss: 9.8606 - eval_dice: 0.0225 - val_loss: 10.9146 - val_eval_dice: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 53/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8564 - eval_dice: 0.0234\n",
      "Epoch 53: val_loss did not improve from 10.84843\n",
      "104/104 [==============================] - 245s 2s/step - loss: 9.8564 - eval_dice: 0.0234 - val_loss: 10.9133 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 54/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8299 - eval_dice: 0.0221\n",
      "Epoch 54: val_loss did not improve from 10.84843\n",
      "104/104 [==============================] - 235s 2s/step - loss: 9.8299 - eval_dice: 0.0221 - val_loss: 10.9710 - val_eval_dice: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 55/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8330 - eval_dice: 0.0226\n",
      "Epoch 55: val_loss did not improve from 10.84843\n",
      "104/104 [==============================] - 244s 2s/step - loss: 9.8330 - eval_dice: 0.0226 - val_loss: 10.9332 - val_eval_dice: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 56/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8151 - eval_dice: 0.0220\n",
      "Epoch 56: val_loss did not improve from 10.84843\n",
      "104/104 [==============================] - 241s 2s/step - loss: 9.8151 - eval_dice: 0.0220 - val_loss: 10.9042 - val_eval_dice: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 57/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8185 - eval_dice: 0.0209\n",
      "Epoch 57: val_loss did not improve from 10.84843\n",
      "104/104 [==============================] - 244s 2s/step - loss: 9.8185 - eval_dice: 0.0209 - val_loss: 10.9346 - val_eval_dice: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 58/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8167 - eval_dice: 0.0213\n",
      "Epoch 58: val_loss improved from 10.84843 to 10.84821, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 236s 2s/step - loss: 9.8167 - eval_dice: 0.0213 - val_loss: 10.8482 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 59/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8095 - eval_dice: 0.0218\n",
      "Epoch 59: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 235s 2s/step - loss: 9.8095 - eval_dice: 0.0218 - val_loss: 10.9156 - val_eval_dice: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 60/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.8079 - eval_dice: 0.0225\n",
      "Epoch 60: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 237s 2s/step - loss: 9.8079 - eval_dice: 0.0225 - val_loss: 10.8663 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 61/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7739 - eval_dice: 0.0213\n",
      "Epoch 61: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 235s 2s/step - loss: 9.7739 - eval_dice: 0.0213 - val_loss: 10.8944 - val_eval_dice: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 62/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7801 - eval_dice: 0.0207\n",
      "Epoch 62: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 239s 2s/step - loss: 9.7801 - eval_dice: 0.0207 - val_loss: 10.9314 - val_eval_dice: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 63/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7976 - eval_dice: 0.0225\n",
      "Epoch 63: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 240s 2s/step - loss: 9.7976 - eval_dice: 0.0225 - val_loss: 10.8787 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 64/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7800 - eval_dice: 0.0224\n",
      "Epoch 64: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 237s 2s/step - loss: 9.7800 - eval_dice: 0.0224 - val_loss: 10.8615 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 65/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7665 - eval_dice: 0.0214\n",
      "Epoch 65: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 233s 2s/step - loss: 9.7665 - eval_dice: 0.0214 - val_loss: 10.8744 - val_eval_dice: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 66/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7640 - eval_dice: 0.0228\n",
      "Epoch 66: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 244s 2s/step - loss: 9.7640 - eval_dice: 0.0228 - val_loss: 10.8618 - val_eval_dice: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 67/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7456 - eval_dice: 0.0215\n",
      "Epoch 67: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 241s 2s/step - loss: 9.7456 - eval_dice: 0.0215 - val_loss: 10.8773 - val_eval_dice: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 68/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7514 - eval_dice: 0.0224\n",
      "Epoch 68: val_loss did not improve from 10.84821\n",
      "104/104 [==============================] - 245s 2s/step - loss: 9.7514 - eval_dice: 0.0224 - val_loss: 10.8680 - val_eval_dice: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 69/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.7157 - eval_dice: 0.0215\n",
      "Epoch 69: val_loss improved from 10.84821 to 10.82005, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 249s 2s/step - loss: 9.7157 - eval_dice: 0.0215 - val_loss: 10.8201 - val_eval_dice: 0.0084 - lr: 2.0000e-05\n",
      "Epoch 70/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 9.6795 - eval_dice: 0.0209\n",
      "Epoch 70: val_loss improved from 10.82005 to 10.81623, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_1_(2024-06-27)/10.57.41\\cp.ckpt\n",
      "104/104 [==============================] - 241s 2s/step - loss: 9.6795 - eval_dice: 0.0209 - val_loss: 10.8162 - val_eval_dice: 0.0083 - lr: 2.0000e-05\n",
      "Fold 2/5\n",
      "Number of training tuple paths: 209\n",
      "Number of validation tuple paths: 53\n",
      "Number of test tuple paths: 68\n",
      "Normalization parameters training: (0.0, 1072.0)\n",
      "Layer Normalization:  groupnorm\n",
      "Epoch 1/70\n",
      "    104/Unknown - 219s 2s/step - loss: 20.5055 - eval_dice: 0.7825\n",
      "Epoch 1: val_loss improved from inf to 19.28562, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 259s 2s/step - loss: 20.5055 - eval_dice: 0.7825 - val_loss: 19.2856 - val_eval_dice: 0.5453 - lr: 1.0000e-04\n",
      "Epoch 2/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.5893 - eval_dice: 0.2999\n",
      "Epoch 2: val_loss improved from 19.28562 to 18.40676, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 259s 2s/step - loss: 18.5893 - eval_dice: 0.2999 - val_loss: 18.4068 - val_eval_dice: 0.2083 - lr: 1.0000e-04\n",
      "Epoch 3/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 18.1893 - eval_dice: 0.1261\n",
      "Epoch 3: val_loss improved from 18.40676 to 18.08877, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 248s 2s/step - loss: 18.1893 - eval_dice: 0.1261 - val_loss: 18.0888 - val_eval_dice: 0.0819 - lr: 1.0000e-04\n",
      "Epoch 4/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.9628 - eval_dice: 0.0623\n",
      "Epoch 4: val_loss improved from 18.08877 to 17.92487, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 249s 2s/step - loss: 17.9628 - eval_dice: 0.0623 - val_loss: 17.9249 - val_eval_dice: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 5/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.7527 - eval_dice: 0.0392\n",
      "Epoch 5: val_loss improved from 17.92487 to 17.70352, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 259s 2s/step - loss: 17.7527 - eval_dice: 0.0392 - val_loss: 17.7035 - val_eval_dice: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 6/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 17.3525 - eval_dice: 0.0325\n",
      "Epoch 6: val_loss improved from 17.70352 to 17.42555, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 263s 2s/step - loss: 17.3525 - eval_dice: 0.0325 - val_loss: 17.4255 - val_eval_dice: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 7/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 16.5971 - eval_dice: 0.0288\n",
      "Epoch 7: val_loss improved from 17.42555 to 16.80442, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 253s 2s/step - loss: 16.5971 - eval_dice: 0.0288 - val_loss: 16.8044 - val_eval_dice: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 8/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 15.7269 - eval_dice: 0.0273\n",
      "Epoch 8: val_loss improved from 16.80442 to 16.08171, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 257s 2s/step - loss: 15.7269 - eval_dice: 0.0273 - val_loss: 16.0817 - val_eval_dice: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 9/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.8316 - eval_dice: 0.0246\n",
      "Epoch 9: val_loss improved from 16.08171 to 15.70440, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 256s 2s/step - loss: 14.8316 - eval_dice: 0.0246 - val_loss: 15.7044 - val_eval_dice: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 10/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 14.1149 - eval_dice: 0.0252\n",
      "Epoch 10: val_loss improved from 15.70440 to 15.21185, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 253s 2s/step - loss: 14.1149 - eval_dice: 0.0252 - val_loss: 15.2118 - val_eval_dice: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 11/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 13.3537 - eval_dice: 0.0251\n",
      "Epoch 11: val_loss improved from 15.21185 to 14.81396, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 267s 2s/step - loss: 13.3537 - eval_dice: 0.0251 - val_loss: 14.8140 - val_eval_dice: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 12/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.8386 - eval_dice: 0.0226\n",
      "Epoch 12: val_loss improved from 14.81396 to 14.60489, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 255s 2s/step - loss: 12.8386 - eval_dice: 0.0226 - val_loss: 14.6049 - val_eval_dice: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 13/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.5546 - eval_dice: 0.0226\n",
      "Epoch 13: val_loss improved from 14.60489 to 14.28430, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 256s 2s/step - loss: 12.5546 - eval_dice: 0.0226 - val_loss: 14.2843 - val_eval_dice: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 14/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.3696 - eval_dice: 0.0225\n",
      "Epoch 14: val_loss improved from 14.28430 to 14.17923, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 255s 2s/step - loss: 12.3696 - eval_dice: 0.0225 - val_loss: 14.1792 - val_eval_dice: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 15/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.3039 - eval_dice: 0.0228\n",
      "Epoch 15: val_loss did not improve from 14.17923\n",
      "104/104 [==============================] - 259s 2s/step - loss: 12.3039 - eval_dice: 0.0228 - val_loss: 14.2006 - val_eval_dice: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 16/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.2133 - eval_dice: 0.0241\n",
      "Epoch 16: val_loss improved from 14.17923 to 14.12197, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 256s 2s/step - loss: 12.2133 - eval_dice: 0.0241 - val_loss: 14.1220 - val_eval_dice: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 17/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.1705 - eval_dice: 0.0233\n",
      "Epoch 17: val_loss improved from 14.12197 to 14.05356, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 251s 2s/step - loss: 12.1705 - eval_dice: 0.0233 - val_loss: 14.0536 - val_eval_dice: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 18/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.1411 - eval_dice: 0.0240\n",
      "Epoch 18: val_loss improved from 14.05356 to 13.92430, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 262s 2s/step - loss: 12.1411 - eval_dice: 0.0240 - val_loss: 13.9243 - val_eval_dice: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 19/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 12.0316 - eval_dice: 0.0237\n",
      "Epoch 19: val_loss did not improve from 13.92430\n",
      "104/104 [==============================] - 254s 2s/step - loss: 12.0316 - eval_dice: 0.0237 - val_loss: 14.0056 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 20/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.8243 - eval_dice: 0.0241\n",
      "Epoch 20: val_loss did not improve from 13.92430\n",
      "104/104 [==============================] - 253s 2s/step - loss: 11.8243 - eval_dice: 0.0241 - val_loss: 14.0454 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 21/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.7127 - eval_dice: 0.0239\n",
      "Epoch 21: val_loss improved from 13.92430 to 13.74433, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 252s 2s/step - loss: 11.7127 - eval_dice: 0.0239 - val_loss: 13.7443 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 22/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.6470 - eval_dice: 0.0222\n",
      "Epoch 22: val_loss did not improve from 13.74433\n",
      "104/104 [==============================] - 249s 2s/step - loss: 11.6470 - eval_dice: 0.0222 - val_loss: 14.2196 - val_eval_dice: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 23/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.6280 - eval_dice: 0.0219\n",
      "Epoch 23: val_loss did not improve from 13.74433\n",
      "104/104 [==============================] - 251s 2s/step - loss: 11.6280 - eval_dice: 0.0219 - val_loss: 13.8586 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 24/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5855 - eval_dice: 0.0240\n",
      "Epoch 24: val_loss improved from 13.74433 to 13.59153, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 265s 2s/step - loss: 11.5855 - eval_dice: 0.0240 - val_loss: 13.5915 - val_eval_dice: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 25/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5523 - eval_dice: 0.0230\n",
      "Epoch 25: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 262s 2s/step - loss: 11.5523 - eval_dice: 0.0230 - val_loss: 13.8544 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 26/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.5196 - eval_dice: 0.0235\n",
      "Epoch 26: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 262s 2s/step - loss: 11.5196 - eval_dice: 0.0235 - val_loss: 13.7119 - val_eval_dice: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 27/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4883 - eval_dice: 0.0229\n",
      "Epoch 27: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 255s 2s/step - loss: 11.4883 - eval_dice: 0.0229 - val_loss: 13.6685 - val_eval_dice: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 28/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4516 - eval_dice: 0.0226\n",
      "Epoch 28: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 252s 2s/step - loss: 11.4516 - eval_dice: 0.0226 - val_loss: 13.7471 - val_eval_dice: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 29/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4544 - eval_dice: 0.0232\n",
      "Epoch 29: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 252s 2s/step - loss: 11.4544 - eval_dice: 0.0232 - val_loss: 13.6309 - val_eval_dice: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 30/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4402 - eval_dice: 0.0226\n",
      "Epoch 30: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 252s 2s/step - loss: 11.4402 - eval_dice: 0.0226 - val_loss: 13.8123 - val_eval_dice: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 31/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4298 - eval_dice: 0.0240\n",
      "Epoch 31: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 249s 2s/step - loss: 11.4298 - eval_dice: 0.0240 - val_loss: 13.7710 - val_eval_dice: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 32/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.4329 - eval_dice: 0.0218\n",
      "Epoch 32: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 250s 2s/step - loss: 11.4329 - eval_dice: 0.0218 - val_loss: 13.7304 - val_eval_dice: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 33/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3943 - eval_dice: 0.0224\n",
      "Epoch 33: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 247s 2s/step - loss: 11.3943 - eval_dice: 0.0224 - val_loss: 13.8783 - val_eval_dice: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 34/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3983 - eval_dice: 0.0241\n",
      "Epoch 34: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 259s 2s/step - loss: 11.3983 - eval_dice: 0.0241 - val_loss: 13.7966 - val_eval_dice: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 35/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3409 - eval_dice: 0.0233\n",
      "Epoch 35: val_loss did not improve from 13.59153\n",
      "104/104 [==============================] - 255s 2s/step - loss: 11.3409 - eval_dice: 0.0233 - val_loss: 13.6005 - val_eval_dice: 0.0122 - lr: 2.0000e-05\n",
      "Epoch 36/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.3093 - eval_dice: 0.0234\n",
      "Epoch 36: val_loss improved from 13.59153 to 13.57538, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 251s 2s/step - loss: 11.3093 - eval_dice: 0.0234 - val_loss: 13.5754 - val_eval_dice: 0.0124 - lr: 2.0000e-05\n",
      "Epoch 37/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2985 - eval_dice: 0.0234\n",
      "Epoch 37: val_loss did not improve from 13.57538\n",
      "104/104 [==============================] - 256s 2s/step - loss: 11.2985 - eval_dice: 0.0234 - val_loss: 13.6300 - val_eval_dice: 0.0119 - lr: 2.0000e-05\n",
      "Epoch 38/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2799 - eval_dice: 0.0213\n",
      "Epoch 38: val_loss improved from 13.57538 to 13.55429, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 245s 2s/step - loss: 11.2799 - eval_dice: 0.0213 - val_loss: 13.5543 - val_eval_dice: 0.0122 - lr: 2.0000e-05\n",
      "Epoch 39/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2780 - eval_dice: 0.0229\n",
      "Epoch 39: val_loss improved from 13.55429 to 13.54487, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 250s 2s/step - loss: 11.2780 - eval_dice: 0.0229 - val_loss: 13.5449 - val_eval_dice: 0.0121 - lr: 2.0000e-05\n",
      "Epoch 40/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2787 - eval_dice: 0.0245\n",
      "Epoch 40: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 254s 2s/step - loss: 11.2787 - eval_dice: 0.0245 - val_loss: 13.6280 - val_eval_dice: 0.0120 - lr: 2.0000e-05\n",
      "Epoch 41/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2700 - eval_dice: 0.0220\n",
      "Epoch 41: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 246s 2s/step - loss: 11.2700 - eval_dice: 0.0220 - val_loss: 13.5991 - val_eval_dice: 0.0120 - lr: 2.0000e-05\n",
      "Epoch 42/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2626 - eval_dice: 0.0231\n",
      "Epoch 42: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 254s 2s/step - loss: 11.2626 - eval_dice: 0.0231 - val_loss: 13.5469 - val_eval_dice: 0.0121 - lr: 2.0000e-05\n",
      "Epoch 43/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2515 - eval_dice: 0.0208\n",
      "Epoch 43: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 253s 2s/step - loss: 11.2515 - eval_dice: 0.0208 - val_loss: 13.6646 - val_eval_dice: 0.0118 - lr: 2.0000e-05\n",
      "Epoch 44/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2539 - eval_dice: 0.0234\n",
      "Epoch 44: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 257s 2s/step - loss: 11.2539 - eval_dice: 0.0234 - val_loss: 13.5739 - val_eval_dice: 0.0122 - lr: 2.0000e-05\n",
      "Epoch 45/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2504 - eval_dice: 0.0234\n",
      "Epoch 45: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 252s 2s/step - loss: 11.2504 - eval_dice: 0.0234 - val_loss: 13.6303 - val_eval_dice: 0.0118 - lr: 2.0000e-05\n",
      "Epoch 46/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2425 - eval_dice: 0.0225\n",
      "Epoch 46: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 253s 2s/step - loss: 11.2425 - eval_dice: 0.0225 - val_loss: 13.5958 - val_eval_dice: 0.0123 - lr: 2.0000e-05\n",
      "Epoch 47/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2369 - eval_dice: 0.0221\n",
      "Epoch 47: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 255s 2s/step - loss: 11.2369 - eval_dice: 0.0221 - val_loss: 13.5639 - val_eval_dice: 0.0119 - lr: 2.0000e-05\n",
      "Epoch 48/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.2340 - eval_dice: 0.0221\n",
      "Epoch 48: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 251s 2s/step - loss: 11.2340 - eval_dice: 0.0221 - val_loss: 13.6535 - val_eval_dice: 0.0119 - lr: 2.0000e-05\n",
      "Epoch 49/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1638 - eval_dice: 0.0228\n",
      "Epoch 49: val_loss did not improve from 13.54487\n",
      "104/104 [==============================] - 247s 2s/step - loss: 11.1638 - eval_dice: 0.0228 - val_loss: 13.5827 - val_eval_dice: 0.0119 - lr: 2.0000e-05\n",
      "Epoch 50/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1158 - eval_dice: 0.0238\n",
      "Epoch 50: val_loss improved from 13.54487 to 13.50729, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 254s 2s/step - loss: 11.1158 - eval_dice: 0.0238 - val_loss: 13.5073 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 51/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.1043 - eval_dice: 0.0220\n",
      "Epoch 51: val_loss did not improve from 13.50729\n",
      "104/104 [==============================] - 252s 2s/step - loss: 11.1043 - eval_dice: 0.0220 - val_loss: 13.5199 - val_eval_dice: 0.0120 - lr: 4.0000e-06\n",
      "Epoch 52/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0989 - eval_dice: 0.0228\n",
      "Epoch 52: val_loss did not improve from 13.50729\n",
      "104/104 [==============================] - 261s 2s/step - loss: 11.0989 - eval_dice: 0.0228 - val_loss: 13.5079 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 53/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 11.0829 - eval_dice: 0.0225\n",
      "Epoch 53: val_loss improved from 13.50729 to 13.45121, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 249s 2s/step - loss: 11.0829 - eval_dice: 0.0225 - val_loss: 13.4512 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 54/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9735 - eval_dice: 0.0217\n",
      "Epoch 54: val_loss improved from 13.45121 to 13.40957, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 259s 2s/step - loss: 10.9735 - eval_dice: 0.0217 - val_loss: 13.4096 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 55/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.9052 - eval_dice: 0.0216\n",
      "Epoch 55: val_loss improved from 13.40957 to 13.38679, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 257s 2s/step - loss: 10.9052 - eval_dice: 0.0216 - val_loss: 13.3868 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 56/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8807 - eval_dice: 0.0223\n",
      "Epoch 56: val_loss did not improve from 13.38679\n",
      "104/104 [==============================] - 255s 2s/step - loss: 10.8807 - eval_dice: 0.0223 - val_loss: 13.3868 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 57/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8678 - eval_dice: 0.0226\n",
      "Epoch 57: val_loss improved from 13.38679 to 13.35718, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 264s 2s/step - loss: 10.8678 - eval_dice: 0.0226 - val_loss: 13.3572 - val_eval_dice: 0.0121 - lr: 4.0000e-06\n",
      "Epoch 58/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8575 - eval_dice: 0.0224\n",
      "Epoch 58: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 275s 2s/step - loss: 10.8575 - eval_dice: 0.0224 - val_loss: 13.3628 - val_eval_dice: 0.0120 - lr: 4.0000e-06\n",
      "Epoch 59/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8497 - eval_dice: 0.0226\n",
      "Epoch 59: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 276s 2s/step - loss: 10.8497 - eval_dice: 0.0226 - val_loss: 13.3737 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 60/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8393 - eval_dice: 0.0225\n",
      "Epoch 60: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 257s 2s/step - loss: 10.8393 - eval_dice: 0.0225 - val_loss: 13.3657 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 61/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8393 - eval_dice: 0.0231\n",
      "Epoch 61: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 254s 2s/step - loss: 10.8393 - eval_dice: 0.0231 - val_loss: 13.3736 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 62/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8321 - eval_dice: 0.0222\n",
      "Epoch 62: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 256s 2s/step - loss: 10.8321 - eval_dice: 0.0222 - val_loss: 13.3760 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 63/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8288 - eval_dice: 0.0223\n",
      "Epoch 63: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 249s 2s/step - loss: 10.8288 - eval_dice: 0.0223 - val_loss: 13.3620 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 64/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8243 - eval_dice: 0.0228\n",
      "Epoch 64: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 262s 2s/step - loss: 10.8243 - eval_dice: 0.0228 - val_loss: 13.3716 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 65/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8292 - eval_dice: 0.0241\n",
      "Epoch 65: val_loss did not improve from 13.35718\n",
      "104/104 [==============================] - 254s 2s/step - loss: 10.8292 - eval_dice: 0.0241 - val_loss: 13.3956 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 66/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8151 - eval_dice: 0.0213\n",
      "Epoch 66: val_loss improved from 13.35718 to 13.35525, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 250s 2s/step - loss: 10.8151 - eval_dice: 0.0213 - val_loss: 13.3553 - val_eval_dice: 0.0120 - lr: 4.0000e-06\n",
      "Epoch 67/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8144 - eval_dice: 0.0227\n",
      "Epoch 67: val_loss improved from 13.35525 to 13.33120, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_2_(2024-06-27)/15.37.49\\cp.ckpt\n",
      "104/104 [==============================] - 249s 2s/step - loss: 10.8144 - eval_dice: 0.0227 - val_loss: 13.3312 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 68/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8129 - eval_dice: 0.0228\n",
      "Epoch 68: val_loss did not improve from 13.33120\n",
      "104/104 [==============================] - 254s 2s/step - loss: 10.8129 - eval_dice: 0.0228 - val_loss: 13.3543 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Epoch 69/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8097 - eval_dice: 0.0228\n",
      "Epoch 69: val_loss did not improve from 13.33120\n",
      "104/104 [==============================] - 258s 2s/step - loss: 10.8097 - eval_dice: 0.0228 - val_loss: 13.3705 - val_eval_dice: 0.0120 - lr: 4.0000e-06\n",
      "Epoch 70/70\n",
      "104/104 [==============================] - ETA: 0s - loss: 10.8153 - eval_dice: 0.0238\n",
      "Epoch 70: val_loss did not improve from 13.33120\n",
      "104/104 [==============================] - 261s 2s/step - loss: 10.8153 - eval_dice: 0.0238 - val_loss: 13.3396 - val_eval_dice: 0.0119 - lr: 4.0000e-06\n",
      "Fold 3/5\n",
      "Number of training tuple paths: 210\n",
      "Number of validation tuple paths: 52\n",
      "Number of test tuple paths: 68\n",
      "Normalization parameters training: (0.0, 1849.0938)\n",
      "Layer Normalization:  groupnorm\n",
      "Epoch 1/70\n",
      "    105/Unknown - 218s 2s/step - loss: 21.2307 - eval_dice: 0.8724\n",
      "Epoch 1: val_loss improved from inf to 19.82834, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_3_(2024-06-27)/20.38.25\\cp.ckpt\n",
      "105/105 [==============================] - 259s 2s/step - loss: 21.2307 - eval_dice: 0.8724 - val_loss: 19.8283 - val_eval_dice: 0.6923 - lr: 1.0000e-04\n",
      "Epoch 2/70\n",
      "105/105 [==============================] - ETA: 0s - loss: 18.8500 - eval_dice: 0.3956\n",
      "Epoch 2: val_loss improved from 19.82834 to 18.48528, saving model to C:/Users/Eduardo/Desktop/dataortho_edu/logs/Linear Interpolation/DATASET_DYNAMIC/Sigma3/GroupNorm/cross_validation/unet3d-GN_fold_3_(2024-06-27)/20.38.25\\cp.ckpt\n",
      "105/105 [==============================] - 249s 2s/step - loss: 18.8500 - eval_dice: 0.3956 - val_loss: 18.4853 - val_eval_dice: 0.2539 - lr: 1.0000e-04\n",
      "Epoch 3/70\n",
      " 48/105 [============>.................] - ETA: 1:55 - loss: 18.2949 - eval_dice: 0.1716"
     ]
    }
   ],
   "source": [
    "histories = run_kfold_cross_val(n_folds,params,tfrecords_dir,test_subjects,batch_size,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = run_kfold_cross_val(n_folds,params,tfrecords_dir,test_subjects,batch_size,seed)\n",
    "history_dict = histories[4].history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "visualize_learning_curves(epochs,train_loss,val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
